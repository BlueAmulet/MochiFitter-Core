# -*- coding: utf-8 -*-

"""
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.
"""

import bpy
import os
import json
import math
import numpy as np
import argparse
import sys
import re
from mathutils import Matrix, Vector, Euler
from mathutils.kdtree import KDTree
from typing import Dict, Optional, Tuple, Set
from scipy.spatial import cKDTree
import bmesh
from mathutils.bvhtree import BVHTree
import mathutils
import time
from collections import deque, defaultdict

# Add Global Cache Dictionary
_mesh_cache = {}

# Global Variables: Pose State Management
_saved_pose_state = None
_previous_pose_state = None

_is_A_pose = False

def save_pose_state(armature_obj: bpy.types.Object) -> dict:
    """
    Save the current pose state of the armature

    Parameters:
        armature_obj: Armature Object

    Returns:
        Dictionary of saved pose states
    """
    if not armature_obj or armature_obj.type != 'ARMATURE':
        return None

    pose_state = {}
    for bone in armature_obj.pose.bones:
        pose_state[bone.name] = {
            'matrix': bone.matrix.copy(),
            'location': bone.location.copy(),
            'rotation_euler': bone.rotation_euler.copy(),
            'rotation_quaternion': bone.rotation_quaternion.copy(),
            'scale': bone.scale.copy()
        }

    return pose_state

def restore_pose_state(armature_obj: bpy.types.Object, pose_state: dict) -> None:
    """
    Restore the armature's pose state

    Parameters:
        armature_obj: Armature Object
        pose_state: Dictionary of pose states to restore
    """
    if not armature_obj or armature_obj.type != 'ARMATURE' or not pose_state:
        return

    for bone_name, state in pose_state.items():
        if bone_name in armature_obj.pose.bones:
            bone = armature_obj.pose.bones[bone_name]
            bone.matrix = state['matrix']
            bone.location = state['location']
            bone.rotation_euler = state['rotation_euler']
            bone.rotation_quaternion = state['rotation_quaternion']
            bone.scale = state['scale']

    # Force pose update
    bpy.context.view_layer.update()

def save_shape_key_state(mesh_obj: bpy.types.Object) -> dict:
    """
    Save the shape key state of a mesh object

    Parameters:
        mesh_obj: Mesh object

    Returns:
        Dictionary of saved shape key states
    """
    if not mesh_obj or not mesh_obj.data.shape_keys:
        return {}

    shape_key_state = {}
    for key_block in mesh_obj.data.shape_keys.key_blocks:
        shape_key_state[key_block.name] = key_block.value

    return shape_key_state

def restore_shape_key_state(mesh_obj: bpy.types.Object, shape_key_state: dict) -> None:
    """
    Restore the shape key state of the mesh object

    Parameters:
        mesh_obj: Mesh object
        shape_key_state: Dictionary of shape key states to restore
    """
    if not mesh_obj or not mesh_obj.data.shape_keys or not shape_key_state:
        return

    for key_name, value in shape_key_state.items():
        if key_name in mesh_obj.data.shape_keys.key_blocks:
            mesh_obj.data.shape_keys.key_blocks[key_name].value = value

def apply_blend_shape_settings(mesh_obj: bpy.types.Object, blend_shape_settings: list, ignore_missing_shape_keys: bool = True) -> None:
    """
    Apply shape key settings

    Parameters:
        mesh_obj: Mesh object
        blend_shape_settings: List of shape key settings to apply
    """
    if not mesh_obj or not mesh_obj.data.shape_keys or not blend_shape_settings:
        return False

    for setting in blend_shape_settings:
        shape_name = setting.get("name")
        if shape_name not in mesh_obj.data.shape_keys.key_blocks:
            temp_shape_key_name = f"{shape_name}_temp"
            if temp_shape_key_name not in mesh_obj.data.shape_keys.key_blocks:
                if not ignore_missing_shape_keys:
                    print(f"Required shape key does not exist: {shape_name}")
                    return False

    for setting in blend_shape_settings:
        shape_name = setting.get("name")
        shape_value = setting.get("value", 0.0)

        if shape_name in mesh_obj.data.shape_keys.key_blocks:
            mesh_obj.data.shape_keys.key_blocks[shape_name].value = shape_value
            print(f"Applied shape key setting: {shape_name} = {shape_value}")
        else:
            temp_shape_key_name = f"{shape_name}_temp"
            if temp_shape_key_name in mesh_obj.data.shape_keys.key_blocks:
                mesh_obj.data.shape_keys.key_blocks[temp_shape_key_name].value = shape_value
                print(f"Applied shape key setting: {temp_shape_key_name} = {shape_value}")
    return True

def store_pose_globally(armature_obj: bpy.types.Object) -> None:
    """
    Save the pause state to a global variable

    Parameters:
        armature_obj: Armature Object
    """
    global _saved_pose_state
    _saved_pose_state = save_pose_state(armature_obj)

def restore_global_pose(armature_obj: bpy.types.Object) -> None:
    """
    Restore the pose state from the global variable

    Parameters:
        armature_obj: Armature Object
    """
    global _saved_pose_state
    if _saved_pose_state is not None:
        restore_pose_state(armature_obj, _saved_pose_state)

def store_current_pose_as_previous(armature_obj: bpy.types.Object) -> None:
    """
    Save the current pose as the previous pose

    Parameters:
        armature_obj: Armature Object
    """
    global _previous_pose_state
    _previous_pose_state = save_pose_state(armature_obj)

def restore_previous_pose(armature_obj: bpy.types.Object) -> None:
    """
    Restore previous pose

    Parameters:
        armature_obj: Armature Object
    """
    global _previous_pose_state
    if _previous_pose_state is not None:
        restore_pose_state(armature_obj, _previous_pose_state)

def load_avatar_data_for_blendshape_analysis(avatar_data_path: str) -> dict:
    """
    Load avatar data for BlendShape analysis

    Parameters:
        avatar_data_path: Path to the avatar data file

    Returns:
        dict: Avatar data
    """
    try:
        with open(avatar_data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading avatar data {avatar_data_path}: {e}")
        return {}

def get_blendshape_groups(avatar_data: dict) -> dict:
    """
    Retrieve BlendShapeGroups from avatar data

    Parameters:
        avatar_data: Avatar data

    Returns:
        dict: A dictionary where the key is the BlendShapeGroup name and the value is a list of BlendShape names contained within that group
    """
    groups = {}
    blend_shape_groups = avatar_data.get('blendShapeGroups', [])
    for group in blend_shape_groups:
        group_name = group.get('name', '')
        blend_shape_fields = group.get('blendShapeFields', [])
        groups[group_name] = blend_shape_fields
    return groups

def get_deformation_fields_mapping(avatar_data: dict) -> tuple:
    """
    Retrieve the deformation field mapping for BlendShape from the avatar data

    Parameters:
        avatar_data: Avatar data

    Returns:
        tuple: (blendShapeFields, invertedBlendShapeFields) mapping dictionary tuple
    """
    blend_shape_fields = {}
    inverted_fields = {}

    # Retrieve from blendShapeFields
    for field in avatar_data.get('blendShapeFields', []):
        label = field.get('label', '')
        if label:
            blend_shape_fields[label] = field

    # Retrieve from invertedBlendShapeFields
    for field in avatar_data.get('invertedBlendShapeFields', []):
        label = field.get('label', '')
        if label:
            inverted_fields[label] = field

    return blend_shape_fields, inverted_fields

def load_deformation_field_num_steps(field_file_path: str, config_dir: str) -> int:
    """
    Read num_steps from the deformation field file

    Parameters:
        field_file_path: Path to the transformed field file (relative paths allowed)
        config_dir: Configuration file directory

    Returns:
        int: value of num_steps, 1 if not readable
    """
    try:
        # Convert relative paths to absolute paths
        if not os.path.isabs(field_file_path):
            field_file_path = os.path.join(config_dir, field_file_path)

        if os.path.exists(field_file_path):
            field_data = np.load(field_file_path, allow_pickle=True)
            return int(field_data.get('num_steps', 1))
        else:
            print(f"Warning: Deformation field file not found: {field_file_path}")
            return 1
    except Exception as e:
        print(f"Warning: Failed to load num_steps from {field_file_path}: {e}")
        return 1

def process_single_blendshape_transition_set(current_settings: list, next_settings: list,
                                           label: str, source_label: str, blend_shape_groups: dict,
                                           blend_shape_fields: dict, inverted_blend_shape_fields: dict,
                                           current_config_dir: str, mask_bones: list = None) -> dict:
    """
    Handling transitions between single sets of BlendShape settings

    Parameters:
        current_settings: Current settings list
        next_settings: Next settings list
        label: Label name ('Basis' or specific blendShapeFields label)
        blend_shape_groups: BlendShapeGroups dictionary
        blend_shape_fields: Dictionary of BlendShapeFields
        inverted_blend_shape_fields: invertedBlendShapeFields dictionary
        current_config_dir: Directory of the current configuration file

    Returns:
        list: List of transition data
    """
    # Convert settings to dictionary format
    current_dict = {item['name']: item['value'] for item in current_settings}
    next_dict = {item['name']: item['value'] for item in next_settings}

    # Collect all BlendShape names
    all_blend_shapes = set(current_dict.keys()) | set(next_dict.keys())

    transitions = []

    processed_blend_shapes = set()

    for blend_shape_name in all_blend_shapes:

        if blend_shape_name in processed_blend_shapes:
            continue

        current_value = current_dict.get(blend_shape_name, 0.0)
        next_value = next_dict.get(blend_shape_name, 0.0)

        # Process only if the value changes
        if current_value != next_value:
            transition = {
                'label': label,
                'blend_shape_name': blend_shape_name,
                'from_value': current_value,
                'to_value': next_value,
                'operations': [],
            }

            # Special Handling in BlendShapeGroups
            group_processed = False
            for group_name, group_blend_shapes in blend_shape_groups.items():
                if blend_shape_name in group_blend_shapes:
                    # Find the current non-zero value within the group
                    current_non_zero = None
                    for group_blend_shape in group_blend_shapes:
                        if current_dict.get(group_blend_shape, 0.0) != 0.0:
                            current_non_zero = group_blend_shape
                            break

                    # Find the next non-zero value within the group
                    next_non_zero = None
                    for group_blend_shape in group_blend_shapes:
                        if next_dict.get(group_blend_shape, 0.0) != 0.0:
                            next_non_zero = group_blend_shape
                            break

                    # When different BlendShape within a group take positive values
                    if current_non_zero and next_non_zero and current_non_zero != next_non_zero:
                        # The operation to set the previous value to 0
                        field_file_path = inverted_blend_shape_fields[current_non_zero]['filePath']
                        num_steps = load_deformation_field_num_steps(field_file_path, current_config_dir)
                        current_value = current_dict.get(current_non_zero, 0.0)
                        from_step = int((1.0 - current_value) * num_steps + 0.5)
                        to_step = num_steps
                        transition['operations'].append({
                            'type': 'set_to_zero',
                            'blend_shape': current_non_zero,
                            'from_value': current_value,
                            'to_value': 0.0,
                            'file_path': os.path.join(current_config_dir, field_file_path),
                            'mask_bones': inverted_blend_shape_fields[current_non_zero]['maskBones'],
                            'num_steps': num_steps,
                            'from_step': from_step,
                            'to_step': to_step,
                            'field_type': 'inverted'
                        })
                        # The operation to set a new value next
                        field_file_path = blend_shape_fields[next_non_zero]['filePath']
                        num_steps = load_deformation_field_num_steps(field_file_path, current_config_dir)
                        next_value = next_dict.get(next_non_zero, 0.0)
                        from_step = 0
                        to_step = int(next_value * num_steps + 0.5)
                        transition['operations'].append({
                            'type': 'set_value',
                            'blend_shape': next_non_zero,
                            'from_value': 0.0,
                            'to_value': next_value,
                            'file_path': os.path.join(current_config_dir, field_file_path),
                            'mask_bones': blend_shape_fields[next_non_zero]['maskBones'],
                            'num_steps': num_steps,
                            'from_step': from_step,
                            'to_step': to_step,
                            'field_type': 'normal'
                        })
                        group_processed = True

                        processed_blend_shapes.add(current_non_zero)
                        processed_blend_shapes.add(next_non_zero)

                        break

            # If group processing was not performed, it is recorded as a simple value change
            if not group_processed:
                if current_value > next_value:
                    # Decrease in value
                    field_file_path = inverted_blend_shape_fields[blend_shape_name]['filePath']
                    num_steps = load_deformation_field_num_steps(field_file_path, current_config_dir)
                    from_step = int((1.0 - current_value) * num_steps + 0.5)
                    to_step = int((1.0 - next_value) * num_steps + 0.5)
                    transition['operations'].append({
                        'type': 'decrease',
                        'blend_shape': blend_shape_name,
                        'from_value': current_value,
                        'to_value': next_value,
                        'file_path': os.path.join(current_config_dir, field_file_path),
                        'mask_bones': inverted_blend_shape_fields[blend_shape_name]['maskBones'],
                        'num_steps': num_steps,
                        'from_step': from_step,
                        'to_step': to_step,
                        'field_type': 'inverted'
                    })
                else:
                    # Increase in value
                    field_file_path = blend_shape_fields[blend_shape_name]['filePath']
                    num_steps = load_deformation_field_num_steps(field_file_path, current_config_dir)
                    from_step = int(current_value * num_steps + 0.5)
                    to_step = int(next_value * num_steps + 0.5)
                    transition['operations'].append({
                        'type': 'increase',
                        'blend_shape': blend_shape_name,
                        'from_value': current_value,
                        'to_value': next_value,
                        'file_path': os.path.join(current_config_dir, field_file_path),
                        'mask_bones': blend_shape_fields[blend_shape_name]['maskBones'],
                        'num_steps': num_steps,
                        'from_step': from_step,
                        'to_step': to_step,
                        'field_type': 'normal'
                    })

            processed_blend_shapes.add(blend_shape_name)

            transitions.append(transition)
            print(f"  Transition detected [{label}]: {blend_shape_name} {current_value} -> {next_value}")

    transition_set = {
        'label': label,
        'source_label': source_label,  # Record the label of the selected targetBlendShapeSettings
        'mask_bones': mask_bones,
        'current_settings': current_settings,
        'next_settings': next_settings,
        'transitions': transitions
    }

    return transition_set

def apply_blendshape_operation_with_shape_key_name(target_obj, operation, target_shape_key_name, rigid_transformation=False):
    target_shape_key = target_obj.data.shape_keys.key_blocks.get(target_shape_key_name)
    if target_shape_key is None:
        print(f"Shape key {target_shape_key_name} not found")
        return

    original_shape_key_state = save_shape_key_state(target_obj)

    #Set all shape key values to 0
    for key_block in target_obj.data.shape_keys.key_blocks:
        key_block.value = 0.0

    target_shape_key.value = 1.0

    apply_blendshape_operation(target_obj, operation, target_shape_key, rigid_transformation)

    restore_shape_key_state(target_obj, original_shape_key_state)


def apply_blendshape_operation(target_obj, operation, target_shape_key, rigid_transformation=False):
    """
    Apply a single BlendShape transition to the specified object

    Parameters:
        target_obj: Target mesh object
        transition: Transition data
        target_shape_key: Name of the shape key to apply to (applies to Basis if None)
    """
    try:
        armature_obj = get_armature_from_modifier(target_obj)

        field_file_path = operation['file_path']
        num_steps = operation['num_steps']
        from_step = operation['from_step']
        to_step = operation['to_step']
        field_type = operation['field_type']

        print(f"Applying operation: {operation['blend_shape']} "
              f"({operation['from_value']} -> {operation['to_value']}) "
              f"steps {from_step}->{to_step}/{num_steps}")

        if not os.path.exists(field_file_path):
            print(f"Warning: Deformation field file not found: {field_file_path}")
            return

        # Calculate the transformation between steps
        if from_step == to_step:
            print("No step change required")
            return

        # Get the vertex position of the current object
        depsgraph = bpy.context.evaluated_depsgraph_get()
        eval_obj = target_obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data
        vertices = np.array([v.co for v in eval_mesh.vertices])
        num_vertices = len(vertices)

        # ③ Obtain the main Deformation Field information
        field_info = get_deformation_field_multi_step(field_file_path)
        all_field_points = field_info['all_field_points']
        all_delta_positions = field_info['all_delta_positions']
        deform_weights = field_info['field_weights']
        field_matrix = field_info['world_matrix']
        field_matrix_inv = field_info['world_matrix_inv']
        k_neighbors = field_info['kdtree_query_k']

        # If deform_weights is None, set the weight of all vertices to 1.0
        if deform_weights is None:
            deform_weights = np.ones(num_vertices)

        from_value = operation['from_value']
        to_value = operation['to_value']
        if field_type == 'inverted':
            from_value = 1.0 - from_value
            to_value = 1.0 - to_value
            if from_value < 0.00001:
                from_value = 0.0
            if to_value < 0.00001:
                to_value = 0.0
            if from_value > 0.99999:
                from_value = 1.0
            if to_value > 0.99999:
                to_value = 1.0

        # Use custom range processing
        world_positions = batch_process_vertices_with_custom_range(
            vertices,
            all_field_points,
            all_delta_positions,
            deform_weights,
            field_matrix,
            field_matrix_inv,
            target_obj.matrix_world,
            target_obj.matrix_world.inverted(),
            from_value,
            to_value,
            deform_weights=deform_weights,
            batch_size=1000,
            k=k_neighbors
        )

        if rigid_transformation:
            # Convert to a numpy array
            source_points = np.array([target_obj.matrix_world @ Vector(v) for v in vertices])
            s, R, t = calculate_optimal_similarity_transform(source_points, world_positions)
            # Calculate the result after applying the similarity transformation
            world_positions = apply_similarity_transform_to_points(source_points, s, R, t)

        # Apply results
        matrix_armature_inv_fallback = Matrix.Identity(4)
        for i in range(len(target_obj.data.vertices)):
            matrix_armature_inv = calculate_inverse_pose_matrix(target_obj, armature_obj, i)
            if matrix_armature_inv is None:
                matrix_armature_inv = matrix_armature_inv_fallback
            undeformed_world_pos = matrix_armature_inv @ Vector(world_positions[i])
            local_pos = target_obj.matrix_world.inverted() @ undeformed_world_pos
            target_shape_key.data[i].co = local_pos
            matrix_armature_inv_fallback = matrix_armature_inv

        return target_shape_key

    except Exception as e:
        print(f"Error applying operation {operation['blend_shape']}: {e}")
        import traceback
        traceback.print_exc()

def get_source_label(transition_label: str, config_data: Optional[dict]) -> Optional[str]:
    if config_data is None:
        return None
    transition_sets = config_data.get('blend_shape_transition_sets', [])
    if not transition_sets:
        return None
    for transition_set in transition_sets:
        if transition_set.get('label', '') == transition_label:
            return transition_set.get('source_label', '')
    return None

def calculate_blendshape_settings_difference(settings1: list, settings2: list,
                                           blend_shape_fields: dict,
                                           config_dir: str) -> float:
    """
    Calculate state differences between BlendShapeSettings

    Parameters:
        settings1: Initial BlendShapeSettings
        settings2: Next BlendShapeSettings
        blend_shape_fields: Dictionary of BlendShapeFields
        config_dir: Configuration file directory

    Returns:
        float: Amount of difference
    """
    # Convert settings to dictionary format
    dict1 = {item['name']: item['value'] for item in settings1}
    dict2 = {item['name']: item['value'] for item in settings2}

    # Collect all BlendShape names
    all_blend_shapes = set(dict1.keys()) | set(dict2.keys())

    total_difference = 0.0

    for blend_shape_name in all_blend_shapes:
        value1 = dict1.get(blend_shape_name, 0.0)
        value2 = dict2.get(blend_shape_name, 0.0)

        # Absolute value of the difference in values
        value_diff = abs(value1 - value2)

        if value_diff > 0.0 and blend_shape_name in blend_shape_fields:
            # Get the file path of the transformed data
            field_file_path = blend_shape_fields[blend_shape_name]['filePath']
            full_field_path = os.path.join(config_dir, field_file_path)

            try:
                # Load transformed data
                data = np.load(full_field_path, allow_pickle=True)
                delta_positions = data['all_delta_positions']

                total_max_displacement = 0.0
                for i in range(len(delta_positions)):
                    max_displacement = np.max(np.linalg.norm(delta_positions[i], axis=1))
                    total_max_displacement += max_displacement

                # if len(delta_positions) > 0:
                #     first_step_deltas = delta_positions[0]
                #     max_displacement = np.max(np.linalg.norm(first_step_deltas, axis=1))

                # Multiply the absolute value of the difference by the maximum displacement and add
                total_difference += value_diff * total_max_displacement

            except Exception as e:
                print(f"Warning: Could not load deformation data for {blend_shape_name}: {e}")
                # If data cannot be read, use the difference in values as-is
                total_difference += value_diff

    return total_difference


def find_best_matching_target_settings(source_label: str,
                                     all_target_settings: dict,
                                     all_target_mask_bones: dict,
                                     source_settings: list,
                                     blend_shape_fields: dict,
                                     config_dir: str,
                                     mask_bones: list = None) -> tuple:
    """
    Find the targetBlendShapeSettings closest to sourceBlendShapeSettings

    Parameters:
        all_target_settings: Dictionary of targetBlendShapeSettings per label
        all_target_mask_bones: Dictionary of maskBones per label
        source_settings: sourceBlendShapeSettings
        blend_shape_fields: Dictionary of BlendShapeFields
        config_dir: Configuration file directory
        mask_bones: Comparison target maskBones

    Returns:
        tuple: (best_label, best_target_settings)
    """
    best_label = None
    best_target_settings = None
    min_difference = float('inf')

    for label, target_settings in all_target_settings.items():
        # Check if there are common elements between mask_bones and all_target_mask_bones[label]
        if mask_bones is not None and label in all_target_mask_bones:
            target_mask_bones = all_target_mask_bones[label]
            if target_mask_bones is not None:
                # Convert to a set and check for common elements
                mask_bones_set = set(mask_bones)
                target_mask_bones_set = set(target_mask_bones)

                # If there are no common elements, skip
                if not mask_bones_set.intersection(target_mask_bones_set):
                    print(f"label: {label} - skip: no common mask_bones")
                    continue

        difference = calculate_blendshape_settings_difference(
            target_settings, source_settings, blend_shape_fields, config_dir
        )

        # Compare label and source_label after removing ___id
        label_without_id = label.split('___')[0] if '___' in label else label
        source_label_without_id = source_label.split('___')[0] if '___' in source_label else source_label

        # If label is source_label, divide the difference by 1.5 to increase priority
        if label_without_id == source_label_without_id:
            difference = difference / 1.5

        print(f"label: {label} difference: {difference}")

        if difference < min_difference:
            min_difference = difference
            best_label = label
            best_target_settings = target_settings

    return best_label, best_target_settings


def process_blendshape_transitions(current_config: dict, next_config: dict) -> None:
    """
    Detect differences in BlendShape settings between two consecutive Config files and generate transition data

    Parameters:
        current_config: Settings from the previous Config file
        next_config: Settings in the Config file that follow
    """
    try:
        blendshape_settings = next_config['config_data'].get('sourceBlendShapeSettings', [])
        current_config['next_blendshape_settings'] = blendshape_settings

        # Load avatar data from the baseAvatarDataPath in the previous Config
        current_base_avatar_data = load_avatar_data_for_blendshape_analysis(current_config['base_avatar_data'])

        # Get BlendShapeGroups and DeformationFields
        blend_shape_groups = get_blendshape_groups(current_base_avatar_data)
        blend_shape_fields, inverted_blend_shape_fields = get_deformation_fields_mapping(current_base_avatar_data)

        # Retrieve the configuration file directory
        current_config_dir = os.path.dirname(os.path.abspath(current_config['config_path']))

        print(f"Processing BlendShape transitions between {os.path.basename(current_config['config_path'])} and {os.path.basename(next_config['config_path'])}")

        all_transition_sets = []
        all_default_transition_sets = []

        # 1. Root-level processing
        # Collect all targetBlendShapeSettings
        all_target_settings = {}
        all_target_mask_bones = {}

        # Root-level targetBlendShapeSettings
        current_target_settings = current_config['config_data'].get('targetBlendShapeSettings', [])
        all_target_settings['Basis'] = current_target_settings
        all_target_mask_bones['Basis'] = None

        # targetBlendShapeSettings within blendShapeFields
        current_blend_shape_fields = current_config['config_data'].get('blendShapeFields', [])
        for field in current_blend_shape_fields:
            field_label = field.get('label', '')
            field_target_settings = field.get('targetBlendShapeSettings', [])
            all_target_settings[field_label] = field_target_settings
            all_target_mask_bones[field_label] = field.get('maskBones', [])

        # Find the targetBlendShapeSettings closest to the sourceBlendShapeSettings in next_config
        next_source_settings = next_config['config_data'].get('sourceBlendShapeSettings', [])
        if all_target_settings:
            best_label, best_target_settings = find_best_matching_target_settings(
                'Basis', all_target_settings, all_target_mask_bones, next_source_settings, blend_shape_fields, current_config_dir, None
            )

            print(f"  Best matching target for root level: '{best_label}'")

            # Create the optimal transition between targetBlendShapeSettings and sourceBlendShapeSettings
            basis_transitions = process_single_blendshape_transition_set(
                best_target_settings, next_source_settings, 'Basis', best_label,
                blend_shape_groups, blend_shape_fields, inverted_blend_shape_fields,
                current_config_dir
            )
            all_transition_sets.append(basis_transitions)

            basis_default_transitions = process_single_blendshape_transition_set(
                all_target_settings['Basis'], next_source_settings, 'Basis', 'Basis',
                blend_shape_groups, blend_shape_fields, inverted_blend_shape_fields,
                current_config_dir
            )
            all_default_transition_sets.append(basis_default_transitions)

        # 2. Processing within blendShapeFields
        next_blend_shape_fields = next_config['config_data'].get('blendShapeFields', [])

        for next_field in next_blend_shape_fields:
            next_field_source_label = next_field.get('sourceLabel', '')
            next_field_source_settings = next_field.get('sourceBlendShapeSettings', [])
            next_field_mask_bones = next_field.get('maskBones', [])

            if all_target_settings:
                # Find the optimal targetBlendShapeSettings
                best_label, best_target_settings = find_best_matching_target_settings(
                    next_field_source_label, all_target_settings, all_target_mask_bones, next_field_source_settings, blend_shape_fields, current_config_dir, next_field_mask_bones
                )

                print(f"  Best matching target for field '{next_field_source_label}': '{best_label}'")

                # Create a transition
                field_transitions = process_single_blendshape_transition_set(
                    best_target_settings, next_field_source_settings, next_field_source_label, best_label,
                    blend_shape_groups, blend_shape_fields, inverted_blend_shape_fields,
                    current_config_dir,
                    next_field_mask_bones
                )
                all_transition_sets.append(field_transitions)

                default_target_setting = None
                if next_field_source_label in all_target_settings.keys():
                    default_target_setting = all_target_settings[next_field_source_label]
                if default_target_setting is not None:
                    field_default_transitions = process_single_blendshape_transition_set(
                        default_target_setting, next_field_source_settings, next_field_source_label, next_field_source_label,
                        blend_shape_groups, blend_shape_fields, inverted_blend_shape_fields,
                        current_config_dir,
                        next_field_mask_bones
                    )
                    all_default_transition_sets.append(field_default_transitions)

        # Insert transition data into the following config object
        current_config['config_data']['blend_shape_transition_sets'] = all_transition_sets
        current_config['config_data']['blend_shape_default_transition_sets'] = all_default_transition_sets

        print(f"  Total transition sets: {len(all_transition_sets)}")
        print(f"  Total default transition sets: {len(all_default_transition_sets)}")

    except Exception as e:
        print(f"Error processing BlendShape transitions: {e}")
        import traceback
        traceback.print_exc()

def parse_args():
    parser = argparse.ArgumentParser()

    # Existing arguments
    parser.add_argument('--input', required=True, help='Input clothing FBX file path')
    parser.add_argument('--output', required=True, help='Output FBX file path')
    parser.add_argument('--base', required=True, help='Base Blender file path')
    parser.add_argument('--base-fbx', required=True, help='Comma-separated list of base avatar FBX file paths')
    parser.add_argument('--config', required=True, help='Comma-separated list of config file paths')
    parser.add_argument('--hips-position', type=str, help='Target Hips bone world position (x,y,z format)')
    parser.add_argument('--blend-shapes', type=str, help='Comma-separated list of blend shape labels to apply')
    parser.add_argument('--cloth-metadata', type=str, help='Path to cloth metadata JSON file')
    parser.add_argument('--mesh-material-data', type=str, help='Path to mesh material data JSON file')
    parser.add_argument('--init-pose', required=True, help='Initial pose data JSON file path')
    parser.add_argument('--target-meshes', required=False, help='Comma-separated list of mesh names to process')
    parser.add_argument('--no-subdivision', action='store_true', help='Disable subdivision during DeformationField deformation')
    parser.add_argument('--no-triangle', action='store_true', help='Disable mesh triangulation')
    parser.add_argument('--blend-shape-values', type=str, help='Comma-separated list of float values for blend shape intensities')
    parser.add_argument('--blend-shape-mappings', type=str, help='Semicolon-separated mappings of label,customName pairs')
    parser.add_argument('--name-conv', type=str, help='Path to bone name conversion JSON file')
    parser.add_argument('--mesh-renderers', type=str, help='Semicolon-separated list of meshObject,parentObject pairs')

    print(sys.argv)

    # Get all args after "--"
    argv = sys.argv
    if "--" not in argv:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args(argv[argv.index("--") + 1:])

    # Parse comma-separated base-fbx and config paths
    base_fbx_paths = [path.strip() for path in args.base_fbx.split(',')]
    config_paths = [path.strip() for path in args.config.split(',')]

    # Validate that base-fbx and config have the same number of entries
    if len(base_fbx_paths) != len(config_paths):
        print(f"Error: Number of base-fbx files ({len(base_fbx_paths)}) must match number of config files ({len(config_paths)})")
        sys.exit(1)

    # Validate basic file paths
    required_paths = [
        args.input, args.base,
        args.init_pose
    ]
    for path in required_paths:
        if not os.path.exists(path):
            print(f"Error: File not found: {path}")
            sys.exit(1)

    # Validate all base-fbx files exist
    for path in base_fbx_paths:
        if not os.path.exists(path):
            print(f"Error: Base FBX file not found: {path}")
            sys.exit(1)

    # Validate all config files exist
    for path in config_paths:
        if not os.path.exists(path):
            print(f"Error: Config file not found: {path}")
            sys.exit(1)

    # Process each config file and create configuration pairs
    config_pairs = []
    for i, (base_fbx_path, config_path) in enumerate(zip(base_fbx_paths, config_paths)):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)

            # Add ___id to duplicate label and sourceLabel in blendShapeFields
            if 'blendShapeFields' in config_data:
                blend_shape_fields = config_data['blendShapeFields']

                # Check for duplicate label and append an ___id
                label_counts = {}
                for field in blend_shape_fields:
                    label = field.get('label', '')
                    if label:
                        label_counts[label] = label_counts.get(label, 0) + 1

                label_ids = {}
                for field in blend_shape_fields:
                    label = field.get('label', '')
                    if label and label_counts[label] > 1:
                        current_id = label_ids.get(label, 0)
                        field['label'] = f"{label}___{current_id}"
                        label_ids[label] = current_id + 1

                # Check for duplicate sourceLabel and append ___id
                source_label_counts = {}
                for field in blend_shape_fields:
                    source_label = field.get('sourceLabel', '')
                    if source_label:
                        source_label_counts[source_label] = source_label_counts.get(source_label, 0) + 1

                source_label_ids = {}
                for field in blend_shape_fields:
                    source_label = field.get('sourceLabel', '')
                    if source_label and source_label_counts[source_label] > 1:
                        current_id = source_label_ids.get(source_label, 0)
                        field['sourceLabel'] = f"{source_label}___{current_id}"
                        source_label_ids[source_label] = current_id + 1

            # Get config file directory
            config_dir = os.path.dirname(os.path.abspath(config_path))

            # Extract and resolve avatar data paths
            pose_data_path = config_data.get('poseDataPath')
            field_data_path = config_data.get('fieldDataPath')
            base_avatar_data_path = config_data.get('baseAvatarDataPath')
            clothing_avatar_data_path = config_data.get('clothingAvatarDataPath')

            if not pose_data_path:
                print(f"Error: poseDataPath not found in config file: {config_path}")
                sys.exit(1)
            if not field_data_path:
                print(f"Error: fieldDataPath not found in config file: {config_path}")
                sys.exit(1)
            if not base_avatar_data_path:
                print(f"Error: baseAvatarDataPath not found in config file: {config_path}")
                sys.exit(1)
            if not clothing_avatar_data_path:
                print(f"Error: clothingAvatarDataPath not found in config file: {config_path}")
                sys.exit(1)

            # Convert relative paths to absolute paths
            if not os.path.isabs(pose_data_path):
                pose_data_path = os.path.join(config_dir, pose_data_path)
            if not os.path.isabs(field_data_path):
                field_data_path = os.path.join(config_dir, field_data_path)
            if not os.path.isabs(base_avatar_data_path):
                base_avatar_data_path = os.path.join(config_dir, base_avatar_data_path)
            if not os.path.isabs(clothing_avatar_data_path):
                clothing_avatar_data_path = os.path.join(config_dir, clothing_avatar_data_path)

            # Validate avatar data paths
            if not os.path.exists(pose_data_path):
                print(f"Error: Pose data file not found: {pose_data_path} (from config {config_path})")
                sys.exit(1)
            if not os.path.exists(field_data_path):
                print(f"Error: Field data file not found: {field_data_path} (from config {config_path})")
                sys.exit(1)
            if not os.path.exists(base_avatar_data_path):
                print(f"Error: Base avatar data file not found: {base_avatar_data_path} (from config {config_path})")
                sys.exit(1)
            if not os.path.exists(clothing_avatar_data_path):
                print(f"Error: Clothing avatar data file not found: {clothing_avatar_data_path} (from config {config_path})")
                sys.exit(1)

            hips_position = None
            target_meshes = None
            init_pose = None
            blend_shapes = None
            blend_shape_values = None
            blend_shape_mappings = None
            mesh_renderers = None
            input_clothing_fbx_path = args.output
            if i == 0:
                if args.hips_position:
                    x, y, z = map(float, args.hips_position.split(','))
                    hips_position = Vector((x, y, z))
                target_meshes = args.target_meshes
                init_pose = args.init_pose
                blend_shapes = args.blend_shapes
                # Parse blend shape values if provided
                if args.blend_shape_values:
                    try:
                        blend_shape_values = [float(v.strip()) for v in args.blend_shape_values.split(',')]
                    except ValueError as e:
                        print(f"Error: Invalid blend shape values format: {e}")
                        sys.exit(1)
                # Parse blend shape mappings if provided
                if args.blend_shape_mappings:
                    try:
                        blend_shape_mappings = {}
                        pairs = args.blend_shape_mappings.split(';')
                        for pair in pairs:
                            if pair.strip():
                                label, custom_name = pair.split(',', 1)
                                blend_shape_mappings[label.strip()] = custom_name.strip()
                    except ValueError as e:
                        print(f"Error: Invalid blend shape mappings format: {e}")
                        sys.exit(1)
                # Parse mesh renderers if provided
                if args.mesh_renderers:
                    try:
                        mesh_renderers = {}
                        pairs = args.mesh_renderers.split(';')
                        for pair in pairs:
                            if pair.strip():
                                mesh_name, parent_name = pair.split(',', 1)
                                mesh_renderers[mesh_name.strip()] = parent_name.strip()
                        print(f"Parsed mesh renderers: {mesh_renderers}")
                    except ValueError as e:
                        print(f"Error: Invalid mesh renderers format: {e}")
                        sys.exit(1)
                input_clothing_fbx_path = args.input

            skip_blend_shape_generation = True
            if i == len(config_paths) - 1:
                skip_blend_shape_generation = False

            do_not_use_base_pose = config_data.get('doNotUseBasePose', 0)

            # Create configuration pair
            config_pair = {
                'base_fbx': base_fbx_path,
                'config_path': config_path,
                'config_data': config_data,
                'pose_data': pose_data_path,
                'field_data': field_data_path,
                'base_avatar_data': base_avatar_data_path,
                'clothing_avatar_data': clothing_avatar_data_path,
                'hips_position': hips_position,
                'target_meshes': target_meshes,
                'init_pose': init_pose,
                'blend_shapes': blend_shapes,
                'blend_shape_values': blend_shape_values,
                'blend_shape_mappings': blend_shape_mappings,
                'mesh_renderers': mesh_renderers,
                'input_clothing_fbx_path': input_clothing_fbx_path,
                'skip_blend_shape_generation': skip_blend_shape_generation,
                'do_not_use_base_pose': do_not_use_base_pose
            }
            config_pairs.append(config_pair)

        except json.JSONDecodeError as e:
            print(f"Error: Invalid JSON in config file {config_path}: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"Error reading config file {config_path}: {e}")
            sys.exit(1)

    # Process BlendShape transitions for consecutive config pairs
    if len(config_pairs) >= 2:
        for i in range(len(config_pairs) - 1):
            process_blendshape_transitions(config_pairs[i], config_pairs[i + 1])
        config_pairs[len(config_pairs) - 1]['next_blendshape_settings'] = config_pairs[len(config_pairs) - 1]['config_data'].get('targetBlendShapeSettings', [])

    # Store configuration pairs in args for later use
    args.config_pairs = config_pairs

    # Parse hips position if provided
    if args.hips_position:
        try:
            x, y, z = map(float, args.hips_position.split(','))
            args.hips_position = Vector((x, y, z))
        except:
            print("Error: Invalid hips position format. Use x,y,z")
            sys.exit(1)

    return args

def load_base_file(filepath: str) -> None:
    """Load the base Blender file containing the character model."""
    try:
        bpy.ops.wm.open_mainfile(filepath=filepath)
    except Exception as e:
        raise Exception(f"Failed to load base file: {str(e)}")

def import_fbx(filepath: str) -> None:
    """Import an FBX file."""
    try:
        bpy.ops.import_scene.fbx(
            filepath=filepath,
            use_anim=False  # Disable animation loading
        )
    except Exception as e:
        raise Exception(f"Failed to import FBX: {str(e)}")

def get_imported_armature() -> Optional[bpy.types.Object]:
    """Get the most recently imported armature object."""
    for obj in bpy.data.objects:
        if obj.type == 'ARMATURE' and obj.name != 'Armature.BaseAvatar':
            return obj
    return None

def load_avatar_data(filepath: str) -> dict:
    """Load and parse avatar data from JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        raise Exception(f"Failed to load avatar data: {str(e)}")

def import_base_fbx(filepath: str, automatic_bone_orientation: bool = False) -> None:
    """Import base avatar FBX file."""
    try:
        bpy.ops.import_scene.fbx(
            filepath=filepath,
            use_anim=False,  # Disable animation loading
            automatic_bone_orientation=automatic_bone_orientation
        )
    except Exception as e:
        raise Exception(f"Failed to import base FBX: {str(e)}")

def calculate_vertices_world(mesh_obj):
    """
    Get the world coordinates of the vertices of the deformed mesh

    Args:
        mesh_obj: Mesh object
    Returns:
        vertices_world: numpy array of world coordinates
    """
    # Acquire the deformed mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    evaluated_obj = mesh_obj.evaluated_get(depsgraph)
    evaluated_mesh = evaluated_obj.data

    # Convert to world coordinates (using transformed vertex positions)
    vertices_world = np.array([evaluated_obj.matrix_world @ v.co for v in evaluated_mesh.vertices])

    return vertices_world

def find_closest_vertices_brute_force(positions, vertices_world, max_distance=0.0001):
    """
    Brute-force search for the closest vertex to multiple positions

    Args:
        positions: List of positions to search (world coordinates)
        vertices_world: A list of the mesh's vertices in world coordinates
        max_distance: Maximum allowable distance
    Returns:
        Dict[int, float]: A dictionary with vertex indices as keys and distances as values
    """
    valid_mappings = {}

    # For each search position
    for i, search_pos in enumerate(positions):
        min_distance = float('inf')
        closest_idx = None

        # Calculate all mesh vertices and distances
        for vertex_idx, vertex_pos in enumerate(vertices_world):
            # Calculate Euclidean distance
            distance = ((search_pos[0] - vertex_pos[0])**2 +
                       (search_pos[1] - vertex_pos[1])**2 +
                       (search_pos[2] - vertex_pos[2])**2)**0.5

            # If a closer vertex is found, update
            if distance < min_distance:
                min_distance = distance
                closest_idx = vertex_idx

        # Add mapping only if within the maximum distance
        if closest_idx is not None and min_distance < max_distance:
            valid_mappings[i] = closest_idx

    return valid_mappings

def load_mesh_material_data(filepath):
    """
    Load mesh material data and assign materials to Blender meshes

    Args:
        filepath: JSON file path for mesh material data
    """
    if not filepath or not os.path.exists(filepath):
        print("Warning: Mesh material data file not found or not specified")
        return

    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
            print(f"Loaded mesh material data from: {filepath}")

            for mesh_data in data.get('meshMaterials', []):
                mesh_name = mesh_data['meshName']

                # Search for mesh objects in Blender
                mesh_obj = None
                for obj in bpy.data.objects:
                    if obj.type == 'MESH' and obj.name == mesh_name:
                        mesh_obj = obj
                        break

                if not mesh_obj:
                    print(f"Warning: Mesh {mesh_name} not found in Blender scene")
                    continue

                print(f"Processing mesh: {mesh_name}")

                # Process each submesh
                for sub_mesh_idx, sub_mesh_data in enumerate(mesh_data['subMeshes']):
                    material_name = sub_mesh_data['materialName']
                    faces_data = sub_mesh_data['faces']

                    if not faces_data:
                        continue

                    # Create or obtain material
                    material = bpy.data.materials.get(material_name)
                    if not material:
                        material = bpy.data.materials.new(name=material_name)
                        # Default material settings
                        material.use_nodes = True
                        print(f"Created material: {material_name}")

                    # Identify the corresponding material index from the surface and swap the material in that slot
                    material_index = find_material_index_from_faces(mesh_obj, faces_data)
                    if material_index is not None:
                        # If the mesh lacks material slots, add them
                        while len(mesh_obj.data.materials) <= material_index:
                            mesh_obj.data.materials.append(None)

                        # Swap the corresponding material slot
                        mesh_obj.data.materials[material_index] = material
                        print(f"Replaced material at index {material_index} with {material_name}")
                    else:
                        print(f"Warning: Could not find matching faces for material {material_name}")

    except Exception as e:
        print(f"Error loading mesh material data: {e}")

def find_material_index_from_faces(mesh_obj, faces_data):
    """
    Identify the corresponding face based on its vertex coordinates, and among the material indices of all matched faces,
    Return the most frequent item

    Args:
        mesh_obj: Blender mesh object
        faces_data: A list of face data from Unity

    Returns:
        int: The most frequently occurring material index (None if not found)
    """
    from collections import Counter

    # Ensure you are in Object Mode
    bpy.context.view_layer.objects.active = mesh_obj
    if bpy.context.object.mode != 'OBJECT':
        bpy.ops.object.mode_set(mode='OBJECT')

    # Update scene evaluation to the latest state
    depsgraph = bpy.context.evaluated_depsgraph_get()
    depsgraph.update()
    mesh = mesh_obj.data

    # Obtain the world transformation matrix
    world_matrix = mesh_obj.matrix_world

    tolerance = 0.00001  # Coordinate tolerance

    # Record the material index of the matched surface
    matched_material_indices = []

    for face_data in faces_data:
        # Convert Unity coordinates to Blender coordinates
        unity_vertices = face_data['vertices']
        blender_vertices = []

        for unity_vertex in unity_vertices:
            # Unity → Blender Coordinate Conversion
            blender_vertex = mathutils.Vector((
                -unity_vertex['x'],  # X-axis reversal
                -unity_vertex['z'],  # Y → Z
                unity_vertex['y']    # Z → Y
            ))
            blender_vertices.append(blender_vertex)

        # Search for matching faces in Blender
        for polygon in mesh.polygons:
            if len(polygon.vertices) == 3:  # Triangular Surface Processing
                # Get the world coordinates of the face's vertices
                face_world_verts = []
                for vert_idx in polygon.vertices:
                    vertex = mesh.vertices[vert_idx]
                    world_vert = world_matrix @ vertex.co
                    face_world_verts.append(world_vert)

                # Check whether all three vertices are close to each other
                match = True
                for i in range(3):
                    closest_dist = min(
                        (face_world_verts[j] - blender_vertices[i]).length
                        for j in range(3)
                    )
                    if closest_dist > tolerance:
                        match = False
                        break

                if match:
                    # Record the material index of the matched surface
                    material_index = polygon.material_index
                    matched_material_indices.append(material_index)
                    print(f"Found matching triangular face with material index: {material_index}")

            elif len(polygon.vertices) >= 4:  # Polygon Surface Processing
                num_vertices = len(polygon.vertices)
                # Get the world coordinates of the face's vertices
                face_world_verts = []
                for vert_idx in polygon.vertices:
                    vertex = mesh.vertices[vert_idx]
                    world_vert = world_matrix @ vertex.co
                    face_world_verts.append(world_vert)

                # Check all combinations of selecting three out of four vertices
                from itertools import combinations

                for face_vert_combo in combinations(range(num_vertices), 3):
                    # Check if this combination matches
                    match = True
                    for i in range(3):
                        closest_dist = min(
                            (face_world_verts[face_vert_combo[j]] - blender_vertices[i]).length
                            for j in range(3)
                        )
                        if closest_dist > tolerance:
                            match = False
                            break

                    if match:
                        # A matching combination has been found
                        material_index = polygon.material_index
                        matched_material_indices.append(material_index)
                        print(f"Found matching face (num_vertices: {num_vertices}) with material index: {material_index}")
                        break  # Avoid counting multiple combinations of the same face twice

    # If no matching surface is found
    if not matched_material_indices:
        return None

    # Obtain the most frequently occurring material index
    material_counter = Counter(matched_material_indices)
    most_common_material = material_counter.most_common(1)[0]
    most_common_index = most_common_material[0]
    most_common_count = most_common_material[1]

    print(f"Material index frequencies: {dict(material_counter)}")
    print(f"Most common material index: {most_common_index} (appears {most_common_count} times)")

    return most_common_index

def load_cloth_metadata(filepath):
    """
    Load Cloth metadata based on the transformed world coordinates

    Returns:
        Tuple[dict, dict]: (Metadata mapping, Unity vertex index to Blender vertex index mapping)
    """
    if not filepath or not os.path.exists(filepath):
        return {}, {}

    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
            metadata_by_mesh = {}
            vertex_index_mapping = {}  # Unity Vertex Index -> Blender Vertex Index Mapping

            # Update scene evaluation to the latest state
            depsgraph = bpy.context.evaluated_depsgraph_get()
            depsgraph.update()

            for metadata in data.get('clothMetadata', []):
                mesh_name = metadata['meshName']
                mesh_obj = None

                # Search for mesh
                for obj in bpy.data.objects:
                    if obj.type == 'MESH' and obj.name == mesh_name:
                        mesh_obj = obj
                        break

                if not mesh_obj:
                    print(f"Warning: Mesh {mesh_name} not found")
                    continue

                # Convert Unity world coordinates to Blender world coordinates
                unity_positions = []
                max_distances = []
                for i, vertex_data in enumerate(metadata.get('vertexData', [])):
                    pos = vertex_data['position']
                    # Conversion from Unity Coordinate System to Blender Coordinate System
                    unity_positions.append([
                        -pos['x'],      # Unity X → Blender X
                        -pos['z'],      # Unity Y → Blender Z
                        pos['y']       # Unity Z → Blender Y
                    ])
                    max_distances.append(vertex_data['maxDistance'])

                # Search for the nearest vertex in bulk
                vertices_world = calculate_vertices_world(mesh_obj)
                vertex_mappings = find_closest_vertices_brute_force(
                    unity_positions,
                    vertices_world,
                    max_distance=0.0005
                )

                # Convert the results to a mapping of vertex indices and maxDistance
                vertex_max_distances = {}
                mesh_vertex_mapping = {}  # Unity -> Blender mapping for this mesh

                for unity_idx, blender_idx in sorted(vertex_mappings.items()):
                    if unity_idx is not None and blender_idx is not None:
                        vertex_max_distances[str(blender_idx)] = max_distances[unity_idx]
                        mesh_vertex_mapping[unity_idx] = blender_idx

                metadata_by_mesh[mesh_name] = vertex_max_distances
                vertex_index_mapping[mesh_name] = mesh_vertex_mapping

                print(f"Processed {len(vertex_max_distances)} vertices for mesh {mesh_name}")
                print(f"Original vertex count: {len(metadata['vertexData'])}")
                print(f"Original unity position count: {len(unity_positions)}")
                print(f"Mapped vertex count: {len(vertex_max_distances)}")

                # Identify vertices that could not be mapped
                mapped_indices = set(int(idx) for idx in vertex_max_distances.keys())
                unmapped_indices = set(range(len(vertices_world))) - mapped_indices

                if unmapped_indices:
                    print(f"Warning: Could not map {len(unmapped_indices)} vertices")

                    # Create a vertex group for debugging
                    debug_group_name = "DEBUG_UnmappedVertices"
                    if debug_group_name in mesh_obj.vertex_groups:
                        mesh_obj.vertex_groups.remove(mesh_obj.vertex_groups[debug_group_name])
                    debug_group = mesh_obj.vertex_groups.new(name=debug_group_name)

                    # Add unmapped vertices to the group
                    for idx in unmapped_indices:
                        debug_group.add([idx], 1.0, 'REPLACE')

                    print(f"Created vertex group '{debug_group_name}' with {len(unmapped_indices)} vertices")

                    # Debug information
                    print("First 5 unmapped vertices world positions:")
                    for idx in list(unmapped_indices)[:5]:
                        print(f"Vertex {idx}: {vertices_world[idx]}")

            return metadata_by_mesh, vertex_index_mapping

    except Exception as e:
        print(f"Failed to load cloth metadata: {e}")
        import traceback
        traceback.print_exc()
        return {}, {}

def rename_base_objects(mesh_obj: bpy.types.Object, armature_obj: bpy.types.Object) -> tuple:
    """Rename base mesh and armature to specific names."""
    # Store original names for reference
    original_mesh_name = mesh_obj.name
    original_armature_name = armature_obj.name

    # Rename mesh to Body.BaseAvatar
    mesh_obj.name = "Body.BaseAvatar"
    mesh_obj.data.name = "Body.BaseAvatar_Mesh"

    # Rename armature to Armature.BaseAvatar
    armature_obj.name = "Armature.BaseAvatar"
    armature_obj.data.name = "Armature.BaseAvatar_Data"

    print(f"Renamed base objects: {original_mesh_name} -> {mesh_obj.name}, {original_armature_name} -> {armature_obj.name}")
    return mesh_obj, armature_obj

def cleanup_base_objects(mesh_name: str) -> tuple:
    """Delete all objects except the specified mesh and its armature."""

    original_mode = bpy.context.object.mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Find the mesh and its armature
    target_mesh = None
    target_armature = None

    for obj in bpy.data.objects:
        if obj.type == 'MESH' and obj.name == mesh_name:
            target_mesh = obj
            # Find associated armature through modifiers
            for modifier in obj.modifiers:
                if modifier.type == 'ARMATURE':
                    target_armature = modifier.object
                    break

    if not target_mesh:
        raise Exception(f"Mesh '{mesh_name}' not found")

    if target_armature and target_armature.parent:
        original_active = bpy.context.view_layer.objects.active
        bpy.context.view_layer.objects.active = target_armature
        bpy.ops.object.parent_clear(type='CLEAR_KEEP_TRANSFORM')
        bpy.context.view_layer.objects.active = original_active

    # Delete all other objects
    for obj in bpy.data.objects[:]:  # Create a copy of the list to avoid modification during iteration
        if obj != target_mesh and obj != target_armature:
            bpy.data.objects.remove(obj, do_unlink=True)

    bpy.ops.object.mode_set(mode=original_mode)

    # Rename objects to specified names
    return rename_base_objects(target_mesh, target_armature)

def apply_blendshape_values(mesh_obj: bpy.types.Object, blendshapes: list) -> None:
    """Apply blendshape values from avatar data."""
    if not mesh_obj.data.shape_keys:
        return

    # Create a mapping of shape key names
    shape_keys = mesh_obj.data.shape_keys.key_blocks

    # Apply values
    for blendshape in blendshapes:
        shape_key_name = blendshape["name"]
        if shape_key_name in shape_keys:
            # Set value to 1% of the specified value
            shape_keys[shape_key_name].value = blendshape["value"] * 0.01

def merge_auxiliary_bone_weights(mesh_obj: bpy.types.Object, auxiliary_bones_data: list) -> None:
    """
    Merge auxiliary bone weights into their parent humanoid bone weights

    Parameters:
        mesh_obj: Mesh object to process
        auxiliary_bones_data: List of auxiliary bones data from avatar data
    """
    if not mesh_obj.vertex_groups:
        return

    # Process each auxiliary bone set
    for aux_bone_set in auxiliary_bones_data:
        humanoid_bone = aux_bone_set["humanoidBoneName"]
        auxiliary_bones = aux_bone_set["auxiliaryBones"]

        # Find the humanoid bone vertex group
        humanoid_group = None
        for bone in aux_bone_set.get("boneName", [humanoid_bone]):
            humanoid_group = mesh_obj.vertex_groups.get(bone)
            if humanoid_group:
                break

        if not humanoid_group:
            print(f"Warning: Humanoid bone group '{humanoid_bone}' not found in {mesh_obj.name}")
            continue

        # Process each auxiliary bone
        for aux_bone_name in auxiliary_bones:
            aux_group = mesh_obj.vertex_groups.get(aux_bone_name)
            if not aux_group:
                continue

            # For each vertex, add auxiliary bone weight to humanoid bone
            for vert in mesh_obj.data.vertices:
                aux_weight = 0
                for group in vert.groups:
                    if group.group == aux_group.index:
                        aux_weight = group.weight
                        break

                if aux_weight > 0:
                    # Add weight to humanoid bone group
                    humanoid_group.add([vert.index], aux_weight, 'ADD')

            # Remove auxiliary bone vertex group
            mesh_obj.vertex_groups.remove(aux_group)
            print(f"Merged weights from {aux_bone_name} to {humanoid_bone} in {mesh_obj.name}")

def merge_humanoid_bone_weights(mesh_obj: bpy.types.Object, avatar_data: dict) -> None:
    """
    Process humanoid and auxiliary bone weights for a mesh

    Parameters:
        mesh_obj: Mesh object to process
        avatar_data: Avatar data containing bone mapping information
    """
    # Create mapping from boneName to humanoidBoneName
    bone_mapping = {}
    for bone_map in avatar_data.get("humanoidBones", []):
        if "boneName" in bone_map and "humanoidBoneName" in bone_map:
            bone_mapping[bone_map["boneName"]] = bone_map["humanoidBoneName"]

    # Process auxiliary bones if they exist
    auxiliary_bones = avatar_data.get("auxiliaryBones", [])
    if auxiliary_bones:
        merge_auxiliary_bone_weights(mesh_obj, auxiliary_bones)

def set_humanoid_bone_inherit_scale(armature_obj: bpy.types.Object, avatar_data: dict) -> None:

    print("set_humanoid_bone_inherit_scale")

    # Retrieve Humanoid bone information
    bone_parents, humanoid_to_bone, bone_to_humanoid = get_humanoid_bone_hierarchy(avatar_data)

    # Switch to EditMode
    bpy.context.view_layer.objects.active = armature_obj
    bpy.ops.object.mode_set(mode='EDIT')

    modified_count = 0

    # Set Inherit Scale for each Humanoid bone
    for humanoid_bone_name, bone_name in humanoid_to_bone.items():
        if bone_name in armature_obj.data.edit_bones:
            edit_bone = armature_obj.data.edit_bones[bone_name]

            # Set only when Inherit Scale is not None
            if edit_bone.inherit_scale != 'NONE':
                # Set the humanoid bones for UpperChest, Chest, Toes, and Feet to Full
                if 'Breast' in humanoid_bone_name or 'UpperChest' in humanoid_bone_name or 'Toe' in humanoid_bone_name or ('Foot' in humanoid_bone_name and ('Index' in humanoid_bone_name or 'Little' in humanoid_bone_name or 'Middle' in humanoid_bone_name or 'Ring' in humanoid_bone_name or 'Thumb' in humanoid_bone_name)):
                    edit_bone.inherit_scale = 'FULL'
                else:
                    edit_bone.inherit_scale = 'AVERAGE'
                modified_count += 1

    # Return to ObjectMode
    bpy.ops.object.mode_set(mode='OBJECT')

    if modified_count > 0:
        print(f"Set the Inherit Scale of {modified_count} Humanoid bones to Average")
    else:
        print("There were no bones to modify")

def process_base_avatar(base_fbx_path: str, avatar_data_path: str) -> tuple:
    """Process base avatar according to avatar data."""
    # Load avatar data
    avatar_data = load_avatar_data(avatar_data_path)

    # Import base FBX
    automatic_bone_orientation_int = avatar_data.get("enableAutomaticBoneOrientation", 0)
    if automatic_bone_orientation_int == 1:
        import_base_fbx(base_fbx_path, True)
    else:
        import_base_fbx(base_fbx_path, False)

    # Clean up objects and get references
    mesh_obj, armature_obj = cleanup_base_objects(avatar_data["meshName"])

    set_humanoid_bone_inherit_scale(armature_obj, avatar_data)

    # Apply blendshape values if they exist
    if mesh_obj and "blendshapes" in avatar_data:
        apply_blendshape_values(mesh_obj, avatar_data["blendshapes"])

    return mesh_obj, armature_obj, avatar_data

def adjust_armature_hips_position(armature_obj: bpy.types.Object, target_position: Vector, clothing_avatar_data: dict) -> None:
    """
    Move the Hips bone of the armature to the specified position.
    Maintain the child object's position in world space.
    If the target position and current position are the same, skip processing.

    Parameters:
        armature_obj: Armature Object
        target_position: World coordinates of the target Hips bone
        clothing_avatar_data: Clothing avatar data
    """
    if not armature_obj or armature_obj.type != 'ARMATURE':
        return

    # Get the name of the Hips bone
    hips_bone_name = None
    for bone_map in clothing_avatar_data.get("humanoidBones", []):
        if bone_map["humanoidBoneName"] == "Hips":
            hips_bone_name = bone_map["boneName"]
            break

    if not hips_bone_name:
        print("Warning: Hips bone not found in avatar data")
        return

    # Get the world coordinates of the current Hips bone
    pose_bone = armature_obj.pose.bones.get(hips_bone_name)
    if not pose_bone:
        print(f"Warning: Bone {hips_bone_name} not found in armature")
        return

    current_position = armature_obj.matrix_world @ pose_bone.head

    # Calculate the difference between the current position and the target position
    offset = target_position - current_position
    print(f"Hip Offset: {offset}")

    # If the positional difference is sufficiently small, skip processing
    if offset.length < 0.0001:  # Differences less than 0.1mm are ignored
        print("Hips position is already at target position, skipping adjustment")
        return

    # Save the current active object and mode
    current_active = bpy.context.active_object
    current_mode = current_active.mode if current_active else 'OBJECT'

    # Switch to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Get the child object of the armature
    children = []
    for child in bpy.data.objects:
        if child.parent == armature_obj:
            # Save child object information
            children.append(child)

    # Terminate the parent-child relationship
    for child in children:
        # Deselect other objects
        bpy.ops.object.select_all(action='DESELECT')

        # Select the child object to make it active
        child.select_set(True)
        bpy.context.view_layer.objects.active = child

        # Terminate the parent-child relationship
        bpy.ops.object.parent_clear(type='CLEAR_KEEP_TRANSFORM')

    # Move the armature
    armature_obj.location += offset

    # Restore parent-child relationships for child objects
    for child in children:
        # Deselect other objects
        bpy.ops.object.select_all(action='DESELECT')

        # Select the armature and child objects
        armature_obj.select_set(True)
        child.select_set(True)
        bpy.context.view_layer.objects.active = armature_obj

        # Set parent-child relationship
        bpy.ops.object.parent_set(type='OBJECT', keep_transform=True)

    # Restore the original active object and selection state
    bpy.ops.object.select_all(action='DESELECT')
    if current_active:
        current_active.select_set(True)
        bpy.context.view_layer.objects.active = current_active
        if current_mode != 'OBJECT':
            bpy.ops.object.mode_set(mode=current_mode)

    # Refresh View
    bpy.context.view_layer.update()

def process_clothing_avatar(input_fbx, clothing_avatar_data_path, hips_position=None, target_meshes=None, mesh_renderers=None):
    """Process clothing avatar."""

    original_active = bpy.context.view_layer.objects.active

    # Import clothing FBX
    bpy.ops.import_scene.fbx(filepath=input_fbx, use_anim=False)

    # Delete inactive objects and their children
    def remove_inactive_objects():
        """Delete inactive objects and all their children"""
        objects_to_remove = []

        def is_object_inactive(obj):
            """Determining whether an object is inactive"""
            # If hide_viewport or hide_render is True, it is determined to be inactive
            return obj.hide_viewport or obj.hide_render or obj.hide_get()

        def collect_children_recursive(obj, collected_list):
            """Recursively collect all children of an object"""
            for child in obj.children:
                collected_list.append(child)
                collect_children_recursive(child, collected_list)

        # Search for inactive objects
        for obj in bpy.data.objects:
            if is_object_inactive(obj) and obj not in objects_to_remove:
                objects_to_remove.append(obj)
                # All children are collected
                collect_children_recursive(obj, objects_to_remove)

        # Remove duplicates
        objects_to_remove = list(set(objects_to_remove))

        # Delete object
        for obj in objects_to_remove:
            obj_name = obj.name
            try:
                bpy.data.objects.remove(obj, do_unlink=True)
                print(f"Removed inactive object: {obj_name}")
            except Exception as e:
                print(f"Failed to remove object {obj_name}: {e}")

    remove_inactive_objects()

    # Load clothing avatar data
    print(f"Loading clothing avatar data from {clothing_avatar_data_path}")
    with open(clothing_avatar_data_path, 'r', encoding='utf-8') as f:
        clothing_avatar_data = json.load(f)

    # Find clothing armature
    clothing_armature = None
    for obj in bpy.data.objects:
        if obj.type == 'ARMATURE' and obj.name != "Armature.BaseAvatar":
            clothing_armature = obj
            break

    if not clothing_armature:
        raise Exception("Clothing armature not found")

    # Find clothing meshes
    clothing_meshes = []
    for obj in bpy.data.objects:
        if obj.type == 'MESH' and obj.name != "Body.BaseAvatar" and obj.name != "Body.BaseAvatar.RightOnly" and obj.name != "Body.BaseAvatar.LeftOnly":
            # Check if this mesh has an armature modifier
            has_armature = False
            for modifier in obj.modifiers:
                if modifier.type == 'ARMATURE':
                    has_armature = True
                    break

            if has_armature:
                clothing_meshes.append(obj)

    # Filtering: When target_meshes is specified, only the meshes contained within it are retained
    if target_meshes:
        target_mesh_list = [name for name in target_meshes.split(',')]
        print(f"Target mesh list: {target_mesh_list}")
        filtered_meshes = []
        for obj in clothing_meshes:
            if obj.name in target_mesh_list:
                filtered_meshes.append(obj)
            else:
                # Remove non-target meshes
                obj_name = obj.name
                bpy.data.objects.remove(obj, do_unlink=True)
                print(f"Removed non-target mesh: {obj_name}")

        if not filtered_meshes:
            raise Exception(f"No target meshes found. Specified: {target_meshes}")

        clothing_meshes = filtered_meshes

    # Set hips position if provided
    if hips_position:
        adjust_armature_hips_position(clothing_armature, hips_position, clothing_avatar_data)

    # Process mesh renderers if provided
    if mesh_renderers:
        print(f"Processing mesh renderers: {mesh_renderers}")
        for mesh_name, parent_name in mesh_renderers.items():
            # Find a mesh object with the same name as the object that had a MeshRenderer
            mesh_obj = None
            for obj in bpy.data.objects:
                if obj.type == 'MESH' and obj.name == mesh_name:
                    mesh_obj = obj
                    break

            if mesh_obj:
                # Check if the parent's name differs from the parent object's name in the data without an Armature modifier
                has_armature = False
                for modifier in mesh_obj.modifiers:
                    if modifier.type == 'ARMATURE':
                        has_armature = True
                        break

                current_parent_name = mesh_obj.parent.name if mesh_obj.parent else None

                if not has_armature and current_parent_name != parent_name:
                    # Find a bone in the clothing_armature with the same name as the parent object's name in the data
                    bone_found = False
                    if parent_name in clothing_armature.data.bones:
                        # If a bone is found, make that bone the parent of the mesh object
                        # Clear all selections
                        bpy.ops.object.select_all(action='DESELECT')

                        # Select the mesh
                        mesh_obj.select_set(True)

                        # Set the armature to active
                        bpy.context.view_layer.objects.active = clothing_armature
                        clothing_armature.select_set(True)

                        # Switch to Pose Mode and set the bone to active
                        bpy.ops.object.mode_set(mode='POSE')
                        clothing_armature.data.bones.active = clothing_armature.data.bones[parent_name]

                        # Return to Object Mode
                        bpy.ops.object.mode_set(mode='OBJECT')

                        # Set bone parent (preserve world coordinates with keep_transform)
                        bpy.ops.object.parent_set(type='BONE', keep_transform=True)

                        print(f"Set parent bone '{parent_name}' for mesh '{mesh_name}' (world transform preserved)")
                        bone_found = True

                        # Uncheck
                        bpy.ops.object.select_all(action='DESELECT')

                    if not bone_found:
                        print(f"Warning: Bone '{parent_name}' not found in clothing_armature for mesh '{mesh_name}'")
                else:
                    if has_armature:
                        print(f"Skipping mesh '{mesh_name}': already has Armature modifier")
                    else:
                        print(f"Skipping mesh '{mesh_name}': parent already matches ('{current_parent_name}')")
            else:
                print(f"Warning: Mesh object '{mesh_name}' not found")

    bpy.context.view_layer.objects.active = original_active

    return clothing_meshes, clothing_armature, clothing_avatar_data

def setup_weight_transfer() -> None:
    """Setup the Robust Weight Transfer plugin settings."""
    try:
        bpy.context.scene.robust_weight_transfer_settings.source_object = bpy.data.objects["Body.BaseAvatar"]
    except Exception as e:
        raise Exception(f"Failed to setup weight transfer: {str(e)}")

def triangulate_mesh(obj: bpy.types.Object) -> None:
    """
    Utilizing the existing triangle mesh segmentation from the current 3DView rendering,
    Convert all faces of the mesh to triangular faces.

    Args:
        obj: Mesh object to triangulate
    """
    if obj is None or obj.type != 'MESH':
        return

    # Save the original active object
    original_active = bpy.context.view_layer.objects.active

    try:
        # Set the object as active
        bpy.context.view_layer.objects.active = obj
        obj.select_set(True)

        # Switch to edit mode
        bpy.ops.object.mode_set(mode='EDIT')

        # Select all faces
        bpy.ops.mesh.select_all(action='SELECT')

        # Perform triangulation (using bmesh triangulation)
        bpy.ops.mesh.quads_convert_to_tris(quad_method='FIXED', ngon_method='BEAUTY')

        # Return to Object Mode
        bpy.ops.object.mode_set(mode='OBJECT')

        print(f"Triangulated mesh: {obj.name}")

    except Exception as e:
        print(f"Error triangulating mesh {obj.name}: {e}")
        # If an error occurs, return to Object Mode
        try:
            bpy.ops.object.mode_set(mode='OBJECT')
        except:
            pass

    finally:
        # Restore the original active object
        if original_active:
            bpy.context.view_layer.objects.active = original_active
        obj.select_set(False)

def build_bone_hierarchy(bone_node: dict, bone_parents: Dict[str, str], current_path: list):
    """
    Recursively construct the mapping of parent-child relationships from the bone hierarchy

    Parameters:
        bone_node (dict): Current bone node
        bone_parents (Dict[str, str]): A mapping from bone names to parent bone names
        current_path (list): A list of bone names on the current path
    """
    bone_name = bone_node['name']
    if current_path:
        bone_parents[bone_name] = current_path[-1]

    current_path.append(bone_name)
    for child in bone_node.get('children', []):
        build_bone_hierarchy(child, bone_parents, current_path)
    current_path.pop()

def get_humanoid_bone_hierarchy(avatar_data: dict) -> Tuple[Dict[str, str], Dict[str, str], Dict[str, str]]:
    """
    Extract the hierarchical relationship of Humanoid bones from avatar data

    Parameters:
        avatar_data (dict): Avatar data

    Returns:
        Tuple[Dict[str, str], Dict[str, str], Dict[str, str]]:
            (Dictionary from bone names to parents, Dictionary from Humanoid bone names to bone names, Dictionary from bone names to Humanoid bone names)
    """
    # Establishing bone parent-child relationships
    bone_parents = {}
    build_bone_hierarchy(avatar_data['boneHierarchy'], bone_parents, [])

    # Create a correspondence map between Humanoid bone names and bone names
    humanoid_to_bone = {bone_map['humanoidBoneName']: bone_map['boneName']
                       for bone_map in avatar_data['humanoidBones']}
    bone_to_humanoid = {bone_map['boneName']: bone_map['humanoidBoneName']
                       for bone_map in avatar_data['humanoidBones']}

    return bone_parents, humanoid_to_bone, bone_to_humanoid

def find_nearest_parent_with_pose(bone_name: str,
                                bone_parents: Dict[str, str],
                                bone_to_humanoid: Dict[str, str],
                                pose_data: dict) -> Optional[str]:
    """
    Trace the parent of the specified bone and return the name of the closest parent Humanoid bone that has pose data

    Parameters:
        bone_name (str): Start bone name
        bone_parents (Dict[str, str]): Bone parent-child relationship dictionary
        bone_to_humanoid (Dict[str, str]): Dictionary converting bone names to Humanoid bone names
        pose_data (dict): Pose data

    Returns:
        Optional[str]: Name of the found parent Humanoid bone, None if not found
    """
    current_bone = bone_name
    while current_bone in bone_parents:
        parent_bone = bone_parents[current_bone]
        if parent_bone in bone_to_humanoid:
            parent_humanoid = bone_to_humanoid[parent_bone]
            if parent_humanoid in pose_data:
                return parent_humanoid
        current_bone = parent_bone
    return None

def clear_humanoid_bone_relations_preserve_pose(armature_obj, clothing_avatar_data_filepath, base_avatar_data_filepath):
    """
    Maintain the pose in world space while unlinking the parent-child relationship of the Humanoid bones.
    Preserve the parent-child relationships of Humanoid bones not present in the base avatar's avatar data.

    Args:
        armature_obj: bpy.types.Object - Armature object
        clothing_avatar_data_filepath: str - The JSON filename for the clothing avatar data
        base_avatar_data_filepath: str - JSON filename for the base avatar's avatar data
    """
    if armature_obj.type != 'ARMATURE':
        raise ValueError("Selected object must be an armature")

    # Load avatar data
    clothing_avatar_data = load_avatar_data(clothing_avatar_data_filepath)
    base_avatar_data = load_avatar_data(base_avatar_data_filepath)

    # Create a set of Humanoid bones for the clothing
    clothing_humanoid_bones = {bone_map['boneName'] for bone_map in clothing_avatar_data['humanoidBones']}

    # Create a set of Humanoid bones for the base avatar
    base_humanoid_bones = {bone_map['humanoidBoneName'] for bone_map in base_avatar_data['humanoidBones']}

    # Create a conversion map from clothing Humanoid bone names to Humanoid bone names
    clothing_bone_to_humanoid = {bone_map['boneName']: bone_map['humanoidBoneName']
                                for bone_map in clothing_avatar_data['humanoidBones']}

    # Identify the bone to unlink the parent-child relationship (only Humanoid bones present in the base avatar)
    bones_to_unparent = set()
    for bone_name in clothing_humanoid_bones:
        humanoid_name = clothing_bone_to_humanoid.get(bone_name)
        if humanoid_name == "UpperChest" or humanoid_name == "LeftBreast" or humanoid_name == "RightBreast" or humanoid_name == "LeftToes" or humanoid_name == "RightToes":
            continue
        bones_to_unparent.add(bone_name)
        #if humanoid_name in base_humanoid_bones:
        #    bones_to_unparent.add(bone_name)

    # Get the armature data
    armature = armature_obj.data

    # Store original world space matrices for bones to unparent
    original_matrices = {}
    for bone in armature.bones:
        if bone.name in bones_to_unparent:
            pose_bone = armature_obj.pose.bones[bone.name]
            original_matrices[bone.name] = armature_obj.matrix_world @ pose_bone.matrix

    bpy.ops.object.mode_set(mode='OBJECT')
    bpy.ops.object.select_all(action='DESELECT')

    # Switch to edit mode to modify bone relations
    bpy.context.view_layer.objects.active = armature_obj
    original_mode = bpy.context.object.mode
    bpy.ops.object.mode_set(mode='EDIT')

    # Clear parent relationships for specified bones only
    for edit_bone in armature.edit_bones:
        if edit_bone.name in bones_to_unparent:
            edit_bone.parent = None

    # Return to pose mode
    bpy.ops.object.mode_set(mode='POSE')

    # Restore original world space positions for unparented bones
    for bone_name, original_matrix in original_matrices.items():
        pose_bone = armature_obj.pose.bones[bone_name]
        pose_bone.matrix = armature_obj.matrix_world.inverted() @ original_matrix

    # Return to original mode
    bpy.ops.object.mode_set(mode=original_mode)

def is_finger_bone(humanoid_bone: str) -> bool:
    """
    Determine whether it is a finger bone

    Parameters:
        humanoid_bone (str): Humanoid bone name

    Returns:
        bool: True for finger bones
    """
    finger_keywords = [
        "Thumb", "Index", "Middle", "Ring", "Little",
        "Toe"
    ]
    return any(keyword in humanoid_bone for keyword in finger_keywords)

def get_next_joint_bone(humanoid_bone: str) -> Optional[str]:
    """
    Get the bone name of the next joint on the finger

    Parameters:
        humanoid_bone (str): Humanoid bone name

    Returns:
        Optional[str]: Bone name of the next joint, or None if not present
    """
    joint_mapping = {
        "Proximal": "Intermediate",
        "Intermediate": "Distal",
    }

    # Identify the current joint type
    current_joint = None
    for joint_type in joint_mapping.keys():
        if joint_type in humanoid_bone:
            current_joint = joint_type
            break

    if not current_joint:
        return None

    # Generate the bone name for the next joint
    next_joint = joint_mapping[current_joint]
    return humanoid_bone.replace(current_joint, next_joint)

def apply_finger_bone_adjustments(
    armature_obj: bpy.types.Object,
    humanoid_to_bone: Dict[str, str],
    bone_to_humanoid: Dict[str, str]
) -> None:
    """
    Adjust the position of the finger bone
    Adjust each bone's Tail to align with the next joint's Head

    Parameters:
        armature_obj: Armature Object
        humanoid_to_bone: Conversion dictionary from Humanoid bone names to bone names
        bone_to_humanoid: Conversion dictionary from bone names to Humanoid bone names
    """
    # Process all finger bones
    for bone_name, pose_bone in armature_obj.pose.bones.items():
        if bone_name not in bone_to_humanoid:
            continue

        humanoid_bone = bone_to_humanoid[bone_name]
        if not is_finger_bone(humanoid_bone):
            continue

        # Get the next joint
        next_humanoid_bone = get_next_joint_bone(humanoid_bone)
        if not next_humanoid_bone or next_humanoid_bone not in humanoid_to_bone:
            continue

        next_bone_name = humanoid_to_bone[next_humanoid_bone]
        if next_bone_name not in armature_obj.pose.bones:
            continue

        next_bone = armature_obj.pose.bones[next_bone_name]

        # Get the current bone's direction vector
        current_dir = ((armature_obj.matrix_world @ pose_bone.tail) - (armature_obj.matrix_world @ pose_bone.head)).normalized()

        # Calculate position in world space
        head_world = armature_obj.matrix_world @ pose_bone.head
        next_head_world = armature_obj.matrix_world @ next_bone.head

        # Calculate the new direction vector
        new_dir = (next_head_world - head_world).normalized()

        # Calculate the difference in rotation
        #rot_diff = new_dir.rotation_difference(current_dir)
        rot_diff = current_dir.rotation_difference(new_dir)

        # Get the current queue
        current_matrix = pose_bone.matrix.copy()

        translation, rotation, scale = current_matrix.decompose()
        trans_mat = Matrix.Translation(translation)

        # Create a new matrix with rotation applied
        rot_matrix = rot_diff.to_matrix().to_4x4()
        new_matrix = trans_mat @ rot_matrix @ trans_mat.inverted() @ current_matrix

        # Apply the new matrix
        pose_bone.matrix = new_matrix

def list_to_matrix(matrix_list):
    """
    Convert from list to Matrix type (for JSON loading)

    Parameters:
        matrix_list: list - A two-dimensional list containing matrix data

    Returns:
        Matrix: Transformed matrix
    """
    return Matrix(matrix_list)

def add_pose_from_json(armature_obj, filepath, avatar_data, invert=False):
    """
    Add the pose data loaded from the JSON file to the current pose of the active Armature

    Parameters:
        armature_obj: Armature Object
        filepath (str): Path to the JSON file to load
        avatar_data (dict): Avatar data
        invert (bool): Whether to apply inverse transformation
    """
    # Retrieve active objects
    if not armature_obj:
        raise ValueError("No active object found")

    if armature_obj.type != 'ARMATURE':
        raise ValueError(f"Active object '{armature_obj.name}' is not an armature")

    # Acquire hierarchical relationships and transformation maps
    bone_parents, humanoid_to_bone, bone_to_humanoid = get_humanoid_bone_hierarchy(avatar_data)

    # File existence check
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Pose data file not found: {filepath}")

    # Read the JSON file
    with open(filepath, 'r', encoding='utf-8') as f:
        pose_data = json.load(f)

    # Create a step for undo
    bpy.ops.ed.undo_push(message="Add Pose from JSON")

    bpy.ops.object.mode_set(mode='OBJECT')
    # Clear all selections
    bpy.ops.object.select_all(action='DESELECT')
    bpy.context.view_layer.objects.active = armature_obj

    # Switch to edit mode
    bpy.ops.object.mode_set(mode='EDIT')

    # Unlink all editing bones
    for bone in armature_obj.data.edit_bones:
        bone.use_connect = False

    # Return to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # To process while maintaining parent-child relationships, retrieve bones in hierarchical order
    def get_bone_hierarchy_order():
        """Retrieve Humanoid bones in parent-to-child order"""
        order = []
        visited = set()

        def add_bone_and_children(humanoid_bone):
            if humanoid_bone in visited:
                return
            visited.add(humanoid_bone)
            order.append(humanoid_bone)

            # Search for child bones
            for child_bone, parent_bone in bone_parents.items():
                if parent_bone == humanoid_bone and child_bone not in visited:
                    add_bone_and_children(child_bone)

        # Start with Root Bone (Hips)
        root_bones = []
        root_bones.append(humanoid_to_bone['Hips'])

        for root_bone in root_bones:
            add_bone_and_children(root_bone)

        return order

    bone_order = get_bone_hierarchy_order()

    # Dictionary recording processed Humanoid bones
    processed_bones = {}

    # Save the pre-deformation state of all bones beforehand
    original_bone_data = {}
    for humanoid_bone in humanoid_to_bone.keys():
        bone_name = humanoid_to_bone.get(humanoid_bone)
        if bone_name and bone_name in armature_obj.pose.bones:
            bone = armature_obj.pose.bones[bone_name]
            original_bone_data[humanoid_bone] = {
                'matrix': bone.matrix.copy(),
                'head': bone.head.copy(),
                'tail': bone.tail.copy(),
                'bone_name': bone_name
            }

    # Execute pose data calculations in hierarchical order
    for bone_name in bone_order:
        if not bone_name or bone_name not in armature_obj.pose.bones:
            continue

        humanoid_bone = bone_to_humanoid.get(bone_name)
        if not humanoid_bone:
            continue

        # If already processed, skip
        if humanoid_bone in processed_bones:
            continue

        # Determine whether to hold pose data directly or inherit it from the parent
        source_humanoid_bone = humanoid_bone
        if humanoid_bone not in pose_data:
            parent_with_pose = find_nearest_parent_with_pose(
                bone_name, bone_parents, bone_to_humanoid, pose_data)
            if not parent_with_pose:
                continue
            source_humanoid_bone = parent_with_pose
            print(f"Using pose data from parent bone {source_humanoid_bone} for {humanoid_bone}")

        # Calculations using the saved original data
        if humanoid_bone not in original_bone_data:
            continue

        bone = armature_obj.pose.bones[bone_name]

        original_data = original_bone_data[humanoid_bone]

        # Get the matrix in the current world space (using the original data)
        current_world_matrix = armature_obj.matrix_world @ original_data['matrix']

        # Obtain the difference transformation matrix
        delta_matrix = list_to_matrix(pose_data[source_humanoid_bone]['delta_matrix'])

        if invert:
            delta_matrix = delta_matrix.inverted()

        # Add to the current queue
        combined_matrix = delta_matrix @ current_world_matrix

        # Convert to local space and apply
        bone.matrix = armature_obj.matrix_world.inverted() @ combined_matrix

        # Changes are reflected immediately (as they affect child bone calculations)
        bpy.context.view_layer.update()

        # Mark as processed
        processed_bones[humanoid_bone] = True

    # Force final pose update
    bpy.context.view_layer.update()
    print(f"Pose data added to armature '{armature_obj.name}' from {filepath}")

def add_clothing_pose_from_json(armature_obj, pose_filepath="pose_data.json", init_pose_filepath="initial_pose.json", clothing_avatar_data_filepath="avatar_data.json", base_avatar_data_filepath="avatar_data.json", invert=False):
    """
    Add the pose data loaded from the JSON file to the current pose of the active Armature

    Parameters:
        filename (str): The name of the JSON file to load
        avatar_data_file (str): JSON file name for avatar data
        invert (bool): Whether to apply inverse transformation
    """

    if not armature_obj:
        raise ValueError("No active object found")

    if armature_obj.type != 'ARMATURE':
        raise ValueError(f"Active object '{armature_obj.name}' is not an armature")

    # Load avatar data
    avatar_data = load_avatar_data(clothing_avatar_data_filepath)

    # Acquire hierarchical relationships and transformation maps
    bone_parents, humanoid_to_bone, bone_to_humanoid = get_humanoid_bone_hierarchy(avatar_data)

    # File existence check
    if not os.path.exists(pose_filepath):
        raise FileNotFoundError(f"Pose data file not found: {pose_filepath}")

    # Read the JSON file
    with open(pose_filepath, 'r', encoding='utf-8') as f:
        pose_data = json.load(f)

    # Create a step for undo
    bpy.ops.ed.undo_push(message="Add Pose from JSON")

    bpy.ops.object.mode_set(mode='OBJECT')
    # Clear all selections
    bpy.ops.object.select_all(action='DESELECT')
    bpy.context.view_layer.objects.active = armature_obj

    # Switch to edit mode
    bpy.ops.object.mode_set(mode='EDIT')

    # Unlink all editing bones
    for bone in armature_obj.data.edit_bones:
        bone.use_connect = False

    # Return to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # To process while maintaining parent-child relationships, retrieve bones in hierarchical order
    def get_bone_hierarchy_order():
        """Retrieve Humanoid bones in parent-to-child order"""
        order = []
        visited = set()

        def add_bone_and_children(humanoid_bone):
            if humanoid_bone in visited:
                return
            visited.add(humanoid_bone)
            order.append(humanoid_bone)

            # Search for child bones
            for child_bone, parent_bone in bone_parents.items():
                if parent_bone == humanoid_bone and child_bone not in visited:
                    add_bone_and_children(child_bone)

        # Start with Root Bone (Hips)
        root_bones = []
        root_bones.append(humanoid_to_bone['Hips'])

        for root_bone in root_bones:
            add_bone_and_children(root_bone)

        return order

    bone_order = get_bone_hierarchy_order()

    # Unlink parent-child relationships for Humanoid bones
    clear_humanoid_bone_relations_preserve_pose(armature_obj, clothing_avatar_data_filepath, base_avatar_data_filepath)

    bpy.context.view_layer.update()

    # Record the current pose before applying a new pose
    store_pose_globally(armature_obj)
    print(f"Pose state stored globally before applying pose from {pose_filepath}")

    # Applying the Initial Pose (Using a New Independent Function)
    if init_pose_filepath:
        apply_initial_pose_to_armature(armature_obj, init_pose_filepath, clothing_avatar_data_filepath)

    # Dictionary recording processed Humanoid bones
    processed_bones = {}

    # Add pose data to the current pose
    for humanoid_bone in bone_to_humanoid.values():
        # If already processed, skip
        if humanoid_bone in processed_bones:
            continue

        if humanoid_bone == "UpperChest" or \
           humanoid_bone == "LeftBreast" or humanoid_bone == "RightBreast" or \
           humanoid_bone == "LeftToes" or humanoid_bone == "RightToes":
            continue

        bone_name = humanoid_to_bone.get(humanoid_bone)
        if not bone_name or bone_name not in armature_obj.pose.bones:
            continue

        # Determine whether to hold pose data directly or inherit it from the parent
        source_humanoid_bone = humanoid_bone
        if humanoid_bone not in pose_data:
            parent_with_pose = find_nearest_parent_with_pose(
                bone_name, bone_parents, bone_to_humanoid, pose_data)
            if not parent_with_pose:
                continue
            source_humanoid_bone = parent_with_pose
            print(f"Using pose data from parent bone {source_humanoid_bone} for {humanoid_bone}")

        # Apply pose data
        bone = armature_obj.pose.bones[bone_name]

        # Get the current matrix in world space
        current_world_matrix = armature_obj.matrix_world @ bone.matrix

        # Obtain the difference transformation matrix
        delta_matrix = list_to_matrix(pose_data[source_humanoid_bone]['delta_matrix'])

        if invert:
            delta_matrix = delta_matrix.inverted()

        # Add to the current queue
        combined_matrix = delta_matrix @ current_world_matrix

        # Convert to local space and apply
        bone.matrix = armature_obj.matrix_world.inverted() @ combined_matrix

        # Mark as processed
        processed_bones[humanoid_bone] = True

    # Force pose update
    bpy.context.view_layer.update()
    print(f"Pose data added to armature '{armature_obj.name}' from {pose_filepath}")

    for bone_name in armature_obj.pose.bones.keys():
        if bone_name in bone_to_humanoid:
            humanoid_name = bone_to_humanoid[bone_name]
            if humanoid_name in processed_bones:
                mat = armature_obj.pose.bones[bone_name].matrix
                print(f"'{humanoid_name}' ({bone_name}) bone.matrix_final {mat}")



def remove_empty_vertex_groups(mesh_obj: bpy.types.Object) -> None:
    """Remove vertex groups that are empty or have zero weights for all vertices."""
    if not mesh_obj.type == 'MESH' or not mesh_obj.vertex_groups:
        return

    used_vertex_groups = {i: False for i in range(len(mesh_obj.vertex_groups))}
    for vert in mesh_obj.data.vertices:
        for g in vert.groups:
            if g.weight > 0.0005:
                used_vertex_groups[g.group] = True

    groups_to_remove = []

    for i, used in used_vertex_groups.items():
        if not used:
            groups_to_remove.append(mesh_obj.vertex_groups[i].name)

    for group_name in groups_to_remove:
        if group_name in mesh_obj.vertex_groups:
            mesh_obj.vertex_groups.remove(mesh_obj.vertex_groups[group_name])
            print(f"Removed empty vertex group: {group_name}")

def find_parent_bone_hierarchy(current_node: dict, target_bone: str, parent_bone: str = None) -> str:
    """
    Recursively search for a bone in the hierarchy and return its parent.

    Parameters:
        current_node: Current node in the bone hierarchy
        target_bone: Name of the bone to find
        parent_bone: Name of the parent bone (used in recursion)

    Returns:
        Name of the parent bone or None if not found
    """
    # Check if current node is the target
    if current_node["name"] == target_bone:
        return parent_bone

    # Search children
    for child in current_node.get("children", []):
        result = find_parent_bone_hierarchy(child, target_bone, current_node["name"])
        if result is not None:
            return result

    return None

def get_bone_parent_map(bone_hierarchy: dict) -> dict:
    """
    Create a map of bones to their parents from the hierarchy.

    Parameters:
        bone_hierarchy: Bone hierarchy data from avatar data

    Returns:
        Dictionary mapping bone names to their parent bone names
    """
    parent_map = {}

    def traverse_hierarchy(node, parent=None):
        current_bone = node["name"]
        parent_map[current_bone] = parent

        for child in node.get("children", []):
            traverse_hierarchy(child, current_bone)

    traverse_hierarchy(bone_hierarchy)
    return parent_map

def merge_weights_to_parent(mesh_obj: bpy.types.Object, source_bone: str, target_bone: str) -> None:
    """
    Merge weights from source bone to target bone and remove source bone vertex group.

    Parameters:
        mesh_obj: Mesh object to process
        source_bone: Name of the source bone (whose weights will be moved)
        target_bone: Name of the target bone (that will receive the weights)
    """
    source_group = mesh_obj.vertex_groups.get(source_bone)
    target_group = mesh_obj.vertex_groups.get(target_bone)

    if not source_group:
        return

    if not target_group:
        # Create target group if it doesn't exist
        target_group = mesh_obj.vertex_groups.new(name=target_bone)

    # Transfer weights
    for vert in mesh_obj.data.vertices:
        source_weight = 0
        for group in vert.groups:
            if group.group == source_group.index:
                source_weight = group.weight
                break

        if source_weight > 0:
            target_group.add([vert.index], source_weight, 'ADD')

    # Remove source group
    mesh_obj.vertex_groups.remove(source_group)
    print(f"Merged weights from {source_bone} to {target_bone} in {mesh_obj.name}")

def apply_bone_name_conversion(clothing_armature: bpy.types.Object, clothing_meshes: list, name_conv_data: dict) -> None:
    """
    According to the bone renaming mapping specified in the JSON file,
    Rename the bones in clothing_armature and the vertex groups in clothing_meshes

    Parameters:
        clothing_armature: Clothing Armature Object
        clothing_meshes: List of clothing mesh objects
        name_conv_data: JSON data for bone name conversion mapping
    """
    if not name_conv_data or 'boneMapping' not in name_conv_data:
        print("Bone rename data not found")
        return

    bone_mappings = name_conv_data['boneMapping']
    renamed_bones = {}

    print(f"Bone renaming process initiated: {len(bone_mappings)} mappings")

    # 1. Rename the armature's bone name
    if clothing_armature and clothing_armature.type == 'ARMATURE':
        # Enter Edit mode and rename the bone
        bpy.context.view_layer.objects.active = clothing_armature
        bpy.ops.object.mode_set(mode='EDIT')

        for mapping in bone_mappings:
            fbx_bone = mapping.get('fbxBone')
            prefab_bone = mapping.get('prefabBone')

            if not fbx_bone or not prefab_bone or fbx_bone == prefab_bone:
                continue

            # Find the bone corresponding to fbxBone within the armature
            if fbx_bone in clothing_armature.data.edit_bones:
                edit_bone = clothing_armature.data.edit_bones[fbx_bone]
                edit_bone.name = prefab_bone
                renamed_bones[fbx_bone] = prefab_bone
                print(f"Rename the armature bone: {fbx_bone} -> {prefab_bone}")

        bpy.ops.object.mode_set(mode='OBJECT')

    # 2. Change the mesh vertex group name
    for mesh_obj in clothing_meshes:
        if not mesh_obj or mesh_obj.type != 'MESH':
            continue

        for mapping in bone_mappings:
            fbx_bone = mapping.get('fbxBone')
            prefab_bone = mapping.get('prefabBone')

            if not fbx_bone or not prefab_bone or fbx_bone == prefab_bone:
                continue

            # Rename the vertex group
            if fbx_bone in mesh_obj.vertex_groups:
                vertex_group = mesh_obj.vertex_groups[fbx_bone]
                vertex_group.name = prefab_bone
                print(f"Rename vertex group name for mesh {mesh_obj.name}: {fbx_bone} -> {prefab_bone}")

    print(f"Bone renaming process complete: {len(renamed_bones)} bones have been renamed")

def normalize_clothing_bone_names(clothing_armature: bpy.types.Object, clothing_avatar_data: dict,
                                clothing_meshes: list) -> None:
    """
    Normalize bone names in clothing_avatar_data to match existing bones in clothing_armature.

    For each humanoidBone in clothing_avatar_data:
    1. Check if boneName exists in clothing_armature
    2. If not, convert boneName to lowercase alphabetic characters and find matching bone
    3. Update boneName in clothing_avatar_data if match found
    4. Update corresponding vertex group names in all clothing_meshes
    """
    import re

    # Get all bone names from clothing armature
    armature_bone_names = {bone.name for bone in clothing_armature.data.bones}
    print(f"Available bones in clothing armature: {sorted(armature_bone_names)}")

    # Store name changes for vertex group updates
    bone_name_changes = {}

    # Process each humanoid bone mapping
    for bone_map in clothing_avatar_data.get("humanoidBones", []):
        if "boneName" not in bone_map:
            continue

        original_bone_name = bone_map["boneName"]

        # Check if bone exists in armature
        if original_bone_name in armature_bone_names:
            print(f"Bone '{original_bone_name}' found in armature")
            continue

        # Extract alphabetic characters and convert to lowercase
        normalized_pattern = re.sub(r'[^a-zA-Z]', '', original_bone_name).lower()
        if not normalized_pattern:
            print(f"Warning: No alphabetic characters found in bone name '{original_bone_name}'")
            continue

        print(f"Looking for bone matching pattern '{normalized_pattern}' (from '{original_bone_name}')")

        # Find matching bone in armature
        matching_bone = None
        for armature_bone_name in armature_bone_names:
            armature_normalized = re.sub(r'[^a-zA-Z]', '', armature_bone_name).lower()
            if armature_normalized == normalized_pattern:
                matching_bone = armature_bone_name
                break

        if matching_bone:
            print(f"Found matching bone: '{original_bone_name}' -> '{matching_bone}'")
            bone_name_changes[matching_bone] = original_bone_name
        else:
            print(f"Warning: No matching bone found for '{original_bone_name}' (pattern: '{normalized_pattern}')")

    # Update vertex group names in all clothing meshes
    if bone_name_changes:
        print(f"Updating vertex groups with bone name changes: {bone_name_changes}")

        for mesh_obj in clothing_meshes:
            if not mesh_obj or mesh_obj.type != 'MESH':
                continue

            for old_name, new_name in bone_name_changes.items():
                if old_name in mesh_obj.vertex_groups:
                    vertex_group = mesh_obj.vertex_groups[old_name]
                    vertex_group.name = new_name
                    print(f"Updated vertex group '{old_name}' -> '{new_name}' in mesh '{mesh_obj.name}'")

        # Update bone names in clothing armature
        print(f"Updating bone names in clothing armature: {bone_name_changes}")
        for old_name, new_name in bone_name_changes.items():
            if old_name in clothing_armature.data.bones:
                bone = clothing_armature.data.bones[old_name]
                bone.name = new_name
                print(f"Updated armature bone '{old_name}' -> '{new_name}'")

    print("Bone name normalization completed")

def apply_initial_pose_to_armature(armature_obj, init_pose_filepath, clothing_avatar_data_filepath):
    """
    Apply initial pose from JSON to the armature.

    Parameters:
        armature_obj: Target armature object
        init_pose_filepath: Path to initial pose JSON file
        clothing_avatar_data_filepath: Path to avatar data JSON file
    """
    if not init_pose_filepath or not os.path.exists(init_pose_filepath):
        return

    # Load avatar data
    avatar_data = load_avatar_data(clothing_avatar_data_filepath)

    # Acquire hierarchical relationships and transformation maps
    bone_parents, humanoid_to_bone, bone_to_humanoid = get_humanoid_bone_hierarchy(avatar_data)

    # Retrieve Humanoid bones in parent-to-child order
    def get_bone_hierarchy_order():
        order = []
        visited = set()

        def add_bone_and_children(humanoid_bone):
            if humanoid_bone in visited:
                return
            visited.add(humanoid_bone)
            order.append(humanoid_bone)

            # Search for child bones
            for child_bone, parent_bone in bone_parents.items():
                if parent_bone == humanoid_bone and child_bone not in visited:
                    add_bone_and_children(child_bone)

        # Start with Root Bone (Hips)
        root_bones = []
        root_bones.append(humanoid_to_bone['Hips'])

        for root_bone in root_bones:
            add_bone_and_children(root_bone)

        return order

    bone_order = get_bone_hierarchy_order()

    # Applying the initial pose
    with open(init_pose_filepath, 'r', encoding='utf-8') as f:
        init_pose_data = json.load(f)

    # Create a mapping using bone names as keys
    bone_transforms = {}
    for bone_data in init_pose_data.get("bones", []):
        bone_name = bone_data["boneName"]
        transform = bone_data["transform"]
        bone_transforms[bone_name] = transform

    # Dictionary recording processed Humanoid bones
    processed_bones = {}

    # Save the pre-deformation state of all bones beforehand
    original_bone_data = {}
    for bone_name in bone_order:
        if bone_name and bone_name in armature_obj.pose.bones:
            bone = armature_obj.pose.bones[bone_name]
            original_bone_data[bone_name] = {
                'matrix': bone.matrix.copy(),
                'head': bone.head.copy(),
                'tail': bone.tail.copy(),
            }

    # Apply the initial pose to each bone of the armature
    for bone_name in bone_order:
        if not bone_name or bone_name not in armature_obj.pose.bones:
            continue

        # If already processed, skip
        if bone_name in processed_bones:
            continue

        # Calculations using the saved original data
        if bone_name not in original_bone_data:
            continue

        if bone_name not in bone_transforms:
            continue

        bone = armature_obj.pose.bones[bone_name]

        original_data = original_bone_data[bone_name]

        # Get the matrix in the current world space (using the original data)
        current_world_matrix = armature_obj.matrix_world @ original_data['matrix']

        transform = bone_transforms[bone_name]

        # Check if delta_matrix exists
        if "delta_matrix" in transform:
            # Obtain the difference transformation matrix
            delta_matrix = list_to_matrix(transform['delta_matrix'])

            # Apply to the current matrix
            combined_matrix = delta_matrix @ current_world_matrix

            # Convert to local space and apply
            bone.matrix = armature_obj.matrix_world.inverted() @ combined_matrix
        else:
            # For backward compatibility, the older format (position, rotation, scale) is also supported
            # Set position
            pos = transform.get("position", [0, 0, 0])
            init_loc = Vector((pos[0], pos[1], pos[2]))

            # Set rotation (convert degrees to radians)
            rot = transform.get("rotation", [0, 0, 0])
            init_rot = Euler([math.radians(r) for r in rot], 'XYZ')

            # Set the scale
            scale = transform.get("scale", [1, 1, 1])
            init_scale = Vector((scale[0], scale[1], scale[2]))

            head_world = armature_obj.matrix_world @ bone.head
            offset_matrix = Matrix.Translation(head_world)

            # Create a new array
            delta_matrix = Matrix.Translation(init_loc) @ \
                        init_rot.to_matrix().to_4x4() @ \
                        Matrix.Scale(init_scale.x, 4, (1, 0, 0)) @ \
                        Matrix.Scale(init_scale.y, 4, (0, 1, 0)) @ \
                        Matrix.Scale(init_scale.z, 4, (0, 0, 1))

            # Add to the current queue
            combined_matrix = offset_matrix @ delta_matrix @ offset_matrix.inverted() @ current_world_matrix

            # Convert to local space and apply
            bone.matrix = armature_obj.matrix_world.inverted() @ combined_matrix

        # Changes are reflected immediately (as they affect child bone calculations)
        bpy.context.view_layer.update()

        # Mark as processed
        processed_bones[bone_name] = True

    # Refresh View
    bpy.context.view_layer.update()

def is_A_pose(avatar_data: dict, armature: bpy.types.Object, init_pose_filepath=None, pose_filepath=None, clothing_avatar_data_filepath=None) -> bool:
    """
    Check if the avatar data is in A pose.
    Creates a temporary copy of the armature, applies initial pose, checks A-pose, then deletes the copy.

    Parameters:
        avatar_data: Avatar data dictionary
        armature: Target armature object
        init_pose_filepath: Path to initial pose JSON file (optional)
        clothing_avatar_data_filepath: Path to avatar data JSON file (optional)
    """
    # Temporarily copy the armature
    original_active = bpy.context.view_layer.objects.active
    original_mode = armature.mode if hasattr(armature, 'mode') else 'OBJECT'

    # Switch to Object Mode
    if bpy.context.object and bpy.context.object.mode != 'OBJECT':
        bpy.ops.object.mode_set(mode='OBJECT')

    # Uncheck
    bpy.ops.object.select_all(action='DESELECT')

    # Copy the armature
    armature.select_set(True)
    bpy.context.view_layer.objects.active = armature
    bpy.ops.object.duplicate()
    temp_armature = bpy.context.active_object
    temp_armature.name = f"{armature.name}_temp_A_pose_check"

    try:
        # Apply initial pose
        if init_pose_filepath and clothing_avatar_data_filepath:
            apply_initial_pose_to_armature(temp_armature, init_pose_filepath, clothing_avatar_data_filepath)

        if pose_filepath and clothing_avatar_data_filepath:
            with open(clothing_avatar_data_filepath, 'r', encoding='utf-8') as f:
                clothing_avatar_data = json.load(f)
            add_pose_from_json(temp_armature, pose_filepath, clothing_avatar_data, invert=False)

        # Create mappings for clothing
        humanoid_to_bone = {}
        for bone_map in avatar_data.get("humanoidBones", []):
            if "humanoidBoneName" in bone_map and "boneName" in bone_map:
                humanoid_to_bone[bone_map["humanoidBoneName"]] = bone_map["boneName"]

        arm_bone = None
        lower_arm_bone = None
        for bone in temp_armature.pose.bones:
            if bone.name == humanoid_to_bone.get("LeftUpperArm"):
                for bone2 in temp_armature.pose.bones:
                    if bone2.name == humanoid_to_bone.get("LeftLowerArm"):
                        lower_arm_bone = bone2
                        break
                if lower_arm_bone:
                    arm_bone = bone
                    break
            elif bone.name == humanoid_to_bone.get("RightUpperArm"):
                for bone2 in temp_armature.pose.bones:
                    if bone2.name == humanoid_to_bone.get("RightLowerArm"):
                        lower_arm_bone = bone2
                        break
                if lower_arm_bone:
                    arm_bone = bone
                    break

        result = False
        if arm_bone and lower_arm_bone:
            arm_bone_direction = (temp_armature.matrix_world @ lower_arm_bone.head) - (temp_armature.matrix_world @ arm_bone.head)
            arm_bone_direction = arm_bone_direction.normalized()
            arm_bone_angle = math.acos(abs(arm_bone_direction.dot(Vector((1, 0, 0)))))
            print(f"arm_bone: {arm_bone.name}")
            print(f"lower_arm_bone: {lower_arm_bone.name}")
            print(f"arm_bone_head: {temp_armature.matrix_world @ arm_bone.head}")
            print(f"lower_arm_bone_head: {temp_armature.matrix_world @ lower_arm_bone.head}")
            print(f"arm_bone_direction: {arm_bone_direction}")
            print(f"arm_bone_angle: {math.degrees(arm_bone_angle)}")
            if math.degrees(arm_bone_angle) > 30:
                result = True
            else:
                result = False
        else:
            result = False

    finally:
        # Remove temporary armature
        bpy.ops.object.select_all(action='DESELECT')
        temp_armature.select_set(True)
        bpy.context.view_layer.objects.active = temp_armature
        bpy.ops.object.delete()

        # Restore the original active object
        if original_active:
            bpy.context.view_layer.objects.active = original_active

    return result

def generate_temp_shapekeys_for_weight_transfer(obj: bpy.types.Object, armature_obj: bpy.types.Object, avatar_data: dict, is_A_pose: bool) -> None:
    """
    Generate temp shapekeys for weight transfer.
    """
    if obj.type != 'MESH':
        return

    set_armature_modifier_visibility(obj, True, True)

    for sk in obj.data.shape_keys.key_blocks:
        if sk.name != "Basis":
            if sk.name == "SymmetricDeformed":
                sk.value = 1.0
            else:
                sk.value = 0.0

    A_pose_shape_verts = None
    crotch_shape_verts = None

    original_shape_key_state = save_shape_key_state(obj)

    if is_A_pose:
        restore_shape_key_state(obj, original_shape_key_state)

        # Apply Y-axis rotation to all bones of the left and right arms
        print("  Apply Y-axis rotation to all bones of the left and right arms")
        bpy.context.view_layer.objects.active = armature_obj
        bpy.ops.object.mode_set(mode='POSE')

        # Get the boneName for the entire left and right arms from humanoidBones
        left_arm_humanoid_names = [
            "LeftUpperArm", "LeftLowerArm", "LeftHand",
            "LeftThumbProximal", "LeftThumbIntermediate", "LeftThumbDistal",
            "LeftIndexProximal", "LeftIndexIntermediate", "LeftIndexDistal",
            "LeftMiddleProximal", "LeftMiddleIntermediate", "LeftMiddleDistal",
            "LeftRingProximal", "LeftRingIntermediate", "LeftRingDistal",
            "LeftLittleProximal", "LeftLittleIntermediate", "LeftLittleDistal"
        ]

        right_arm_humanoid_names = [
            "RightUpperArm", "RightLowerArm", "RightHand",
            "RightThumbProximal", "RightThumbIntermediate", "RightThumbDistal",
            "RightIndexProximal", "RightIndexIntermediate", "RightIndexDistal",
            "RightMiddleProximal", "RightMiddleIntermediate", "RightMiddleDistal",
            "RightRingProximal", "RightRingIntermediate", "RightRingDistal",
            "RightLittleProximal", "RightLittleIntermediate", "RightLittleDistal"
        ]

        left_arm_bones = []
        right_arm_bones = []
        left_upper_arm_bone = None
        right_upper_arm_bone = None

        # Acquire the humanoid bone
        for bone_map in avatar_data.get("humanoidBones", []):
            humanoid_name = bone_map.get("humanoidBoneName")
            bone_name = bone_map.get("boneName")
            if humanoid_name == "LeftUpperArm":
                left_upper_arm_bone = bone_name
            elif humanoid_name == "RightUpperArm":
                right_upper_arm_bone = bone_name

            if humanoid_name in left_arm_humanoid_names:
                left_arm_bones.append(bone_name)
            elif humanoid_name in right_arm_humanoid_names:
                right_arm_bones.append(bone_name)

        # Get starting from the head of the LeftUpperArm
        left_pivot_point = None
        if left_upper_arm_bone and left_upper_arm_bone in armature_obj.pose.bones:
            left_pivot_point = armature_obj.matrix_world @ armature_obj.pose.bones[left_upper_arm_bone].head

        # Acquire starting from the head of the RightUpperArm
        right_pivot_point = None
        if right_upper_arm_bone and right_upper_arm_bone in armature_obj.pose.bones:
            right_pivot_point = armature_obj.matrix_world @ armature_obj.pose.bones[right_upper_arm_bone].head

        # Apply a -45-degree rotation along the Y-axis to the entire left arm (using the head of LeftUpperArm as the origin)
        if left_pivot_point:
            for bone_name in left_arm_bones:
                if bone_name and bone_name in armature_obj.pose.bones:
                    bone = armature_obj.pose.bones[bone_name]
                    current_world_matrix = armature_obj.matrix_world @ bone.matrix
                    # Apply a -45-degree rotation around the Y-axis in the global coordinate system (using the head of the LeftUpperArm as the origin)
                    offset_matrix = mathutils.Matrix.Translation(left_pivot_point * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(-45), 4, 'Y')
                    bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        # Apply a 45-degree rotation around the Y-axis to the entire right upper arm (using the head of RightUpperArm as the origin)
        if right_pivot_point:
            for bone_name in right_arm_bones:
                if bone_name and bone_name in armature_obj.pose.bones:
                    bone = armature_obj.pose.bones[bone_name]
                    current_world_matrix = armature_obj.matrix_world @ bone.matrix
                    # Apply a 45-degree rotation around the Y-axis in the global coordinate system (using the head of the RightUpperArm as the origin)
                    offset_matrix = mathutils.Matrix.Translation(right_pivot_point * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(45), 4, 'Y')
                    bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        bpy.ops.object.mode_set(mode='OBJECT')
        bpy.context.view_layer.objects.active = obj
        bpy.context.view_layer.update()

        #Retrieve the current evaluated mesh and save the state after armature deformation
        depsgraph = bpy.context.evaluated_depsgraph_get()
        eval_obj = obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data
        A_pose_shape_verts = np.array([v.co.copy() for v in eval_mesh.vertices])

        # Apply (or reset) a 45-degree rotation around the Y-axis to the entire left arm (using the head of LeftUpperArm as the origin)
        if left_pivot_point:
            for bone_name in left_arm_bones:
                if bone_name and bone_name in armature_obj.pose.bones:
                    bone = armature_obj.pose.bones[bone_name]
                    current_world_matrix = armature_obj.matrix_world @ bone.matrix
                    # Apply a 45-degree rotation around the Y-axis in the global coordinate system (using the head of the LeftUpperArm as the origin)
                    offset_matrix = mathutils.Matrix.Translation(left_pivot_point * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(45), 4, 'Y')
                    bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        # Apply a -45 degree Y-axis rotation to the entire right upper arm (reset) (using the head of RightUpperArm as the origin)
        if right_pivot_point:
            for bone_name in right_arm_bones:
                if bone_name and bone_name in armature_obj.pose.bones:
                    bone = armature_obj.pose.bones[bone_name]
                    current_world_matrix = armature_obj.matrix_world @ bone.matrix
                    # Apply a -45-degree rotation around the Y-axis in the global coordinate system (using the head of the RightUpperArm as the origin)
                    offset_matrix = mathutils.Matrix.Translation(right_pivot_point * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(-45), 4, 'Y')
                    bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

    restore_shape_key_state(obj, original_shape_key_state)

    # Apply Y-axis rotation to all bones of the left and right legs
    print("  Apply Y-axis rotation to all bones of the left and right legs")
    bpy.context.view_layer.objects.active = armature_obj
    bpy.ops.object.mode_set(mode='POSE')

    # Get the boneName for the entire left and right legs from humanoidBones
    left_leg_humanoid_names = [
        "LeftUpperLeg", "LeftLowerLeg", "LeftFoot"
    ]

    right_leg_humanoid_names = [
        "RightUpperLeg", "RightLowerLeg", "RightFoot"
    ]

    left_leg_bones = []
    right_leg_bones = []
    left_upper_leg_bone = None
    right_upper_leg_bone = None

    # Acquire the humanoid bone
    for bone_map in avatar_data.get("humanoidBones", []):
        humanoid_name = bone_map.get("humanoidBoneName")
        bone_name = bone_map.get("boneName")
        if humanoid_name == "LeftUpperLeg":
            left_upper_leg_bone = bone_name
        elif humanoid_name == "RightUpperLeg":
            right_upper_leg_bone = bone_name

        if humanoid_name in left_leg_humanoid_names:
            left_leg_bones.append(bone_name)
        elif humanoid_name in right_leg_humanoid_names:
            right_leg_bones.append(bone_name)

    # Retrieve starting from the head of the LeftUpperLeg
    left_leg_pivot_point = None
    if left_upper_leg_bone and left_upper_leg_bone in armature_obj.pose.bones:
        left_leg_pivot_point = armature_obj.matrix_world @ armature_obj.pose.bones[left_upper_leg_bone].head

    # Starting from the head of the RightUpperLeg
    right_leg_pivot_point = None
    if right_upper_leg_bone and right_upper_leg_bone in armature_obj.pose.bones:
        right_leg_pivot_point = armature_obj.matrix_world @ armature_obj.pose.bones[right_upper_leg_bone].head

    # Apply a -70-degree Y-axis rotation to the entire left leg (using the head of LeftUpperLeg as the origin)
    if left_leg_pivot_point:
        for bone_name in left_leg_bones:
            if bone_name and bone_name in armature_obj.pose.bones:
                bone = armature_obj.pose.bones[bone_name]
                current_world_matrix = armature_obj.matrix_world @ bone.matrix
                # Apply a -70-degree rotation around the Y-axis in the global coordinate system (using the head of the LeftUpperLeg as the origin)
                offset_matrix = mathutils.Matrix.Translation(left_leg_pivot_point * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(-70), 4, 'Y')
                bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

    # Apply a 70-degree rotation around the Y-axis to the entire right leg (using the head of RightUpperLeg as the pivot point)
    if right_leg_pivot_point:
        for bone_name in right_leg_bones:
            if bone_name and bone_name in armature_obj.pose.bones:
                bone = armature_obj.pose.bones[bone_name]
                current_world_matrix = armature_obj.matrix_world @ bone.matrix
                # Apply a 70-degree rotation around the Y-axis in the global coordinate system (using the head of the RightUpperLeg as the origin)
                offset_matrix = mathutils.Matrix.Translation(right_leg_pivot_point * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(70), 4, 'Y')
                bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

    bpy.ops.object.mode_set(mode='OBJECT')
    bpy.context.view_layer.objects.active = obj
    bpy.context.view_layer.update()

    #Retrieve the current evaluated mesh and save the state after armature deformation
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data
    crotch_shape_verts = np.array([v.co.copy() for v in eval_mesh.vertices])

    # Apply (or reset) a 70-degree Y-axis rotation to the entire left leg (using the head of LeftUpperLeg as the pivot point)
    if left_leg_pivot_point:
        for bone_name in left_leg_bones:
            if bone_name and bone_name in armature_obj.pose.bones:
                bone = armature_obj.pose.bones[bone_name]
                current_world_matrix = armature_obj.matrix_world @ bone.matrix
                # Apply a 70-degree rotation around the Y-axis in the global coordinate system (using the head of the LeftUpperLeg as the origin)
                offset_matrix = mathutils.Matrix.Translation(left_leg_pivot_point * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(70), 4, 'Y')
                bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

    # Apply (or reset) -70-degree Y-axis rotation to the entire right leg (using the head of RightUpperLeg as the origin)
    if right_leg_pivot_point:
        for bone_name in right_leg_bones:
            if bone_name and bone_name in armature_obj.pose.bones:
                bone = armature_obj.pose.bones[bone_name]
                current_world_matrix = armature_obj.matrix_world @ bone.matrix
                # Apply a -70-degree rotation around the Y-axis in the global coordinate system (using the head of the RightUpperLeg as the origin)
                offset_matrix = mathutils.Matrix.Translation(right_leg_pivot_point * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(-70), 4, 'Y')
                bone.matrix = armature_obj.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

    apply_modifiers_keep_shapekeys_with_temp(obj)

    if obj.data.shape_keys is None:
        obj.shape_key_add(name='Basis')

    if is_A_pose:
        # Create a temporary shape key
        shape_key_forA = obj.shape_key_add(name="WT_shape_forA.MFTemp")
        shape_key_forA.value = 0.0

        for i in range(len(A_pose_shape_verts)):
            shape_key_forA.data[i].co = A_pose_shape_verts[i]

    # Create a temporary shape key
    shape_key_forCrotch = obj.shape_key_add(name="WT_shape_forCrotch.MFTemp")
    shape_key_forCrotch.value = 0.0

    for i in range(len(crotch_shape_verts)):
        shape_key_forCrotch.data[i].co = crotch_shape_verts[i]

    restore_shape_key_state(obj, original_shape_key_state)



def process_missing_bone_weights(base_mesh: bpy.types.Object, clothing_armature: bpy.types.Object,
                               base_avatar_data: dict, clothing_avatar_data: dict, preserve_optional_humanoid_bones: bool) -> None:
    """
    Process weights for humanoid bones that exist in base avatar but not in clothing.
    """
    # Get bone names from clothing armature
    clothing_bone_names = set(bone.name for bone in clothing_armature.data.bones)

    # Create mappings for base avatar
    base_humanoid_to_bone = {}
    base_bone_to_humanoid = {}
    for bone_map in base_avatar_data.get("humanoidBones", []):
        if "humanoidBoneName" in bone_map and "boneName" in bone_map:
            base_humanoid_to_bone[bone_map["humanoidBoneName"]] = bone_map["boneName"]
            base_bone_to_humanoid[bone_map["boneName"]] = bone_map["humanoidBoneName"]

    # Create mappings for clothing
    clothing_humanoid_to_bone = {}
    for bone_map in clothing_avatar_data.get("humanoidBones", []):
        if "humanoidBoneName" in bone_map and "boneName" in bone_map:
            clothing_humanoid_to_bone[bone_map["humanoidBoneName"]] = bone_map["boneName"]

    # Create auxiliary bones mapping
    aux_bones_map = {}
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        bone_name = base_humanoid_to_bone.get(humanoid_bone)
        if bone_name:
            aux_bones_map[bone_name] = aux_set["auxiliaryBones"]

    # Create parent map from bone hierarchy
    parent_map = get_bone_parent_map(base_avatar_data["boneHierarchy"])

    # Process each humanoid bone from base avatar
    for humanoid_name, bone_name in base_humanoid_to_bone.items():
        # Skip if bone exists in clothing armature
        if clothing_humanoid_to_bone.get(humanoid_name) in clothing_bone_names:
            continue

        # Check if this bone should be preserved when preserve_optional_humanoid_bones is True
        if preserve_optional_humanoid_bones:
            should_preserve = False

            # Condition 1: Chest exists in clothing, UpperChest missing in clothing but exists in base
            if (humanoid_name == "UpperChest" and
                "Chest" in clothing_humanoid_to_bone and
                clothing_humanoid_to_bone["Chest"] in clothing_bone_names and
                "UpperChest" not in clothing_humanoid_to_bone and
                "UpperChest" in base_humanoid_to_bone):
                should_preserve = True
                print("Preserving UpperChest bone weights due to Chest condition")

            # Condition 2: LeftLowerLeg exists in clothing, LeftFoot missing in clothing but exists in base
            elif (humanoid_name == "LeftFoot" and
                  "LeftLowerLeg" in clothing_humanoid_to_bone and
                  clothing_humanoid_to_bone["LeftLowerLeg"] in clothing_bone_names and
                  "LeftFoot" not in clothing_humanoid_to_bone and
                  "LeftFoot" in base_humanoid_to_bone):
                should_preserve = True
                print("Preserving LeftFoot bone weights due to LeftLowerLeg condition")

            # Condition 2: RightLowerLeg exists in clothing, RightFoot missing in clothing but exists in base
            elif (humanoid_name == "RightFoot" and
                  "RightLowerLeg" in clothing_humanoid_to_bone and
                  clothing_humanoid_to_bone["RightLowerLeg"] in clothing_bone_names and
                  "RightFoot" not in clothing_humanoid_to_bone and
                  "RightFoot" in base_humanoid_to_bone):
                should_preserve = True
                print("Preserving RightFoot bone weights due to RightLowerLeg condition")

            # Condition 3: LeftLowerLeg or LeftFoot exists in clothing, LeftToe missing in clothing but exists in base
            elif (humanoid_name == "LeftToe" and
                  (("LeftLowerLeg" in clothing_humanoid_to_bone and clothing_humanoid_to_bone["LeftLowerLeg"] in clothing_bone_names) or
                   ("LeftFoot" in clothing_humanoid_to_bone and clothing_humanoid_to_bone["LeftFoot"] in clothing_bone_names)) and
                  "LeftToe" not in clothing_humanoid_to_bone and
                  "LeftToe" in base_humanoid_to_bone):
                should_preserve = True
                print("Preserving LeftToe bone weights due to LeftLowerLeg/LeftFoot condition")

            # Condition 3: RightLowerLeg or RightFoot exists in clothing, RightToe missing in clothing but exists in base
            elif (humanoid_name == "RightToe" and
                  (("RightLowerLeg" in clothing_humanoid_to_bone and clothing_humanoid_to_bone["RightLowerLeg"] in clothing_bone_names) or
                   ("RightFoot" in clothing_humanoid_to_bone and clothing_humanoid_to_bone["RightFoot"] in clothing_bone_names)) and
                  "RightToe" not in clothing_humanoid_to_bone and
                  "RightToe" in base_humanoid_to_bone):
                should_preserve = True
                print("Preserving RightToe bone weights due to RightLowerLeg/RightFoot condition")

            elif (humanoid_name == "LeftBreast" and
                  "LeftBreast" not in clothing_humanoid_to_bone and
                  ("Chest" in clothing_humanoid_to_bone or "UpperChest" in clothing_humanoid_to_bone) and
                  (clothing_humanoid_to_bone["Chest"] in clothing_bone_names or clothing_humanoid_to_bone["UpperChest"] in clothing_bone_names) and
                  "LeftBreast" in base_humanoid_to_bone):
                should_preserve = True
                print("Preserving LeftBreast bone weights due to Chest condition")

            elif (humanoid_name == "RightBreast" and
                  "RightBreast" not in clothing_humanoid_to_bone and
                  ("Chest" in clothing_humanoid_to_bone or "UpperChest" in clothing_humanoid_to_bone) and
                  (clothing_humanoid_to_bone["Chest"] in clothing_bone_names or clothing_humanoid_to_bone["UpperChest"] in clothing_bone_names) and
                  "RightBreast" in base_humanoid_to_bone):
                should_preserve = True
                print("Preserving RightBreast bone weights due to Chest condition")

            if should_preserve:
                print(f"Skipping processing for preserved bone: {humanoid_name} ({bone_name})")
                continue

        print(f"Processing missing humanoid bone: {humanoid_name} ({bone_name})")

        # Find parent that exists in clothing armature
        current_bone = bone_name
        target_bone = None

        while current_bone and not target_bone:
            parent_bone = parent_map.get(current_bone)
            if not parent_bone:
                break

            parent_humanoid = base_bone_to_humanoid.get(parent_bone)
            if parent_humanoid and clothing_humanoid_to_bone.get(parent_humanoid) in clothing_bone_names:
                target_bone = base_humanoid_to_bone[parent_humanoid]
                break

            current_bone = parent_bone

        if target_bone:
            # Transfer main bone weights
            source_group = base_mesh.vertex_groups.get(bone_name)
            if source_group:
                merge_weights_to_parent(base_mesh, bone_name, target_bone)

                # Transfer auxiliary bone weights
                for aux_bone in aux_bones_map.get(bone_name, []):
                    if aux_bone in base_mesh.vertex_groups:
                        merge_weights_to_parent(base_mesh, aux_bone, target_bone)

                # Remove source groups
                if bone_name in base_mesh.vertex_groups:
                    base_mesh.vertex_groups.remove(base_mesh.vertex_groups[bone_name])
                for aux_bone in aux_bones_map.get(bone_name, []):
                    if aux_bone in base_mesh.vertex_groups:
                        base_mesh.vertex_groups.remove(base_mesh.vertex_groups[aux_bone])

def update_base_avatar_weights(base_mesh: bpy.types.Object, clothing_armature: bpy.types.Object,
                             base_avatar_data: dict, clothing_avatar_data: dict, preserve_optional_humanoid_bones: bool) -> None:
    """
    Update base avatar weights based on clothing armature structure.

    Parameters:
        base_mesh: Base avatar mesh object
        clothing_armature: Clothing armature object
        base_avatar_data: Base avatar data
        clothing_avatar_data: Clothing avatar data
    """
    # Then process missing bone weights
    process_missing_bone_weights(base_mesh, clothing_armature, base_avatar_data, clothing_avatar_data, preserve_optional_humanoid_bones)



def normalize_bone_weights(obj: bpy.types.Object, avatar_data: dict) -> None:
    """
    Normalize vertex weights related to mesh bone deformation.

    Parameters:
        obj: Mesh object
        avatar_data: Avatar data
    """
    if obj.type != 'MESH':
        return

    # Acquire the bong group to be normalized
    target_groups = set()
    # Add Humanoid Bone
    for bone_map in avatar_data.get("humanoidBones", []):
        if "boneName" in bone_map:
            target_groups.add(bone_map["boneName"])

    # Add auxiliary bones
    for aux_set in avatar_data.get("auxiliaryBones", []):
        for aux_bone in aux_set.get("auxiliaryBones", []):
            target_groups.add(aux_bone)

    # Process each vertex
    for vert in obj.data.vertices:
        # Calculate the total weight of the target group
        total_weight = 0.0
        weights = {}

        for g in vert.groups:
            group_name = obj.vertex_groups[g.group].name
            if group_name in target_groups:
                total_weight += g.weight
                weights[group_name] = g.weight

        # Weight Normalization
        for group_name, weight in weights.items():
            normalized_weight = weight / total_weight
            obj.vertex_groups[group_name].add([vert.index], normalized_weight, 'REPLACE')



def create_hinge_bone_group(obj: bpy.types.Object, armature: bpy.types.Object, avatar_data: dict) -> None:
    """
    Create a hinge bone group.
    """
    bone_groups = get_humanoid_and_auxiliary_bone_groups(avatar_data)

    # Create a target group that includes the clothing armature's bone group
    all_deform_groups = set(bone_groups)
    if armature:
        all_deform_groups.update(bone.name for bone in armature.data.bones)

    # Save the weights for the groups excluding bone_groups from original_groups
    original_non_humanoid_groups = all_deform_groups - bone_groups

    cloth_bm = get_evaluated_mesh(obj)
    cloth_bm.verts.ensure_lookup_table()
    cloth_bm.faces.ensure_lookup_table()
    vertex_coords = np.array([v.co for v in cloth_bm.verts])
    kdtree = cKDTree(vertex_coords)

    hinge_bone_group = obj.vertex_groups.new(name="HingeBone")
    for bone_name in original_non_humanoid_groups:
        bone = armature.pose.bones.get(bone_name)
        if bone.parent and bone.parent.name in bone_groups:
            group_index = obj.vertex_groups.find(bone_name)
            print(f"Processing hinge bone: {bone_name}")
            print(f"Bone parent: {bone.parent.name}")
            print(f"Group index: {group_index}")
            if group_index != -1:
                bone_head = armature.matrix_world @ bone.head
                neighbor_indices = kdtree.query_ball_point(bone_head, 0.01)
                for index in neighbor_indices:
                    for g in obj.data.vertices[index].groups:
                        if g.group == group_index:
                            weight = g.weight
                            hinge_bone_group.add([index], weight, 'REPLACE')
                            print(f"Added weight to {index}")
                            break


def get_humanoid_and_auxiliary_bones(avatar_data: dict) -> set:
    """
    Get a set of all humanoid and auxiliary bone names from avatar data.

    Parameters:
        avatar_data: Avatar data containing bone information

    Returns:
        Set of bone names
    """
    bone_names = set()

    # Add humanoid bones
    for bone_map in avatar_data.get("humanoidBones", []):
        if "boneName" in bone_map:
            bone_names.add(bone_map["boneName"])

    # Add auxiliary bones
    for aux_set in avatar_data.get("auxiliaryBones", []):
        for aux_bone in aux_set.get("auxiliaryBones", []):
            bone_names.add(aux_bone)

    return bone_names

def copy_bone_transform(source_bone: bpy.types.EditBone, target_bone: bpy.types.EditBone) -> None:
    """
    Copy transformation data from source bone to target bone.

    Parameters:
        source_bone: Source edit bone
        target_bone: Target edit bone
    """
    target_bone.head = source_bone.head.copy()
    target_bone.tail = source_bone.tail.copy()
    target_bone.roll = source_bone.roll
    target_bone.matrix = source_bone.matrix.copy()
    target_bone.length = source_bone.length

def find_humanoid_parent_in_clothing(bone_name: str, clothing_bones_to_humanoid: dict, clothing_armature: bpy.types.Object) -> Optional[str]:
    """
    Trace the parent of the bone in the clothing_armature and return the first humanoid bone found

    Parameters:
        bone_name: Start bone name
        clothing_bones_to_humanoid: Conversion dictionary from bone names to Humanoid bone names
        clothing_armature: Clothing Armature Object

    Returns:
        Optional[str]: Name of the found parent Humanoid bone, None if not found
    """
    current_bone = clothing_armature.data.bones.get(bone_name)
    while current_bone and current_bone.parent:
        parent_bone = current_bone.parent
        if parent_bone.name in clothing_bones_to_humanoid:
            return clothing_bones_to_humanoid[parent_bone.name]
        current_bone = parent_bone
    return None

def find_humanoid_parent_in_hierarchy(bone_name: str, clothing_avatar_data: dict, base_avatar_data: dict) -> Optional[str]:
    """
    Trace the parent from bone_name in the boneHierarchy of clothing_avatar_data and return the first humanoid bone that also exists in base_armature

    Parameters:
        bone_name: Start bone name
        clothing_avatar_data: Clothing avatar data
        base_avatar_data: Base avatar data

    Returns:
        Optional[str]: Name of the found parent Humanoid bone, None if not found
    """
    # Get the humanoidBoneName for bone_name from the humanoidBones of clothing_avatar_data
    clothing_bones_to_humanoid = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                                for bone_map in clothing_avatar_data["humanoidBones"]}
    base_humanoid_bones = {bone_map["humanoidBoneName"] for bone_map in base_avatar_data["humanoidBones"]}

    def find_bone_in_hierarchy(hierarchy_node, target_name):
        """Recursive function to find a bone within a hierarchy"""
        if hierarchy_node["name"] == target_name:
            return hierarchy_node
        for child in hierarchy_node.get("children", []):
            result = find_bone_in_hierarchy(child, target_name)
            if result:
                return result
        return None

    def find_parent_path(hierarchy_node, target_name, path=[]):
        """Recursive function to find the path to the target bone"""
        current_path = path + [hierarchy_node["name"]]
        if hierarchy_node["name"] == target_name:
            return current_path
        for child in hierarchy_node.get("children", []):
            result = find_parent_path(child, target_name, current_path)
            if result:
                return result
        return None

    # Get the path to bone_name in the boneHierarchy
    bone_hierarchy = clothing_avatar_data.get("boneHierarchy")
    if not bone_hierarchy:
        return None

    path = find_parent_path(bone_hierarchy, bone_name)
    if not path:
        return None

    # Traverse the path in reverse order from the parent
    path.reverse()

    # Search for the humanoid bone from yourself to your parent
    for parent_bone_name in path:
        if parent_bone_name in clothing_bones_to_humanoid:
            humanoid_name = clothing_bones_to_humanoid[parent_bone_name]
            if humanoid_name in base_humanoid_bones:
                return humanoid_name

    return None


def replace_humanoid_bones(base_armature: bpy.types.Object, clothing_armature: bpy.types.Object,
                        base_avatar_data: dict, clothing_avatar_data: dict, preserve_humanoid_bones: bool, base_pose_filepath: Optional[str], clothing_meshes: list, process_upper_chest: bool) -> None:

    current_active = bpy.context.active_object
    current_mode = current_active.mode if current_active else 'OBJECT'

    # Create mappings
    base_humanoid_map = {bone_map["humanoidBoneName"]: bone_map["boneName"]
                        for bone_map in base_avatar_data["humanoidBones"]}
    clothing_humanoid_map = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                            for bone_map in clothing_avatar_data["humanoidBones"]}
    clothing_bones_to_humanoid = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                                for bone_map in clothing_avatar_data["humanoidBones"]}

    # Create reverse mapping for finding bones by humanoid names
    base_bone_to_humanoid = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                            for bone_map in base_avatar_data["humanoidBones"]}

    # Humanoid Bone Alignment
    clothing_humanoid_bones = {bone_map["humanoidBoneName"] for bone_map in clothing_avatar_data["humanoidBones"]}
    base_humanoid_bones = {bone_map["humanoidBoneName"] for bone_map in base_avatar_data["humanoidBones"]}

    # Identify Humanoid bones not present in base_avatar_data
    # missing_humanoid_bones = clothing_humanoid_bones - base_humanoid_bones
    missing_humanoid_bones = {}

    # Map auxiliary bones to humanoid bones
    aux_to_humanoid = {}
    for aux_set in clothing_avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        # Exclude auxiliary bones for Humanoid bones not present in base_avatar_data
        if humanoid_bone not in missing_humanoid_bones:
            for aux_bone in aux_set["auxiliaryBones"]:
                aux_to_humanoid[aux_bone] = humanoid_bone

    # Map humanoid bones to auxiliary bones
    humanoid_to_aux = {}
    for aux_set in clothing_avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        # Exclude auxiliary bones for Humanoid bones not present in base_avatar_data
        if humanoid_bone not in missing_humanoid_bones:
            humanoid_to_aux[humanoid_bone] = aux_set["auxiliaryBones"]

    humanoid_to_aux_base = {}
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        humanoid_to_aux_base[aux_set["humanoidBoneName"]] = aux_set["auxiliaryBones"]

    # Exclude Humanoid bones and their auxiliary bones that do not exist in base_avatar_data from bones_to_replace
    bones_to_replace = set()
    for bone_map in clothing_avatar_data["humanoidBones"]:
        if bone_map["humanoidBoneName"] not in missing_humanoid_bones:
            bones_to_replace.add(bone_map["boneName"])

    for aux_set in clothing_avatar_data.get("auxiliaryBones", []):
        if aux_set["humanoidBoneName"] not in missing_humanoid_bones:
            bones_to_replace.update(aux_set["auxiliaryBones"])

    print(f"bones_to_replace: {bones_to_replace}")

    base_bones = get_humanoid_and_auxiliary_bone_groups_with_intermediate(base_armature, base_avatar_data)

    # Get humanoid bones that should be preserved
    if preserve_humanoid_bones:
        humanoid_bones_to_preserve = {bone_name for bone_name, humanoid_name
                                    in clothing_bones_to_humanoid.items()
                                    if humanoid_name not in missing_humanoid_bones}
    else:
        humanoid_bones_to_preserve = set()

    # Get base mesh and create BVH tree
    base_mesh = bpy.data.objects.get("Body.BaseAvatar")
    if not base_mesh:
        raise Exception("Body.BaseAvatar not found")

    bm = bmesh.new()
    bm.from_mesh(base_mesh.data)
    bm.faces.ensure_lookup_table()
    bm.transform(base_mesh.matrix_world)
    bvh = BVHTree.FromBMesh(bm)

    # Save Armature Modifier settings and temporarily remove
    armature_modifiers = []
    clothing_obj_list = []
    for obj in bpy.data.objects:
        if obj.type == 'MESH':
            for modifier in obj.modifiers[:]:  # Use a copy of the list
                if modifier.type == 'ARMATURE' and modifier.object == clothing_armature:
                    mod_settings = {
                        'object': obj,
                        'name': modifier.name,
                        'target': modifier.object,
                        'vertex_group': modifier.vertex_group,
                        'invert_vertex_group': modifier.invert_vertex_group,
                        'use_vertex_groups': modifier.use_vertex_groups,
                        'use_bone_envelopes': modifier.use_bone_envelopes,
                        'use_deform_preserve_volume': modifier.use_deform_preserve_volume
                    }
                    armature_modifiers.append(mod_settings)
                    obj.modifiers.remove(modifier)
                    clothing_obj_list.append(obj)

    if base_pose_filepath:
        print(f"Applying clothing base pose from {base_pose_filepath}")
        add_pose_from_json(clothing_armature, base_pose_filepath, clothing_avatar_data, invert=True)
        apply_pose_as_rest(clothing_armature)

    # Get clothing bone positions and their original parents
    clothing_bone_data = {}
    clothing_matrix_world = clothing_armature.matrix_world

    for bone in clothing_armature.pose.bones:
        if bone.parent and bone.parent.name in bones_to_replace and bone.name not in bones_to_replace:
            head_pos = clothing_matrix_world @ bone.head

            # First, get the humanoid name of bone.parent
            parent_humanoid = None
            if bone.parent.name in clothing_humanoid_map:
                parent_humanoid = clothing_humanoid_map[bone.parent.name]
            elif bone.parent.name in aux_to_humanoid:
                parent_humanoid = aux_to_humanoid[bone.parent.name]

            # If parent_humanoid does not exist in base_humanoid_map,
            # Trace the parent in clothing_avatar_data and find the first humanoid bone that also exists in base_avatar_data
            if parent_humanoid and parent_humanoid not in base_humanoid_map:
                # parent_humanoid = find_humanoid_parent_in_clothing(bone.parent.name, clothing_bones_to_humanoid, clothing_armature)
                parent_humanoid = find_humanoid_parent_in_hierarchy(bone.parent.name, clothing_avatar_data, base_avatar_data)

            if parent_humanoid and parent_humanoid in base_humanoid_map:
                # Get candidate bone
                candidate_bones = {base_humanoid_map[parent_humanoid]}
                if parent_humanoid in humanoid_to_aux_base:
                    candidate_bones.update(humanoid_to_aux_base[parent_humanoid])

                # Handling when UpperChest exists in Chest
                sub_parent_humanoid = None
                if parent_humanoid == 'Chest' and 'UpperChest' in base_humanoid_map and process_upper_chest:
                    sub_parent_humanoid = base_humanoid_map['UpperChest']
                    candidate_bones.add(sub_parent_humanoid)
                    if 'UpperChest' in humanoid_to_aux_base:
                        candidate_bones.update(humanoid_to_aux_base['UpperChest'])

                clothing_bone_data[bone.name] = {
                    'head_pos': head_pos,
                    'candidate_bones': candidate_bones,
                    'parent_humanoid': base_humanoid_map[parent_humanoid],
                    'sub_parent_humanoid': sub_parent_humanoid
                }

    base_group_index_to_name = {group.index: group.name for group in base_mesh.vertex_groups}

    # Find parent bones using only the candidate bones
    parent_bones = {}
    for bone_name, data in clothing_bone_data.items():
        head_pos = data['head_pos']
        candidate_bones = data['candidate_bones']
        parent_humanoid = data['parent_humanoid']
        sub_parent_humanoid = data.get('sub_parent_humanoid', None)

        # Additional method: Retrieve vertices from clothing_meshes where the target bone's weight exceeds a certain threshold, and from base_mesh vertices close to them
        # Calculate the weight scores for candidate_bones
        bone_scores = defaultdict(float)

        weighted_vertices = []
        for mesh_obj in clothing_meshes:
            if mesh_obj.type != 'MESH':
                continue

            vg_lookup = {vg.name: vg.index for vg in mesh_obj.vertex_groups}
            if bone_name not in vg_lookup:
                continue

            target_group_index = vg_lookup[bone_name]
            mesh_data = mesh_obj.data

            mesh_world_matrix = mesh_obj.matrix_world

            for vertex in mesh_data.vertices:
                weight = 0.0
                for g in vertex.groups:
                    if g.group == target_group_index:
                        weight = g.weight
                        break
                if weight >= 0.001:
                    vertex_world_co = mesh_world_matrix @ vertex.co
                    weighted_vertices.append((vertex_world_co, weight))
            print(f"bone_name: {bone_name}, weighted_vertices: {len(weighted_vertices)}")

        if weighted_vertices:
            weighted_vertices.sort(key=lambda item: item[1], reverse=True)
            top_vertices = weighted_vertices[:100]

            for vertex_world_co, _ in top_vertices:
                closest_point, _, face_idx, _ = bvh.find_nearest(vertex_world_co)
                if closest_point is None or face_idx is None:
                    continue

                face = bm.faces[face_idx]
                vertex_indices = [v.index for v in face.verts]
                closest_vert_idx = min(
                    vertex_indices,
                    key=lambda idx: (base_mesh.data.vertices[idx].co - closest_point).length
                )

                vertex = base_mesh.data.vertices[closest_vert_idx]
                for group_element in vertex.groups:
                    group_name = base_group_index_to_name.get(group_element.group)
                    if group_name in candidate_bones:
                        bone_scores[group_name] += group_element.weight

        chosen_parent = None
        if bone_scores:
            print(f"bone_scores: {bone_scores}")
            chosen_parent = max(bone_scores.items(), key=lambda item: item[1])[0]

        # if not chosen_parent:
        #     # Existing Method: Using vertex distance and bone distance
        #     closest_point, normal, face_idx, vertex_distance = bvh.find_nearest(head_pos)
        #     closest_bone = None
        #     min_vertex_weight_distance = float('inf')

        #     if closest_point and face_idx is not None:
        #         face = bm.faces[face_idx]
        #         vertex_indices = [v.index for v in face.verts]
        #         closest_vert_idx = min(vertex_indices,
        #                             key=lambda idx: (base_mesh.data.vertices[idx].co - closest_point).length)

        #         max_weight = 0
        #         vertex = base_mesh.data.vertices[closest_vert_idx]
        #         for group_element in vertex.groups:
        #             group_name = base_group_index_to_name.get(group_element.group)
        #             if group_name in candidate_bones:
        #                 weight = group_element.weight
        #                 if weight > max_weight:
        #                     max_weight = weight
        #                     closest_bone = group_name
        #                     min_vertex_weight_distance = vertex_distance

        #     min_bone_distance = float('inf')
        #     closest_bone_by_distance = None

        #     for bone in base_armature.pose.bones:
        #         if bone.name in candidate_bones:
        #             bone_head_world = base_armature.matrix_world @ bone.head
        #             distance = (head_pos - bone_head_world).length
        #             if distance < min_bone_distance:
        #                 min_bone_distance = distance
        #                 closest_bone_by_distance = bone.name

        #     if closest_bone_by_distance and min_bone_distance < min_vertex_weight_distance:
        #         chosen_parent = closest_bone_by_distance
        #     elif closest_bone:
        #         chosen_parent = closest_bone

        if chosen_parent and chosen_parent == bone_name and bone_name in clothing_armature.data.bones and clothing_armature.data.bones.get(bone_name).parent:
            chosen_parent = clothing_armature.data.bones.get(bone_name).parent.name
            if chosen_parent not in candidate_bones:
                chosen_parent = None

        if chosen_parent:
            parent_bones[bone_name] = chosen_parent
            print(f"bone_name: {bone_name}, chosen_parent: {chosen_parent}")
        else:
            # If chosen_parent is not found, compare distances if sub_parent_humanoid exists
            if sub_parent_humanoid:
                parent_humanoid_bone = base_armature.pose.bones.get(parent_humanoid)
                sub_parent_humanoid_bone = base_armature.pose.bones.get(sub_parent_humanoid)

                if parent_humanoid_bone and sub_parent_humanoid_bone:
                    parent_distance = (head_pos - (base_armature.matrix_world @ parent_humanoid_bone.head)).length
                    sub_parent_distance = (head_pos - (base_armature.matrix_world @ sub_parent_humanoid_bone.head)).length

                    if sub_parent_distance < parent_distance:
                        parent_bones[bone_name] = sub_parent_humanoid
                        print(f"bone_name: {bone_name}, chosen_parent: {sub_parent_humanoid} (sub_parent, distance: {sub_parent_distance:.4f})")
                    else:
                        parent_bones[bone_name] = parent_humanoid
                        print(f"bone_name: {bone_name}, chosen_parent: {parent_humanoid} (fallback, distance: {parent_distance:.4f})")
                else:
                    parent_bones[bone_name] = parent_humanoid
                    print(f"bone_name: {bone_name}, chosen_parent: {parent_humanoid} (fallback)")
            else:
                parent_bones[bone_name] = parent_humanoid
                print(f"bone_name: {bone_name}, chosen_parent: {parent_humanoid} (fallback)")

    bm.free()

    # Replace bones
    bpy.context.view_layer.objects.active = clothing_armature
    bpy.ops.object.mode_set(mode='EDIT')
    clothing_edit_bones = clothing_armature.data.edit_bones

    # Store children to update
    children_to_update = []
    for bone in clothing_edit_bones:
        if bone.parent and bone.parent.name in bones_to_replace and bone.name not in bones_to_replace:
            children_to_update.append(bone.name)

    # Store base bone parents
    base_bone_parents = {}
    bpy.context.view_layer.objects.active = base_armature
    bpy.ops.object.mode_set(mode='EDIT')
    for bone in base_armature.data.edit_bones:
        if bone.name in base_bones:
            base_bone_parents[bone.name] = bone.parent.name if bone.parent and bone.parent.name in base_bones else None

    print(base_bone_parents)

    bpy.context.view_layer.objects.active = clothing_armature
    bpy.ops.object.mode_set(mode='EDIT')

    # Process bones to preserve or delete
    original_bone_data = {}
    for bone_name in bones_to_replace:
        if bone_name in clothing_edit_bones:
            if bone_name in humanoid_bones_to_preserve:
                # Preserve and rename Humanoid bones
                orig_bone = clothing_edit_bones[bone_name]
                new_name = f"origORS_{bone_name}"
                bone_data = {
                    'head': orig_bone.head.copy(),
                    'tail': orig_bone.tail.copy(),
                    'roll': orig_bone.roll,
                    'matrix': orig_bone.matrix.copy(),
                    'new_name': new_name,
                    'humanoid_name': clothing_bones_to_humanoid[bone_name]  # Store the humanoid name
                }

                original_bone_data[bone_name] = bone_data
                orig_bone.name = new_name
            else:
                # Delete non-Humanoid bones
                clothing_edit_bones.remove(clothing_edit_bones[bone_name])

    # Create new bones
    new_bones = {}
    for bone_name in base_bones:
        source_bone = base_armature.data.edit_bones.get(bone_name)
        if source_bone:
            new_bone = clothing_edit_bones.new(name=bone_name)
            copy_bone_transform(source_bone, new_bone)
            new_bones[bone_name] = new_bone

    # Set parent relationships for new bones
    for bone_name, new_bone in new_bones.items():
        parent_name = base_bone_parents.get(bone_name)
        if parent_name and parent_name in new_bones:
            new_bone.parent = new_bones[parent_name]

    # Make original humanoid bones children of new bones based on boneHierarchy
    for orig_bone_name, data in original_bone_data.items():
        orig_bone = clothing_edit_bones[data['new_name']]
        humanoid_name = data['humanoid_name']  # Get the humanoid name for matching

        # Find parent using boneHierarchy
        parent_humanoid_name = find_humanoid_parent_in_hierarchy(orig_bone_name, clothing_avatar_data, base_avatar_data)

        if parent_humanoid_name:
            # Find the new bone with matching parent humanoid name
            matched_new_bone = None
            for new_bone_name, new_bone in new_bones.items():
                if new_bone_name in base_bone_to_humanoid:
                    if base_bone_to_humanoid[new_bone_name] == parent_humanoid_name:
                        matched_new_bone = new_bone
                        break

            if matched_new_bone:
                orig_bone.parent = matched_new_bone
            else:
                print(f"Warning: No matching new bone found for parent humanoid bone {parent_humanoid_name}")
        else:
            # Fallback to original matching logic
            matched_new_bone = None
            for new_bone_name, new_bone in new_bones.items():
                if new_bone_name in base_bone_to_humanoid:
                    if base_bone_to_humanoid[new_bone_name] == humanoid_name:
                        matched_new_bone = new_bone
                        break

            if matched_new_bone:
                orig_bone.parent = matched_new_bone
            else:
                print(f"Warning: No matching new bone found for humanoid bone {humanoid_name}")

    # If the parent_bone is a HumanoidBone and there exists a subHumanoidBone whose HumanoidBoneName matches, replace it with the subHumanoidBone
    if "subHumanoidBones" in base_avatar_data:
        sub_humanoid_bones = {}
        for sub_humanoid_bone in base_avatar_data["subHumanoidBones"]:
            sub_humanoid_bones[sub_humanoid_bone["humanoidBoneName"]] = sub_humanoid_bone["boneName"]
        for bone_name, parent_name in parent_bones.items():
            if parent_name in base_bone_to_humanoid:
                if base_bone_to_humanoid[parent_name] in sub_humanoid_bones.keys():
                    parent_bones[bone_name] = sub_humanoid_bones[base_bone_to_humanoid[parent_name]]

    # Update children parents
    for child_name in children_to_update:
        child_bone = clothing_edit_bones.get(child_name)
        print(f"child_name: {child_name}, child_bone: {child_bone.name if child_bone else None}")
        if child_bone:
            new_parent_name = parent_bones.get(child_name)
            print(f"child_name: {child_name}, new_parent_name: {new_parent_name}")
            if new_parent_name and new_parent_name in clothing_edit_bones:
                child_bone.parent = clothing_edit_bones[new_parent_name]
                print(f"child_name: {child_name}, new_parent_name: {new_parent_name}")

    bpy.ops.object.mode_set(mode='OBJECT')

    if base_pose_filepath:
        print(f"Applying base pose from {base_pose_filepath}")
        add_pose_from_json(clothing_armature, base_pose_filepath, base_avatar_data, invert=False)
        for obj in clothing_obj_list:
            inverse_bone_deform_all_vertices(clothing_armature, obj)
        add_pose_from_json(clothing_armature, base_pose_filepath, base_avatar_data, invert=True)
        apply_pose_as_rest(clothing_armature)

    # Restore Armature Modifier
    for mod_settings in armature_modifiers:
        obj = mod_settings['object']
        modifier = obj.modifiers.new(name=mod_settings['name'], type='ARMATURE')
        modifier.object = mod_settings['target']
        modifier.vertex_group = mod_settings['vertex_group']
        modifier.invert_vertex_group = mod_settings['invert_vertex_group']
        modifier.use_vertex_groups = mod_settings['use_vertex_groups']
        modifier.use_bone_envelopes = mod_settings['use_bone_envelopes']
        modifier.use_deform_preserve_volume = mod_settings['use_deform_preserve_volume']

    bpy.context.view_layer.objects.active = current_active
    if current_mode != 'OBJECT':
        bpy.ops.object.mode_set(mode=current_mode)



def load_vertex_group(obj, filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        payload = json.load(f)

    group_name = payload.get("vertex_group_name")
    weights = payload.get("weights", [])
    if not group_name:
        print("The JSON does not contain the vertex group name.")
        return group_name

    vg = obj.vertex_groups.get(group_name)
    if vg is None:
        vg = obj.vertex_groups.new(name=group_name)
    else:
        indices = [v.index for v in obj.data.vertices]
        vg.remove(indices)

    missing_vertices = []
    for record in weights:
        vidx = record.get("vertex_index")
        weight = record.get("weight")
        if vidx is None or weight is None:
            continue
        if vidx >= len(obj.data.vertices):
            missing_vertices.append(vidx)
            continue
        vg.add([vidx], weight, 'REPLACE')

    obj.vertex_groups.active = vg
    print(f"{group_name} has been restored from {filepath}.")
    if missing_vertices:
        print(f"Non-existent vertex index: {missing_vertices}")
    return group_name


def reset_shape_keys(obj):
    # Check if the object has a shape key
    if obj.data.shape_keys is not None:
        # Loop the key blocks of the shape key
        for kb in obj.data.shape_keys.key_blocks:
            # Set all values other than the base shape (Basis) to 0
            if kb.name != "Basis":
                kb.value = 0.0

def normalize_vertex_weights(obj):
    """
    Normalize the bone weights of the specified mesh object.
    Args:
        obj: Normalized mesh object
    """
    if obj.type != 'MESH':
        print(f"Error: {obj.name} is not a mesh object")
        return

    # Check if a vertex group exists
    if not obj.vertex_groups:
        print(f"Warning: {obj.name} has no vertex groups")
        return

    # Verify that each vertex belongs to at least one group
    for vert in obj.data.vertices:
        if not vert.groups:
            print(f"Warning: Vertex {vert.index} in {obj.name} has no weights")

    # Verifying the Armature Modifier
    has_armature = any(mod.type == 'ARMATURE' for mod in obj.modifiers)
    if not has_armature:
        print(f"Error: {obj.name} has no Armature modifier")
        return

    # Clear all selections
    bpy.ops.object.select_all(action='DESELECT')

    # Set active objects
    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.mode_set(mode='OBJECT')
    obj.select_set(True)
    bpy.context.view_layer.objects.active = obj

    # Perform weight normalization
    bpy.ops.object.vertex_group_normalize_all(
        group_select_mode='BONE_DEFORM',
        lock_active=False
    )
    print(f"Normalized weights for {obj.name}")



def merge_auxiliary_to_humanoid_weights(mesh_obj: bpy.types.Object, avatar_data: dict) -> None:
    """Create missing Humanoid bone vertex groups and merge auxiliary weights."""
    # Map auxiliary bones to their Humanoid bones
    aux_to_humanoid = {}
    for aux_set in avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        bone_name = None
        # Get the actual bone name for the Humanoid bone
        for bone_map in avatar_data.get("humanoidBones", []):
            if bone_map["humanoidBoneName"] == humanoid_bone:
                bone_name = bone_map["boneName"]
                break
        if bone_name:
            for aux_bone in aux_set["auxiliaryBones"]:
                aux_to_humanoid[aux_bone] = bone_name

    # Check each auxiliary bone vertex group
    for aux_bone in aux_to_humanoid:
        if aux_bone in mesh_obj.vertex_groups:
            humanoid_bone = aux_to_humanoid[aux_bone]
            # Create Humanoid bone group if it doesn't exist
            if humanoid_bone not in mesh_obj.vertex_groups:
                print(f"Creating missing Humanoid bone group {humanoid_bone} for {mesh_obj.name}")
                mesh_obj.vertex_groups.new(name=humanoid_bone)

            # Get the vertex groups
            aux_group = mesh_obj.vertex_groups[aux_bone]
            humanoid_group = mesh_obj.vertex_groups[humanoid_bone]

            # Transfer weights from auxiliary to humanoid group
            for vert in mesh_obj.data.vertices:
                aux_weight = 0
                for group in vert.groups:
                    if group.group == aux_group.index:
                        aux_weight = group.weight
                        break

                if aux_weight > 0:
                    # Add weight to humanoid bone group
                    humanoid_group.add([vert.index], aux_weight, 'ADD')

            # Remove auxiliary bone vertex group
            mesh_obj.vertex_groups.remove(aux_group)
            print(f"Merged weights from {aux_bone} to {humanoid_bone} in {mesh_obj.name}")


def save_vertex_weights(mesh_obj: bpy.types.Object) -> dict:
    """
    Record the weights for all vertex groups of the object (including empty groups)

    Parameters:
        mesh_obj: Mesh object

    Returns:
        Dictionary of saved weight information (vertex_weights, existing_groups, vertex_ids)
    """
    weights_data = {
        'vertex_weights': {},
        'existing_groups': set(),
        'vertex_ids': {}
    }

    # Record all existing vertex group names
    for group in mesh_obj.vertex_groups:
        weights_data['existing_groups'].add(group.name)

    # Create a custom integer attribute at the vertex (delete and recreate if it already exists)
    mesh = mesh_obj.data
    custom_attr_name = "original_vertex_id"

    # Delete existing custom attributes
    if custom_attr_name in mesh.attributes:
        mesh.attributes.remove(mesh.attributes[custom_attr_name])

    # Create a new integer-type custom attribute
    custom_attr = mesh.attributes.new(name=custom_attr_name, type='INT', domain='POINT')

    # Record the weight and vertex ID for each vertex
    for vert in mesh.vertices:
        vertex_weights = {}
        for group in vert.groups:
            group_name = mesh_obj.vertex_groups[group.group].name
            vertex_weights[group_name] = group.weight

        # Record vertex weights (including empty cases)
        weights_data['vertex_weights'][vert.index] = vertex_weights

        # Set the current vertex ID to the custom attribute
        custom_attr.data[vert.index].value = vert.index

        # Record vertex ID in weights_data
        weights_data['vertex_ids'][vert.index] = vert.index

    print(f"Saved vertex weights for {len(mesh.vertices)} vertices with original IDs in {mesh_obj.name}")

    return weights_data


def restore_vertex_weights(mesh_obj: bpy.types.Object, weights_data: dict) -> None:
    """
    Restore vertex group weights using saved weight information
    Use custom attributes to manage vertex ID mapping

    Parameters:
        mesh_obj: Mesh object
        weights_data: Weight information saved by save_vertex_weights()
    """
    vertex_weights = weights_data['vertex_weights']
    original_groups = weights_data['existing_groups']
    saved_vertex_ids = weights_data.get('vertex_ids', {})

    # Delete groups that did not originally exist from among the currently existing groups
    current_groups = set(group.name for group in mesh_obj.vertex_groups)
    groups_to_remove = current_groups - original_groups

    for group_name in groups_to_remove:
        if group_name in mesh_obj.vertex_groups:
            mesh_obj.vertex_groups.remove(mesh_obj.vertex_groups[group_name])
            print(f"Removed vertex group {group_name} from {mesh_obj.name}")

    # If an existing group has been deleted, recreate it
    for group_name in original_groups:
        if group_name not in mesh_obj.vertex_groups:
            mesh_obj.vertex_groups.new(name=group_name)

    # First, remove all vertices from all vertex groups
    for group in mesh_obj.vertex_groups:
        group.remove(list(range(len(mesh_obj.data.vertices))))

    # Retrieve vertex ID mappings from custom attributes
    mesh = mesh_obj.data
    custom_attr_name = "original_vertex_id"

    if custom_attr_name not in mesh.attributes:
        print(f"Warning: Custom attribute '{custom_attr_name}' not found in {mesh_obj.name}. Using direct index mapping.")
        # If no custom attributes exist, use the index directly in the conventional manner
        for vert_index, vertex_weights_dict in vertex_weights.items():
            if vert_index < len(mesh.vertices):
                for group_name, weight in vertex_weights_dict.items():
                    if group_name in mesh_obj.vertex_groups:
                        group = mesh_obj.vertex_groups[group_name]
                        group.add([vert_index], weight, 'REPLACE')
        return

    # Retrieve custom attributes
    custom_attr = mesh.attributes[custom_attr_name]

    # Retrieve the original vertex ID of the current vertex to create a mapping
    current_to_original_mapping = {}
    for current_vert in mesh.vertices:
        original_id = custom_attr.data[current_vert.index].value
        current_to_original_mapping[current_vert.index] = original_id

    print(f"Restoring vertex weights using custom attribute mapping for {len(mesh.vertices)} vertices in {mesh_obj.name}")

    # Restore saved weights (use custom attributes to handle)
    restored_count = 0
    for current_vert_index, original_vert_id in current_to_original_mapping.items():
        if original_vert_id in vertex_weights:
            vertex_weights_dict = vertex_weights[original_vert_id]
            for group_name, weight in vertex_weights_dict.items():
                if group_name in mesh_obj.vertex_groups:
                    group = mesh_obj.vertex_groups[group_name]
                    group.add([current_vert_index], weight, 'REPLACE')
            restored_count += 1

    print(f"Successfully restored weights for {restored_count} vertices in {mesh_obj.name}")


def get_bone_name_from_humanoid(avatar_data: dict, humanoid_bone_name: str) -> str:
    """
    Retrieve the actual bone name from humanoidBoneName

    Parameters:
        avatar_data: Avatar data
        humanoid_bone_name: Humanoid Bone Name

    Returns:
        Actual bone name, None if not found
    """
    for bone_map in avatar_data.get("humanoidBones", []):
        if bone_map["humanoidBoneName"] == humanoid_bone_name:
            return bone_map["boneName"]
    return None


def merge_vertex_group_weights(mesh_obj: bpy.types.Object, source_group_name: str, target_group_name: str) -> None:
    """
    Merge the weights of the specified vertex group into another group

    Parameters:
        mesh_obj: Mesh object
        source_group_name: Name of the source group
        target_group_name: Target group name
    """
    if source_group_name not in mesh_obj.vertex_groups or target_group_name not in mesh_obj.vertex_groups:
        return

    source_group = mesh_obj.vertex_groups[source_group_name]
    target_group = mesh_obj.vertex_groups[target_group_name]

    # Combine the weights of each vertex
    for vert in mesh_obj.data.vertices:
        source_weight = 0
        for group in vert.groups:
            if group.group == source_group.index:
                source_weight = group.weight
                break

        if source_weight > 0:
            # Add weight to the target group
            target_group.add([vert.index], source_weight, 'ADD')


def process_bone_weight_consolidation(mesh_obj: bpy.types.Object, avatar_data: dict) -> None:
    """
    Integrate bone weights according to the specified rules

    Parameters:
        mesh_obj: Mesh object
        avatar_data: Avatar data
    """
    # UpperChest -> Chest integration
    upper_chest_bone = get_bone_name_from_humanoid(avatar_data, "UpperChest")
    chest_bone = get_bone_name_from_humanoid(avatar_data, "Chest")

    if upper_chest_bone and chest_bone and upper_chest_bone in mesh_obj.vertex_groups:
        # If the Chest group does not exist, create it
        if chest_bone not in mesh_obj.vertex_groups:
            mesh_obj.vertex_groups.new(name=chest_bone)
        merge_vertex_group_weights(mesh_obj, upper_chest_bone, chest_bone)
        print(f"Merged {upper_chest_bone} weights to {chest_bone} in {mesh_obj.name}")

    # Chest Bone -> Chest Integration
    breasts_humanoid_bones = [
        "LeftBreasts",
        "RightBreasts"
    ]

    if chest_bone:
        # If the Chest group does not exist, create it
        if chest_bone not in mesh_obj.vertex_groups:
            mesh_obj.vertex_groups.new(name=chest_bone)

        for breasts_humanoid in breasts_humanoid_bones:
            breasts_bone = get_bone_name_from_humanoid(avatar_data, breasts_humanoid)
            if breasts_bone and breasts_bone in mesh_obj.vertex_groups:
                merge_vertex_group_weights(mesh_obj, breasts_bone, chest_bone)
                print(f"Merged {breasts_bone} weights to {chest_bone} in {mesh_obj.name}")

    # Integration of Left Foot Toe System Bones -> LeftFoot
    left_foot_bone = get_bone_name_from_humanoid(avatar_data, "LeftFoot")
    left_toe_humanoid_bones = [
        "LeftToes",
        "LeftFootThumbProximal",
        "LeftFootThumbIntermediate",
        "LeftFootThumbDistal",
        "LeftFootIndexProximal",
        "LeftFootIndexIntermediate",
        "LeftFootIndexDistal",
        "LeftFootMiddleProximal",
        "LeftFootMiddleIntermediate",
        "LeftFootMiddleDistal",
        "LeftFootRingProximal",
        "LeftFootRingIntermediate",
        "LeftFootRingDistal",
        "LeftFootLittleProximal",
        "LeftFootLittleIntermediate",
        "LeftFootLittleDistal"
    ]

    if left_foot_bone:
        # If the LeftFoot group does not exist, create it
        if left_foot_bone not in mesh_obj.vertex_groups:
            mesh_obj.vertex_groups.new(name=left_foot_bone)

        for toe_humanoid in left_toe_humanoid_bones:
            toe_bone = get_bone_name_from_humanoid(avatar_data, toe_humanoid)
            if toe_bone and toe_bone in mesh_obj.vertex_groups:
                merge_vertex_group_weights(mesh_obj, toe_bone, left_foot_bone)
                print(f"Merged {toe_bone} weights to {left_foot_bone} in {mesh_obj.name}")

    # Integration of Right Foot Toe System Bones -> RightFoot
    right_foot_bone = get_bone_name_from_humanoid(avatar_data, "RightFoot")
    right_toe_humanoid_bones = [
        "RightToes",
        "RightFootThumbProximal",
        "RightFootThumbIntermediate",
        "RightFootThumbDistal",
        "RightFootIndexProximal",
        "RightFootIndexIntermediate",
        "RightFootIndexDistal",
        "RightFootMiddleProximal",
        "RightFootMiddleIntermediate",
        "RightFootMiddleDistal",
        "RightFootRingProximal",
        "RightFootRingIntermediate",
        "RightFootRingDistal",
        "RightFootLittleProximal",
        "RightFootLittleIntermediate",
        "RightFootLittleDistal"
    ]

    if right_foot_bone:
        # If the RightFoot group does not exist, create it
        if right_foot_bone not in mesh_obj.vertex_groups:
            mesh_obj.vertex_groups.new(name=right_foot_bone)

        for toe_humanoid in right_toe_humanoid_bones:
            toe_bone = get_bone_name_from_humanoid(avatar_data, toe_humanoid)
            if toe_bone and toe_bone in mesh_obj.vertex_groups:
                merge_vertex_group_weights(mesh_obj, toe_bone, right_foot_bone)
                print(f"Merged {toe_bone} weights to {right_foot_bone} in {mesh_obj.name}")


def get_deformation_bone_groups(avatar_data: dict) -> list:
    """
    Get list of bone groups for deformation mask from avatar data,
    excluding Head and its auxiliary bones.

    Parameters:
        avatar_data: Avatar data containing bone information

    Returns:
        List of bone names for deformation mask
    """
    bone_groups = set()

    # Get mapping of humanoid bones
    for bone_map in avatar_data.get("humanoidBones", []):
        if "humanoidBoneName" in bone_map and "boneName" in bone_map:
            # Skip Head bone
            if bone_map["humanoidBoneName"] != "Head":
                bone_groups.add(bone_map["boneName"])

    # Get auxiliary bones mapping
    for aux_set in avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        # Skip Head's auxiliary bones
        if humanoid_bone != "Head":
            aux_bones = aux_set["auxiliaryBones"]
            bone_groups.update(aux_bones)

    return sorted(list(bone_groups))

def create_deformation_mask(obj: bpy.types.Object, avatar_data: dict) -> None:
    """
    Create deformation mask vertex group based on avatar data.

    Parameters:
        obj: Mesh object to process
        avatar_data: Avatar data containing bone information
    """
    # Input validation
    if obj.type != 'MESH':
        print(f"Error: {obj.name} is not a mesh object")
        return

    # Get bone groups from avatar data
    group_names = get_deformation_bone_groups(avatar_data)

    # If a vertex group named TransferMask already exists, delete it
    if "DeformationMask" in obj.vertex_groups:
        obj.vertex_groups.remove(obj.vertex_groups["DeformationMask"])

    # Create a new vertex group
    deformation_mask = obj.vertex_groups.new(name="DeformationMask")

    # Check each vertex
    for vert in obj.data.vertices:
        should_add = False
        weight_sum = 0.0
        # Check the weights of the specified vertex group
        for group_name in group_names:
            try:
                group = obj.vertex_groups[group_name]
                # Retrieve the weight value of that vertex
                weight = 0
                for g in vert.groups:
                    if g.group == group.index:
                        weight = g.weight
                # If the weight is greater than zero, set the flag
                if weight > 0:
                    should_add = True
                    weight_sum += weight
            except KeyError:
                # If the vertex group does not exist, skip
                continue

        # If the flag is set, add vertices to the DeformationMask group
        if should_add:
            deformation_mask.add([vert.index], weight_sum, 'REPLACE')



def create_field_distance_vertex_group(obj, field_data_path, group_name="FieldDistanceWeights", batch_size=1000, k=8):
    """
    Create vertex groups using the Weight values of nearby vertices in the Deformation Field
    The weight value of each vertex is 1.0 - (weighted average of the Weight values of the vertices in the Field's neighborhood)
    Perform batch processing to speed up operations

    Parameters:
        obj: Mesh object
        field_data_path: Path to Deformation Field data
        group_name: The name of the vertex group to create
        batch_size: Batch size
        k: k-nearest neighbor algorithm's k value

    Returns:
        Created vertex group
    """
    # Acquire Deformation Field data
    field_info = get_deformation_field(field_data_path)
    field_points = field_info['field_points']
    field_weights = field_info['field_weights']
    field_matrix = field_info['world_matrix']
    field_matrix_inv = field_info['world_matrix_inv']
    kdtree = field_info['kdtree']

    # If a vertex group already exists, delete it
    if group_name in obj.vertex_groups:
        obj.vertex_groups.remove(obj.vertex_groups[group_name])

    # Create a new vertex group
    vertex_group = obj.vertex_groups.new(name=group_name)

    # Retrieve the vertex positions of the evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data
    vertices = np.array([v.co for v in eval_mesh.vertices])
    num_vertices = len(vertices)

    # Array to store the results
    vertex_weights = np.zeros(num_vertices)

    # Batch processing
    for start_idx in range(0, num_vertices, batch_size):
        end_idx = min(start_idx + batch_size, num_vertices)
        batch_vertices = vertices[start_idx:end_idx]

        # Transform vertices within the batch into field space
        batch_world = np.array([eval_obj.matrix_world @ Vector(v) for v in batch_vertices])
        batch_field = np.array([field_matrix_inv @ Vector(v) for v in batch_world])

        # k-Nearest Neighbor Search (Batch Processing)
        distances, indices = kdtree.query(batch_field, k=k)

        # Calculate the field weight for each vertex
        for i, (dist, idx) in enumerate(zip(distances, indices)):
            # Distance-based weighting
            weights = 1.0 / (dist + 0.0001)  # Prevent division by zero
            weights = weights / weights.sum()  # Normalization

            # Field-weighted average
            field_weight = np.sum(field_weights[idx] * weights)

            # Save Results (1.0 - Field Weight)
            vertex_weights[start_idx + i] = 1.0 - field_weight

    # Set weights for vertex groups
    for i, weight in enumerate(vertex_weights):
        if weight > 0:
            vertex_group.add([i], weight, 'REPLACE')

    print(f"Created field distance vertex group '{group_name}' for {obj.name} using k={k} neighbors and batch processing")
    return vertex_group


def create_overlapping_vertices_attributes(clothing_meshes, base_avatar_data, distance_threshold=0.0001, edge_angle_threshold=3, weight_similarity_threshold=0.1, overlap_attr_name="Overlapped", world_pos_attr_name="OriginalWorldPosition"):
    """
    Detect vertices that are nearly overlapping in world coordinates and have similar weight patterns,
    Set a flag (1.0) as a custom vertex attribute. Also save the world vertex coordinates as a separate attribute.

    Parameters:
        clothing_meshes: List of clothing meshes to be processed
        base_avatar_data: Base Avatar Data
        distance_threshold: Threshold distance for determining overlap
        weight_similarity_threshold: Weight pattern similarity threshold (lower values indicate stricter criteria)
        overlap_attr_name: Name of the custom attribute for the overlap vertex flag
        world_pos_attr_name: The name of the custom attribute that stores the world coordinates
    """
    print("Creating custom attributes for overlapping vertices with similar weight patterns...")

    # Get the vertex group to be checked
    target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

    # Processing for each mesh
    for mesh_obj in clothing_meshes:
        depsgraph = bpy.context.evaluated_depsgraph_get()
        eval_obj = mesh_obj.evaluated_get(depsgraph)
        mesh = eval_obj.data
        bm = bmesh.new()
        bm.from_mesh(mesh)
        bm.verts.ensure_lookup_table()
        bm.edges.ensure_lookup_table()
        bm.faces.ensure_lookup_table()

        # Create vertex index mapping (BMesh indices → original mesh indices)
        vert_indices = {v.index: i for i, v in enumerate(bm.verts)}

        # Collect vertex data
        all_vertices = []
        for vert_idx, vert in enumerate(bm.verts):
            # Calculate the world coordinates of the vertex
            world_pos = mesh_obj.matrix_world @ vert.co

            # Collect the direction vectors of edges connected to the vertex
            edge_directions = []
            bm_vert = bm.verts[vert_idx]
            for edge in bm_vert.link_edges:
                other_vert = edge.other_vert(bm_vert)
                direction = (other_vert.co - bm_vert.co).normalized()
                edge_directions.append(direction)

            # Collect the weights for the target group
            weights = {}
            orig_vert = mesh_obj.data.vertices[vert_indices[vert_idx]]
            for group_name in target_groups:
                if group_name in mesh_obj.vertex_groups:
                    group = mesh_obj.vertex_groups[group_name]
                    for g in orig_vert.groups:
                        if g.group == group.index:
                            weights[group_name] = g.weight
                            break

            # Save vertex data
            all_vertices.append({
                'vert_idx': vert_idx,
                'world_pos': world_pos,
                'edge_directions': edge_directions,
                'weights': weights
            })

        # Build a KDTree to efficiently search for nearby vertices
        positions = [v['world_pos'] for v in all_vertices]
        kdtree = KDTree(len(positions))
        for i, pos in enumerate(positions):
            kdtree.insert(pos, i)
        kdtree.balance()

        # Create or retrieve custom attributes for overlapping vertices
        if overlap_attr_name not in mesh_obj.data.attributes:
            mesh_obj.data.attributes.new(name=overlap_attr_name, type='FLOAT', domain='POINT')
        overlap_attr = mesh_obj.data.attributes[overlap_attr_name]

        # Create or retrieve custom attributes for world coordinates
        if world_pos_attr_name not in mesh_obj.data.attributes:
            mesh_obj.data.attributes.new(name=world_pos_attr_name, type='FLOAT_VECTOR', domain='POINT')
        pos_attr = mesh_obj.data.attributes[world_pos_attr_name]

        # Set initial values (overlap attribute set to 0, world coordinates set to current position)
        for i, vertex in enumerate(mesh_obj.data.vertices):
            overlap_attr.data[i].value = 0.0
            world_position = mesh_obj.matrix_world @ vertex.co
            pos_attr.data[i].vector = world_position

        # Detect overlapping vertices and set a flag
        processed = set()  # Record processed vertex indices
        cluster_id = 0  # Cluster ID (for debugging)

        for i, vert_data in enumerate(all_vertices):
            mesh_vertex_idx = vert_indices[all_vertices[i]['vert_idx']]
            world_pos = all_vertices[i]['world_pos']
            pos_attr.data[mesh_vertex_idx].vector = world_pos  # Save world coordinates

            if i in processed:
                continue

            # Search for Nearby Vertices
            overlapping_indices = []
            for (co, idx, dist) in kdtree.find_range(vert_data['world_pos'], distance_threshold):
                if idx != i and idx not in processed:  # Exclude the vertex itself and processed vertices
                    # Check similarity in the edge direction
                    if check_edge_direction_similarity(vert_data['edge_directions'], all_vertices[idx]['edge_directions'], edge_angle_threshold):
                        # Check for similarity in weight patterns
                        similarity = calculate_weight_pattern_similarity(
                            vert_data['weights'], all_vertices[idx]['weights'])

                        # Add only if similarity exceeds the threshold
                        if similarity >= (1.0 - weight_similarity_threshold):
                            overlapping_indices.append(idx)

            if not overlapping_indices:
                continue

            # Include overlapping vertex groups
            overlapping_indices.append(i)
            processed.add(i)


            # Set the attributes of overlapping vertices
            for vert_idx in overlapping_indices:
                mesh_vertex_idx = vert_indices[all_vertices[vert_idx]['vert_idx']]
                overlap_attr.data[mesh_vertex_idx].value = 1.0  # Set the overlap detection flag
                processed.add(vert_idx)

            cluster_id += 1

        # Release BMesh
        bm.free()

        mesh_obj.data.update()

        print(f"Created custom attributes '{overlap_attr_name}' and '{world_pos_attr_name}' for {mesh_obj.name} with {cluster_id} overlapping vertex clusters")
        print(f"Distance threshold: {distance_threshold}")
        print(f"Weight similarity threshold: {weight_similarity_threshold}")

def create_overlapping_vertices_uvmap(clothing_meshes, base_avatar_data, distance_threshold=0.0001, edge_angle_threshold=3, weight_similarity_threshold=0.1, uv_name="OverlappingVertices", circle_center=(0.0, 0.0), circle_radius=10.0):
    """
    Detect vertices that are nearly overlapping in world coordinates and have similar weight patterns, then stack and save them along the circumference on the new UV map

    Parameters:
        clothing_meshes: List of clothing meshes to be processed
        base_avatar_data: Base Avatar Data
        distance_threshold: Threshold distance for determining overlap
        weight_similarity_threshold: Weight pattern similarity threshold (lower values indicate stricter criteria)
        uv_name: Name of the UV map to create
        circle_center: Center coordinates (x, y) of the circle in UV coordinates
        circle_radius: Radius of the circle in UV coordinates
    """
    print(f"Creating UV map for overlapping vertices with similar weight patterns, name: {uv_name}...")

    # Get the vertex group to be checked
    target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

    # Processing for each mesh
    for mesh_obj in clothing_meshes:
        depsgraph = bpy.context.evaluated_depsgraph_get()
        eval_obj = mesh_obj.evaluated_get(depsgraph)
        mesh = eval_obj.data
        bm = bmesh.new()
        bm.from_mesh(mesh)
        bm.verts.ensure_lookup_table()
        bm.edges.ensure_lookup_table()
        bm.faces.ensure_lookup_table()

        # Create vertex index mapping (BMesh indices → original mesh indices)
        vert_indices = {v.index: i for i, v in enumerate(bm.verts)}

        # Collect vertex data
        all_vertices = []
        for vert_idx, vert in enumerate(bm.verts):
            # Calculate the world coordinates of the vertex
            world_pos = mesh_obj.matrix_world @ vert.co

            # Collect the direction vectors of edges connected to the vertex
            edge_directions = []
            bm_vert = bm.verts[vert_idx]
            for edge in bm_vert.link_edges:
                other_vert = edge.other_vert(bm_vert)
                direction = (other_vert.co - bm_vert.co).normalized()
                edge_directions.append(direction)

            # Collect the weights for the target group
            weights = {}
            orig_vert = mesh_obj.data.vertices[vert_indices[vert_idx]]
            for group_name in target_groups:
                if group_name in mesh_obj.vertex_groups:
                    group = mesh_obj.vertex_groups[group_name]
                    for g in orig_vert.groups:
                        if g.group == group.index:
                            weights[group_name] = g.weight
                            break

            # Save vertex data
            all_vertices.append({
                'vert_idx': vert_idx,
                'world_pos': world_pos,
                'edge_directions': edge_directions,
                'weights': weights
            })

        # Build a KDTree to efficiently search for nearby vertices
        positions = [v['world_pos'] for v in all_vertices]
        kdtree = KDTree(len(positions))
        for i, pos in enumerate(positions):
            kdtree.insert(pos, i)
        kdtree.balance()

        # Create or obtain a new UV map
        if uv_name not in mesh_obj.data.uv_layers:
            mesh_obj.data.uv_layers.new(name=uv_name)
        uv_layer = mesh_obj.data.uv_layers[uv_name]

        # Reset all UV coordinates to the origin
        for uv_data in uv_layer.data:
            uv_data.uv = (0.0, 0.0)

        # Detect overlapping vertices and set UV coordinates
        processed = set()  # Record processed vertex indices
        cluster_id = 0  # Cluster ID (used for UV coordinate placement)

        for i, vert_data in enumerate(all_vertices):
            if i in processed:
                continue

            # Search for Nearby Vertices
            overlapping_indices = []
            for (co, idx, dist) in kdtree.find_range(vert_data['world_pos'], distance_threshold):
                if idx != i and idx not in processed:  # Exclude the vertex itself and processed vertices
                    # Check similarity in the edge direction
                    if check_edge_direction_similarity(vert_data['edge_directions'], all_vertices[idx]['edge_directions'], edge_angle_threshold):
                        # Check for similarity in weight patterns
                        similarity = calculate_weight_pattern_similarity(
                            vert_data['weights'], all_vertices[idx]['weights'])

                        # Add only if similarity exceeds the threshold
                        if similarity >= (1.0 - weight_similarity_threshold):
                            overlapping_indices.append(idx)

            if not overlapping_indices:
                continue

            # Include overlapping vertex groups
            overlapping_indices.append(i)
            processed.add(i)

            # Calculate the position on the circumference
            angle = 2 * math.pi * cluster_id / max(1, len(all_vertices) // 10)  # Distribute angles according to the number of clusters
            uv_x = circle_center[0] + circle_radius * math.cos(angle)
            uv_y = circle_center[1] + circle_radius * math.sin(angle)

            # Set the UV coordinates for overlapping vertices
            for vert_idx in overlapping_indices:
                for loop in mesh_obj.data.loops:
                    if loop.vertex_index == vert_indices[all_vertices[vert_idx]['vert_idx']]:
                        uv_layer.data[loop.index].uv = (uv_x, uv_y)
                processed.add(vert_idx)

            cluster_id += 1

        # Release BMesh
        bm.free()

        mesh_obj.data.update()

        print(f"Created UV map '{uv_name}' for {mesh_obj.name} with {cluster_id} overlapping vertex clusters")
        print(f"UV circle: center {circle_center}, radius {circle_radius}")
        print(f"Weight similarity threshold: {weight_similarity_threshold}")

# Function to calculate weight pattern similarity
def calculate_weight_pattern_similarity(weights1, weights2):
    """
    Calculate the similarity between two weight patterns

    Parameters:
        weights1: First weight pattern {group_name: weight}
        weights2: Second weight pattern {group_name: weight}

    Returns:
        float: Similarity (0.0 to 1.0, 1.0 being an exact match)
    """
    # Retrieve the group present in both patterns
    all_groups = set(weights1.keys()) | set(weights2.keys())

    if not all_groups:
        return 0.0

    # Calculate the sum of the weight differences for each group
    total_diff = 0.0
    for group in all_groups:
        w1 = weights1.get(group, 0.0)
        w2 = weights2.get(group, 0.0)
        total_diff += abs(w1 - w2)

    # Normalization (dividing by the number of groups)
    normalized_diff = total_diff / len(all_groups)

    # Convert to similarity (the smaller the difference, the higher the similarity)
    similarity = 1.0 - min(normalized_diff, 1.0)

    return similarity



def get_vertex_groups_and_weights(mesh_obj, vertex_index):
    """Get the vertex group and weight to which the vertex belongs"""
    groups = {}
    vertex = mesh_obj.data.vertices[vertex_index]

    for g in vertex.groups:
        group_name = mesh_obj.vertex_groups[g.group].name
        groups[group_name] = g.weight

    return groups

def get_armature_from_modifier(mesh_obj):
    """Get Armature from Armature Modifier"""
    for modifier in mesh_obj.modifiers:
        if modifier.type == 'ARMATURE':
            return modifier.object
    return None

def calculate_inverse_pose_matrix(mesh_obj, armature_obj, vertex_index):
    """Calculate the inverse matrix of the pose for the specified vertex"""

    # Retrieving vertex groups and weights
    weights = get_vertex_groups_and_weights(mesh_obj, vertex_index)
    if not weights:
        print(f"No weight has been assigned to vertex {vertex_index}")
        return None

    # Initialization of the final transformation matrix
    final_matrix = Matrix.Identity(4)
    final_matrix.zero()
    total_weight = 0

    # Calculate the influence of each bone
    for bone_name, weight in weights.items():
        if weight > 0 and bone_name in armature_obj.data.bones:
            bone = armature_obj.data.bones[bone_name]
            pose_bone = armature_obj.pose.bones.get(bone_name)
            if bone and pose_bone:
                # Calculate the final matrix for the bone
                mat = armature_obj.matrix_world @ \
                      pose_bone.matrix @ \
                      bone.matrix_local.inverted() @ \
                      armature_obj.matrix_world.inverted()

                # Add matrices with weight consideration
                final_matrix += mat * weight
                total_weight += weight

    # Normalized by the sum of weights
    if total_weight > 0:
        final_matrix = final_matrix * (1.0 / total_weight)

    # Calculate and return the inverse matrix
    try:
        return final_matrix.inverted()
    except Exception as e:
        print(f"error: {e}")
        return Matrix.Identity(4)

def inverse_bone_deform_all_vertices(armature_obj, mesh_obj):
    """
    From the world coordinates of the vertices after evaluating the mesh object,
    Perform inverse transformation of the current Armature object's pose for all vertices

    Parameters:
        armature_obj: Armature object
        mesh_obj: Mesh object

    Returns:
        np.ndarray: Coordinates of all vertices after inverse transformation (local coordinates)


        Standard bone deformation: Deformed = Σ(weight_i × bone_matrix_i) × Original
        Inverse transformation of this function: Original shape = [Σ(weight_i × bone_matrix_i)]^(-1) × Deformed shape
    """

    if not armature_obj or armature_obj.type != 'ARMATURE':
        raise ValueError("Please specify a valid Armature object")

    if not mesh_obj or mesh_obj.type != 'MESH':
        raise ValueError("Please specify a valid mesh object")

    # Convert to world coordinates (vertex positions after transformation)
    vertices = [v.co.copy() for v in mesh_obj.data.vertices]

    # List to store the results
    inverse_transformed_vertices = []

    print(f"Starting inverse bone transformation: {len(vertices)} vertices")

    # Apply inverse transformation to each vertex
    for vertex_index in range(len(vertices)):
        pos = vertices[vertex_index]

        # Get the bone weight of the vertex
        weights = get_vertex_groups_and_weights(mesh_obj, vertex_index)

        if not weights:
            # If there is no weight, add it as is
            print(f"Warning: No weight for vertex {vertex_index}, using identity matrix")
            inverse_transformed_vertices.append(pos)
            continue

        # Calculate weighted composite transformation matrix
        combined_matrix = Matrix.Identity(4)
        combined_matrix.zero()
        total_weight = 0.0

        for bone_name, weight in weights.items():
            if weight > 0 and bone_name in armature_obj.data.bones:
                bone = armature_obj.data.bones[bone_name]
                pose_bone = armature_obj.pose.bones.get(bone_name)
                if bone and pose_bone:
                    # Calculate the bone transformation matrix
                    # This matrix represents the transformation from the rest pose to the pose after
                    bone_matrix = pose_bone.matrix @ \
                                  bone.matrix_local.inverted()

                    # Add matrices with weight consideration
                    combined_matrix += bone_matrix * weight
                    total_weight += weight

        # Normalized by the sum of weights
        if total_weight > 0:
            combined_matrix = combined_matrix * (1.0 / total_weight)
        else:
            # If there are no weights, use the identity matrix
            print(f"Warning: No weight for vertex {vertex_index}, using identity matrix")
            combined_matrix = Matrix.Identity(4)

        # Calculate the inverse of a composite matrix
        try:
            inverse_matrix = combined_matrix.inverted()
        except:
            # If the inverse matrix cannot be computed, use the identity matrix
            inverse_matrix = Matrix.Identity(4)
            print(f"Warning: Could not compute the inverse matrix for vertex {vertex_index}")

        # Apply inverse transformation
        # Apply inverse_matrix to obtain the "local coordinates of the rest pose"
        rest_pose_pos = inverse_matrix @ pos

        inverse_transformed_vertices.append(rest_pose_pos)

        # Progress Display (Every 1000 Vertices)
        if (vertex_index + 1) % 1000 == 0:
            print(f"Progress: {vertex_index + 1}/{len(vertices)} vertices processed")

    print("The inverse transformation of the bone deformation has been completed")

    # Apply the transformed vertices to the mesh
    if mesh_obj.data.shape_keys:
        for shape_key in mesh_obj.data.shape_keys.key_blocks:
            if shape_key.name != "Basis":
                for i, vert in enumerate(shape_key.data):
                    vert.co += inverse_transformed_vertices[i] - vertices[i]
        basis_shape_key = mesh_obj.data.shape_keys.key_blocks["Basis"]
        for i, vert in enumerate(basis_shape_key.data):
            vert.co = inverse_transformed_vertices[i]

    for vertex_index, pos in enumerate(inverse_transformed_vertices):
        mesh_obj.data.vertices[vertex_index].co = pos

    # Convert to a numpy array and return it (from Vector type to numpy array)
    result = np.array([[v[0], v[1], v[2]] for v in inverse_transformed_vertices])

    return result

def batch_process_vertices_multi_step(vertices, all_field_points, all_delta_positions, field_weights,
                                     field_matrix, field_matrix_inv, target_matrix, target_matrix_inv,
                                     deform_weights=None, rbf_epsilon=0.00001, batch_size=1000, k=8):
    """
    Process vertices using a multi-stage Deformation Field (similar to apply_field_data in SaveAndApplyFieldAuto.py)

    Parameters:
        vertices: Array of vertices to be processed
        all_field_points: Array of field points for each step
        all_delta_positions: Array of delta positions for each step
        field_weights: Field weights
        field_matrix: Field Matrix
        field_matrix_inv: Inverse of the field matrix
        target_matrix: Target Matrix
        target_matrix_inv: Inverse of the target matrix
        rbf_epsilon: Epsilon value for RBF interpolation
        batch_size: Batch size
        k: Nearest neighbor count

    Returns:
        Vertex array after transformation (world coordinates)
    """
    num_vertices = len(vertices)
    num_steps = len(all_field_points)

    # Initialize cumulative displacement
    cumulative_displacements = np.zeros((num_vertices, 3))
    # Save the current vertex position (world coordinates)
    current_world_positions = np.array([target_matrix @ Vector(v) for v in vertices])

    # If deform_weights is None, set the weight of all vertices to 1.0
    if deform_weights is None:
        deform_weights = np.ones(num_vertices)

    # Apply the displacement cumulatively at each step
    for step in range(num_steps):
        field_points = all_field_points[step]
        delta_positions = all_delta_positions[step]

        print(f"Applying transformation for step {step+1}/{num_steps}...")
        print(f"Number of field vertices used: {len(field_points)}")

        # Search for nearest points using KDTree (construct a new KDTree at each step)
        kdtree = cKDTree(field_points, balanced_tree=False, compact_nodes=False)

        # Calculate new vertex positions using custom RBF interpolation
        step_displacements = np.zeros((num_vertices, 3))

        for start_idx in range(0, num_vertices, batch_size):
            end_idx = min(start_idx + batch_size, num_vertices)
            batch_weights = deform_weights[start_idx:end_idx]

            # Transform all vertices within the batch into field space (considering current cumulative displacement)
            batch_world = current_world_positions[start_idx:end_idx]
            batch_field = np.array([field_matrix_inv @ Vector(v) for v in batch_world])

            # Search for Nearest Points (Up to k Points)
            k_use = min(k, len(field_points))
            distances, indices = kdtree.query(batch_field, k=k_use)

            # Calculate inverse distance weights
            weights = 1.0 / np.sqrt(distances**2 + rbf_epsilon**2)

            # Weight normalization
            weights /= np.sum(weights, axis=1, keepdims=True)

            # Calculate displacement using a weighted average
            weighted_deltas = delta_positions[indices] * weights[..., np.newaxis]
            batch_displacements = np.sum(weighted_deltas, axis=1) * batch_weights[:, np.newaxis]

            # Calculate displacement in world space
            world_displacements = np.array([field_matrix.to_3x3() @ Vector(v) for v in batch_displacements])
            step_displacements[start_idx:end_idx] = world_displacements

            # Update the current world position (for the next step)
            current_world_positions[start_idx:end_idx] += world_displacements

        # Add this step displacement to the cumulative displacement
        cumulative_displacements += step_displacements

        #print(f"Step {step+1} complete: Maximum displacement {np.max(np.linalg.norm(step_displacements, axis=1)):.6f}")

    # Return the final position after transformation
    final_world_positions = np.array([target_matrix @ Vector(v) for v in vertices]) + cumulative_displacements
    return final_world_positions

def batch_process_vertices_with_custom_range(vertices, all_field_points, all_delta_positions, field_weights,
                                            field_matrix, field_matrix_inv, target_matrix, target_matrix_inv,
                                            start_value, end_value,
                                            deform_weights=None, rbf_epsilon=0.00001, batch_size=1000, k=8):
    """
    Perform field-based transformations within any specified value range

    Parameters:
        vertices: Array of vertices to be processed
        all_field_points: Array of field points for each step
        all_delta_positions: Array of delta positions for each step
        field_weights: Field weights
        field_matrix: Field Matrix
        field_matrix_inv: Inverse of the field matrix
        target_matrix: Target Matrix
        target_matrix_inv: Inverse of the target matrix
        start_value: Start value (ShapeKey value)
        end_value: End value (ShapeKey value)
        deform_weights: Deformation Weights
        rbf_epsilon: Epsilon value for RBF interpolation
        batch_size: Batch size
        k: Nearest neighbor count

    Returns:
        Vertex array after transformation (world coordinates)
    """
    num_vertices = len(vertices)
    num_steps = len(all_field_points)

    # Initialize cumulative displacement
    cumulative_displacements = np.zeros((num_vertices, 3))
    # Save the current vertex position (world coordinates)
    current_world_positions = np.array([target_matrix @ Vector(v) for v in vertices])

    # If deform_weights is None, set the weight of all vertices to 1.0
    if deform_weights is None:
        deform_weights = np.ones(num_vertices)

    # Calculate the value for each step
    step_size = 1.0 / num_steps

    # Processing at each step
    processed_steps = []
    for step in range(num_steps):
        step_start = step * step_size
        step_end = (step + 1) * step_size
        # Increasing from start_value to end_value (start_value < end_value)
        if step_start + 0.00001 <= end_value and step_end - 0.00001 >= start_value:
            processed_steps.append((step, step_start, step_end))

    print(f"Processed steps: {len(processed_steps)}")

    # Apply the displacement cumulatively at each step
    for step_idx, (step, step_start, step_end) in enumerate(processed_steps):
        field_points = all_field_points[step].copy()
        delta_positions = all_delta_positions[step].copy()
        original_delta_positions = all_delta_positions[step]

        print(f"Applying transformation for step {step_idx+1}/{len(processed_steps)} (step {step})...")
        print(f"Step value range: {step_start:.3f} -> {step_end:.3f}")
        print(f"Number of field vertices used: {len(field_points)}")

        # Transformation from any value
        if start_value != step_start:
            if start_value >= step_start + 0.00001:
                # If the start value is greater than the step start value
                adjustment_factor = (start_value - step_start) / step_size
                adjustment_delta = original_delta_positions * adjustment_factor
                field_points += adjustment_delta
                delta_positions -= adjustment_delta
        if end_value != step_end:
            if end_value <= step_end - 0.00001:
                # If the ending value is smaller than the step's ending value
                adjustment_factor = (step_end - end_value) / step_size
                adjustment_delta = original_delta_positions * adjustment_factor
                delta_positions -= adjustment_delta

        # Searching for Nearest Points Using KDTree
        kdtree = cKDTree(field_points, balanced_tree=False, compact_nodes=False)

        # Calculate new vertex positions using custom RBF interpolation
        step_displacements = np.zeros((num_vertices, 3))

        for start_idx in range(0, num_vertices, batch_size):
            end_idx = min(start_idx + batch_size, num_vertices)
            batch_weights = deform_weights[start_idx:end_idx]

            # Convert all vertices in the batch to field space
            batch_world = current_world_positions[start_idx:end_idx]
            batch_field = np.array([field_matrix_inv @ Vector(v) for v in batch_world])

            # Search for Nearest Points (Up to k Points)
            k_use = min(k, len(field_points))
            distances, indices = kdtree.query(batch_field, k=k_use)

            # Calculate inverse distance weights
            weights = 1.0 / np.sqrt(distances**2 + rbf_epsilon**2)

            # Weight normalization
            weights /= np.sum(weights, axis=1, keepdims=True)

            # Calculate displacement using a weighted average
            weighted_deltas = delta_positions[indices] * weights[..., np.newaxis]
            batch_displacements = np.sum(weighted_deltas, axis=1) * batch_weights[:, np.newaxis]

            # Calculate displacement in world space
            world_displacements = np.array([field_matrix.to_3x3() @ Vector(v) for v in batch_displacements])
            step_displacements[start_idx:end_idx] = world_displacements

            # Update the current world position (for the next step)
            current_world_positions[start_idx:end_idx] += world_displacements

        # Add this step displacement to the cumulative displacement
        cumulative_displacements += step_displacements

        print(f"Step {step_idx+1} Complete")

    # Return the final position after transformation
    final_world_positions = np.array([target_matrix @ Vector(v) for v in vertices]) + cumulative_displacements
    return final_world_positions

def batch_process_vertices(vertices, kdtree, field_points, delta_positions, field_weights,
                         field_matrix, field_matrix_inv, target_matrix, target_matrix_inv,
                         deform_weights=None, batch_size=1000, k=8):
    """
    Process vertices in batches
    """
    num_vertices = len(vertices)
    results = np.zeros((num_vertices, 3))

    # If deform_weights is None, set the weight of all vertices to 1.0
    if deform_weights is None:
        deform_weights = np.ones(num_vertices)

    rbf_epsilon = 0.00001

    for start_idx in range(0, num_vertices, batch_size):
        end_idx = min(start_idx + batch_size, num_vertices)
        batch_vertices = vertices[start_idx:end_idx]
        batch_weights = deform_weights[start_idx:end_idx]

        # Convert all vertices in the batch to field space
        batch_world = np.array([target_matrix @ Vector(v) for v in batch_vertices])
        batch_field = np.array([field_matrix_inv @ Vector(v) for v in batch_world])

        # Search for Recent Contacts (Batch Processing)
        # distances, indices = kdtree.query(batch_field, k=int(k))
        distances, indices = kdtree.query(batch_field, k=27)

        # Calculate the displacement of each vertex
        for i, (vert_field, dist, idx) in enumerate(zip(batch_field, distances, indices)):
            # Calculate weighted displacement
            weights = 1.0 / np.sqrt(dist**2 + rbf_epsilon**2)
            if weights.sum() > 0.0:
                weights /= weights.sum()
            else:
                weights *= 0

            deltas = delta_positions[idx]

            displacement = (deltas * weights[:, np.newaxis]).sum(axis=0) * batch_weights[i]

            # Calculate displacement in world space
            world_displacement = field_matrix.to_3x3() @ Vector(displacement)

            results[start_idx + i] = batch_world[i] + world_displacement

    return results

def get_child_bones_recursive(bone_name: str, armature_obj: bpy.types.Object, clothing_avatar_data: dict = None, is_root: bool = True) -> set:
    """
    Recursively retrieve all child bones of the specified bone
    Exclude Humanoid bones that are not the first specified bone and all subsequent child bones

    Parameters:
        bone_name: Parent bone name
        armature_obj: Armature Object
        clothing_avatar_data: Clothing avatar data (used for Humanoid bone detection)
        is_root: Whether it is the first specified bone

    Returns:
        set: Set of child bone names
    """
    children = set()
    if bone_name not in armature_obj.data.bones:
        return children

    # Create a mapping for determining Humanoid bones
    humanoid_bones = set()
    if clothing_avatar_data:
        for bone_map in clothing_avatar_data.get("humanoidBones", []):
            if "boneName" in bone_map:
                humanoid_bones.add(bone_map["boneName"])

    bone = armature_obj.data.bones[bone_name]
    for child in bone.children:
        # For Humanoid bones that are not the first specified bone, exclude that bone and its child bones
        if not is_root and child.name in humanoid_bones:
            # Skip this bone and its child bones
            continue

        children.add(child.name)
        children.update(get_child_bones_recursive(child.name, armature_obj, clothing_avatar_data, False))

    return children

def create_blendshape_mask(target_obj, mask_bones, clothing_avatar_data, field_name="", store_debug_mask=True):
    """
    Create a mask by combining the weights of the specified bone and its child bones

    Parameters:
        target_obj: Target mesh object
        mask_bones: List of Humanoid bones used for the mask
        clothing_avatar_data: Clothing avatar data (used for converting Humanoid bone names)
        field_name: Field name (used for debugging vertex group names)
        store_debug_mask: Whether to store the mask vertex group for debugging

    Returns:
        numpy.ndarray: Array of mask weight values for each vertex
    """
    #print(f"mask_bones: {mask_bones}")

    mask_weights = np.zeros(len(target_obj.data.vertices))

    # Acquire the armature
    armature_obj = None
    for modifier in target_obj.modifiers:
        if modifier.type == 'ARMATURE':
            armature_obj = modifier.object
            break

    if not armature_obj:
        print(f"Warning: No armature found for {target_obj.name}")
        return mask_weights

    # Create a conversion map from Humanoid bone names to bone names
    humanoid_to_bone = {}
    for bone_map in clothing_avatar_data.get("humanoidBones", []):
        if "humanoidBoneName" in bone_map and "boneName" in bone_map:
            humanoid_to_bone[bone_map["humanoidBoneName"]] = bone_map["boneName"]

    # Create auxiliary bone mapping
    auxiliary_bones = {}
    for aux_set in clothing_avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        auxiliary_bones[humanoid_bone] = aux_set["auxiliaryBones"]

    # Collect information on processed bones for debugging purposes
    processed_bones = set()

    # Collect all target bones (Humanoid bones, auxiliary bones, and their child bones)
    target_bones = set()

    # Processing for each Humanoid bone
    for humanoid_bone in mask_bones:
        # Add the main bone
        bone_name = humanoid_to_bone.get(humanoid_bone)
        if bone_name:
            target_bones.add(bone_name)
            processed_bones.add(bone_name)
            # Add child bone
            target_bones.update(get_child_bones_recursive(bone_name, armature_obj, clothing_avatar_data))

        # Add auxiliary bones and their child bones
        if humanoid_bone in auxiliary_bones:
            for aux_bone in auxiliary_bones[humanoid_bone]:
                target_bones.add(aux_bone)
                processed_bones.add(aux_bone)
                # Add child bones to the auxiliary bone
                target_bones.update(get_child_bones_recursive(aux_bone, armature_obj, clothing_avatar_data))

    #print(f"target_bones: {target_bones}")

    # Calculate the weight of each vertex
    for vert in target_obj.data.vertices:
        for bone_name in target_bones:
            if bone_name in target_obj.vertex_groups:
                group = target_obj.vertex_groups[bone_name]
                for g in vert.groups:
                    if g.group == group.index:
                        mask_weights[vert.index] += g.weight
                        break

    # Clamp the weight within the range of 0 to 1
    mask_weights = np.clip(mask_weights, 0.0, 1.0)

    # Create a vertex group for debugging
    if store_debug_mask:
        # Generate vertex group name
        group_name = f"DEBUG_Mask_{field_name}" if field_name else "DEBUG_Mask"

        # If an existing group exists, delete it
        if group_name in target_obj.vertex_groups:
            target_obj.vertex_groups.remove(target_obj.vertex_groups[group_name])

        # Create a new group
        debug_group = target_obj.vertex_groups.new(name=group_name)

        # Set the weight
        for vert_idx, weight in enumerate(mask_weights):
            if weight > 0:
                debug_group.add([vert_idx], weight, 'REPLACE')

        print(f"Created debug mask group '{group_name}' using bones: {sorted(processed_bones)}")

    return mask_weights

# --------------------------------------------------------------------
# BVH-based Intersection Detection
# --------------------------------------------------------------------
def cross2d(u: Vector, v: Vector) -> float:
    """Cross product of 2D vectors"""
    return u.y * v.x - u.x * v.y

def point_in_triangle2d(p: Vector, a: Vector, b: Vector, c: Vector) -> bool:
    """Check if the point is inside the 2D triangle"""
    pab = cross2d(p - a, b - a)
    pbc = cross2d(p - b, c - b)
    if pab * pbc < 0:
        return False
    pca = cross2d(p - c, a - c)
    if pab * pca < 0:
        return False
    return True

def signed_2d_tri_area(a: Vector, b: Vector, c: Vector) -> float:
    """Signed Area of a 2D Triangle"""
    return (a.x - c.x) * (b.y - c.y) - (a.y - c.y) * (b.x - c.x)

def test_2d_segment_segment(a: Vector, b: Vector, c: Vector, d: Vector) -> bool:
    """2D line segment intersection detection"""
    a1 = signed_2d_tri_area(a, b, d)
    a2 = signed_2d_tri_area(a, b, c)
    if a1 * a2 < 0.0:
        a3 = signed_2d_tri_area(c, d, a)
        a4 = a3 + a2 - a1
        if a3 * a4 < 0.0:
            return True
    return False

def project_triangle_2d(triangle: list[Vector], normal: Vector) -> list[Vector]:
    """Projecting a triangle onto a 2D plane"""
    if abs(normal.x) >= abs(normal.y) and abs(normal.x) >= abs(normal.z):
        # YZ plane
        return [Vector((v.y, v.z)) for v in triangle]
    elif abs(normal.y) >= abs(normal.z):
        # XZ plane
        return [Vector((v.x, v.z)) for v in triangle]
    else:
        # XY plane
        return [Vector((v.x, v.y)) for v in triangle]

def triangle_area(triangle: list[Vector]) -> float:
    a = (triangle[1] - triangle[0]).length
    b = (triangle[2] - triangle[1]).length
    c = (triangle[0] - triangle[2]).length
    s = (a + b + c) / 2  # semi-circumference
    # To prevent negative values due to floating-point errors, use max(..., 0)
    area_val = max(s * (s - a) * (s - b) * (s - c), 0)
    area = math.sqrt(area_val)
    return area

def is_degenerate_triangle(triangle: list[Vector], epsilon: float = 1e-6) -> bool:
    """Check if the triangle is degenerate"""
    area = triangle_area(triangle)
    return area < epsilon

def calc_triangle_normal(triangle: list[Vector]) -> Vector:
    """Calculate triangle normals (weighted by area)"""
    v1 = triangle[1] - triangle[0]
    v2 = triangle[2] - triangle[0]
    normal = v1.cross(v2)
    length = normal.length
    if length > 1e-8:  # For numerical stability
        return normal / length
    return Vector((0, 0, 0))

def intersect_triangle_triangle(t1: list[Vector], t2: list[Vector]) -> bool:
    """Triangle Intersection Detection (Note Numerical Errors)"""
    EPSILON2 = 1e-6  # Numerical calculation tolerance

    # Check for degenerate triangles
    if is_degenerate_triangle(t1, EPSILON2) or is_degenerate_triangle(t2, EPSILON2):
        return False

    # Normal Calculation (Area-Weighted)
    n1 = calc_triangle_normal(t1)
    n2 = calc_triangle_normal(t2)

    # When the normal vector is zero (invalid triangle)
    if n1.length < EPSILON2 or n2.length < EPSILON2:
        return False

    # Constant term of a plane equation
    d1_const = -n1.dot(t1[0])
    d2_const = -n2.dot(t2[0])

    # Calculate the distance between each vertex and the opposing plane
    dist1 = [n2.dot(v) + d2_const for v in t1]
    dist2 = [n1.dot(v) + d1_const for v in t2]

    # If all vertices are on the same side, there is no intersection
    if all(d >= 0 for d in dist1) or all(d <= 0 for d in dist1):
        return False
    if all(d >= 0 for d in dist2) or all(d <= 0 for d in dist2):
        return False

    # Internal function: Calculate intersection points between edges and planes
    def compute_intersection_points(triangle, dists):
        pts = []
        for i in range(3):
            j = (i + 1) % 3
            di = dists[i]
            dj = dists[j]
            # including cases where the vertex lies on the plane
            if abs(di) < 1e-8:
                pts.append(triangle[i])
            if di * dj < 0:
                t = di / (di - dj)
                pt = triangle[i] + t * (triangle[j] - triangle[i])
                pts.append(pt)
            elif abs(dj) < 1e-8:
                pts.append(triangle[j])
        # Remove duplicate points
        unique_pts = []
        for p in pts:
            if not any((p - q).length < 1e-8 for q in unique_pts):
                unique_pts.append(p)
        return unique_pts

    pts1 = compute_intersection_points(t1, dist1)
    pts2 = compute_intersection_points(t2, dist2)

    # If there are fewer than two points of intersection, it is considered not to intersect
    if len(pts1) < 2 or len(pts2) < 2:
        return False

    # Determine the direction of the common line
    d = n1.cross(n2)
    if d.length < 1e-8:
        # If they are nearly coplanar, this method does not process them
        return False
    d.normalize()

    # Project the intersection onto a common line to determine the interval
    s1 = [d.dot(p) for p in pts1]
    s2 = [d.dot(p) for p in pts2]
    seg1_min, seg1_max = min(s1), max(s1)
    seg2_min, seg2_max = min(s2), max(s2)

    # Check for overlapping segments
    if seg1_max < seg2_min or seg2_max < seg1_min:
        return False
    return True

def are_faces_adjacent(face1, face2):
    """Check whether two faces are adjacent"""
    verts1 = set(v.index for v in face1.verts)
    verts2 = set(v.index for v in face2.verts)
    return len(verts1.intersection(verts2)) > 0

def get_face_area(face) -> float:
    """Calculate the area of the surface"""
    if len(face.verts) == 3:
        triangle = [v.co for v in face.verts]
        return triangle_area(triangle)
    else:  # For quadrilaterals
        triangles = [
            [face.verts[0].co, face.verts[1].co, face.verts[2].co],
            [face.verts[0].co, face.verts[2].co, face.verts[3].co]
        ]
        return sum(triangle_area(tri) for tri in triangles)

def is_face_too_small(face, min_area: float = 1e-8) -> bool:
    """Check if the face is too small"""
    return get_face_area(face) < min_area

def get_face_thickness(face, normal: Vector) -> float:
    """Calculate the thickness of the surface"""
    verts = [v.co for v in face.verts]
    min_z = min(v.dot(normal) for v in verts)
    max_z = max(v.dot(normal) for v in verts)
    return max_z - min_z


def find_intersecting_faces_bvh(obj):
    """
    Detect self-intersections within the mesh using BVH.
    Each face (triangular or quadrangular) is first divided into triangles,
    Based on the bounding boxes of each triangle, candidate pairs are
    After performing BVH bounding, conduct detailed triangle intersection testing.
    Adjacent faces (shared vertices) are excluded.
    """
    # Retrieve evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    evaluated_obj = obj.evaluated_get(depsgraph)
    evaluated_mesh = evaluated_obj.data

    # Create a BMesh for work
    bm = bmesh.new()
    bm.from_mesh(evaluated_mesh)
    bm.faces.ensure_lookup_table()
    bm.transform(obj.matrix_world)

    # Create a "triangle" list from each face
    triangles = []         # Each element is [Vector, Vector, Vector]
    face_map = []          # The index of the face to which each triangle originally belonged
    face_vertex_sets = []  # Set of vertex indices for each triangle's original faces (for adjacent face checking)

    for face in bm.faces:
        if len(face.verts) not in [3, 4]:
            continue
        vertex_set = {v.index for v in face.verts}
        if len(face.verts) == 3:
            tri = [v.co.copy() for v in face.verts]
            triangles.append(tri)
            face_map.append(face.index)
            face_vertex_sets.append(vertex_set)
        elif len(face.verts) == 4:
            # Select the division method based on the length of the diagonal
            v = [v.co.copy() for v in face.verts]
            diag1 = (v[2] - v[0]).length_squared
            diag2 = (v[3] - v[1]).length_squared
            if diag1 < diag2:
                tri1 = [v[0], v[1], v[2]]
                tri2 = [v[0], v[2], v[3]]
            else:
                tri1 = [v[0], v[1], v[3]]
                tri2 = [v[1], v[2], v[3]]
            triangles.append(tri1)
            face_map.append(face.index)
            face_vertex_sets.append(vertex_set)
            triangles.append(tri2)
            face_map.append(face.index)
            face_vertex_sets.append(vertex_set)

    # Constructing the vertex list and triangle (polygon) list for BVH tree creation
    bvh_verts = []
    bvh_polys = []
    offset = 0
    for tri in triangles:
        bvh_verts.extend(tri)           # Each triangle is added as an independent vertex set (even duplicating the same vertices)
        bvh_polys.append((offset, offset+1, offset+2))
        offset += 3

    # Create a BVH tree
    epsilon = 1e-6
    bvh_tree = BVHTree.FromPolygons(bvh_verts, bvh_polys, epsilon=epsilon)

    # Acquire candidate pairs from overlaps between BVH
    candidate_pairs = bvh_tree.overlap(bvh_tree)

    intersecting_face_indices = set()
    for i, j in candidate_pairs:
        # To avoid duplicate detection, process only pairs where i < j
        if i >= j:
            continue
        face_i = face_map[i]
        face_j = face_map[j]
        # If the same face is present, exclude it
        if face_i == face_j:
            continue
        # Exclude adjacent faces (those sharing a vertex)
        if face_vertex_sets[i].intersection(face_vertex_sets[j]):
            continue

        tri1 = triangles[i]
        tri2 = triangles[j]
        if intersect_triangle_triangle(tri1, tri2):
            intersecting_face_indices.add(face_i)
            intersecting_face_indices.add(face_j)

    # Clean up BMesh
    bm.free()

    return intersecting_face_indices

def get_new_intersections(obj, original_intersections):
    """
    Detect newly occurring intersections after deformation (BVH version).
    Returns the intersection set excluding the original intersection combinations.
    """
    current_intersections = find_intersecting_faces_bvh(obj)
    new_intersections = current_intersections - original_intersections
    return new_intersections

def find_intersecting_faces_between(clothing_obj, base_obj):
    """
    The intersection surface between clothing_obj and base_obj (surface index on the clothing_obj side)
    Detect quickly using BVHTree, and then use intersect_triangle_triangle to
    Perform detailed intersection detection.
    """

    # Retrieve the mesh from the evaluated object
    depsgraph = bpy.context.evaluated_depsgraph_get()
    cloth_eval = clothing_obj.evaluated_get(depsgraph)
    base_eval = base_obj.evaluated_get(depsgraph)
    mesh_cloth = cloth_eval.to_mesh()
    mesh_base = base_eval.to_mesh()

    # Convert to BMesh and triangulate
    bm_cloth = bmesh.new()
    bm_cloth.from_mesh(mesh_cloth)
    bmesh.ops.triangulate(bm_cloth, faces=bm_cloth.faces[:])
    bm_cloth.faces.ensure_lookup_table()
    bm_cloth.transform(clothing_obj.matrix_world)

    bm_base = bmesh.new()
    bm_base.from_mesh(mesh_base)
    bmesh.ops.triangulate(bm_base, faces=bm_base.faces[:])
    bm_base.faces.ensure_lookup_table()
    bm_base.transform(base_obj.matrix_world)

    # When constructing the BVHTree, the mapping to the face information (the original face.index) is also created simultaneously
    def build_bvh_with_face_mapping(bm):
        verts = [v.co.copy() for v in bm.verts]
        polys = []
        face_indices = []
        for face in bm.faces:
            if len(face.verts) == 3:
                polys.append([v.index for v in face.verts])
                face_indices.append(face.index)
        bvh = BVHTree.FromPolygons(verts, polys, epsilon=0.0001)
        return bvh, face_indices, verts, polys

    bvh_base, base_face_indices, base_verts, base_polys = build_bvh_with_face_mapping(bm_base)
    bvh_cloth, cloth_face_indices, cloth_verts, cloth_polys = build_bvh_with_face_mapping(bm_cloth)

    # BVHTree.overlap retrieves overlapping triangle pairs between both trees
    candidate_pairs = bvh_base.overlap(bvh_cloth)
    intersecting_faces = set()

    # Perform detailed intersection checks (intersect_triangle_triangle) on candidate pairs
    for base_idx, cloth_idx in candidate_pairs:
        base_tri = [base_verts[i] for i in base_polys[base_idx]]
        cloth_tri = [cloth_verts[i] for i in cloth_polys[cloth_idx]]
        if intersect_triangle_triangle(base_tri, cloth_tri):
            # Record the original face index corresponding to the triangle on the clothing side
            intersecting_faces.add(cloth_face_indices[cloth_idx])

    # Cleanup
    bm_cloth.free()
    bm_base.free()
    cloth_eval.to_mesh_clear()
    base_eval.to_mesh_clear()

    return intersecting_faces

def duplicate_geometry_with_positions(obj, new_positions):
    """Duplicate the target object and return an object with its vertex coordinates rewritten to new_positions"""
    new_obj = obj.copy()
    new_obj.data = obj.data.copy()
    bpy.context.scene.collection.objects.link(new_obj)
    mesh = new_obj.data
    armature_obj = get_armature_from_modifier(obj)

    if new_obj.data.shape_keys is None:
        new_obj.shape_key_add(name='Basis')
    tmp_shape_key = new_obj.shape_key_add(name="temp_deformation_shapekey")
    tmp_shape_key.value = 1.0

    matrix_armature_inv_fallback = Matrix.Identity(4)
    for i, v in enumerate(mesh.vertices):
        if armature_obj is not None:
            matrix_armature_inv = calculate_inverse_pose_matrix(new_obj, armature_obj, i)
            if matrix_armature_inv is None:
                matrix_armature_inv = matrix_armature_inv_fallback
            undeformed_world_pos = matrix_armature_inv @ Vector(new_positions[i])
            tmp_shape_key.data[i].co = new_obj.matrix_world.inverted() @ undeformed_world_pos
            matrix_armature_inv_fallback = matrix_armature_inv
        else:
            tmp_shape_key.data[i].co = new_obj.matrix_world.inverted() @ Vector(new_positions[i])

    new_obj.data.update()
    return new_obj

# Subdivide edges with a specified scale factor greater than the median.
def subdivide_long_edges(obj, min_edge_length=0.005, max_edge_length_ratio=2.0, cuts=1):
    """
    Subdivide edges that are longer than the specified median edge length of the designated object by a specified multiple or greater.
    """
    mesh = obj.data
    had_custom_normals = mesh.has_custom_normals

    if not obj or obj.type != 'MESH':
        print("Invalid object")
        return

    if len(obj.data.vertices) == 0:
        print("The mesh has no vertices")
        return

    # --- Save Custom Split Normals Before Subdivision (cKDTree Version) ---
    orig_normals_per_vertex = {}
    kd = None
    if had_custom_normals:
        # Create a normal list for each vertex in a single pass through each loop
        temp_normals = {i: [] for i in range(len(mesh.vertices))}
        for loop in mesh.loops:
            temp_normals[loop.vertex_index].append(loop.normal)
        for v_idx, normals in temp_normals.items():
            if normals:
                avg = Vector((0.0, 0.0, 0.0))
                for n in normals:
                    avg += n
                if avg.length > 1e-8:
                    avg.normalize()
                orig_normals_per_vertex[v_idx] = avg.copy()
        # Combine the coordinates of each vertex into a NumPy array and construct a cKDTree
        points = np.array([v.co[:] for v in mesh.vertices])
        kd = cKDTree(points)

    try:
        # --- Subdivision Processing Using BMesh ---
        bm = bmesh.new()
        bm.from_mesh(mesh)
        bm.edges.ensure_lookup_table()

        # Calculate the length of all edges and find the median
        edge_lengths = []
        for edge in bm.edges:
            if edge.calc_length() >= min_edge_length:
                edge_lengths.append(edge.calc_length())

        if not edge_lengths:
            print("Edge not found")
            bm.free()
            return

        # Sort the edge lengths and calculate the median
        edge_lengths.sort()
        n = len(edge_lengths)
        if n % 2 == 0:
            # For an even number of values, the average of the two middle values
            median_edge_length = (edge_lengths[n//2 - 1] + edge_lengths[n//2]) / 2
        else:
            # For an odd number of values, the median value
            median_edge_length = edge_lengths[n//2]

        threshold_length = median_edge_length * max_edge_length_ratio

        print(f"Median edge length: {median_edge_length:.6f}")
        print(f"Subdivision threshold: {threshold_length:.6f} ({max_edge_length_ratio} times the median)")

        # Identify edges longer than the threshold
        edges_to_subdivide = []
        for edge in bm.edges:
            if edge.calc_length() >= threshold_length:
                edges_to_subdivide.append(edge)

        print(f"Number of edges to subdivide: {len(edges_to_subdivide)} / {len(bm.edges)}")

        if edges_to_subdivide:
            bmesh.ops.subdivide_edges(
                bm,
                edges=edges_to_subdivide,
                cuts=cuts,
                use_grid_fill=True,
                use_single_edge=False,
                use_only_quads=False
            )
            print(f"The edge has been subdivided {cuts} times")

        # Reflect BMesh content onto the mesh
        bm.to_mesh(mesh)
        mesh.update()
        bm.free()
    except Exception as e:
        print(f"An error occurred during subdivision: {e}")
        if 'bm' in locals():
            bm.free()

    # --- After subdivision, reset Custom Split Normals (using cKDTree) ---
    if had_custom_normals and kd is not None:
        new_loop_normals = [None] * len(mesh.loops)
        for i, loop in enumerate(mesh.loops):
            v_index = loop.vertex_index
            v_co = mesh.vertices[v_index].co
            # Search for the nearest vertex in cKDTree (returns distance and index)
            dist, orig_index = kd.query(v_co)
            # Retrieve the saved original normal (if not available, use the current normal)
            new_loop_normals[i] = orig_normals_per_vertex.get(orig_index, mesh.vertices[v_index].normal)
        mesh.use_auto_smooth = True
        mesh.normals_split_custom_set(new_loop_normals)
        mesh.update()

def subdivide_faces(obj, face_indices, cuts=1, max_distance=0.005):
    """
    Subdivide faces within a specified distance in the world coordinate system from the specified faces (face_indices).
    We will use BVHTree for acceleration.
    ※If Custom Split Normals are present, save the average custom normal for each vertex before subdivision,
      After subdivision, the nearest original normal for each loop is interpolated and reset.
    """
    mesh = obj.data
    had_custom_normals = mesh.has_custom_normals

    if not obj or obj.type != 'MESH':
        print("Invalid object")
        return

    if len(obj.data.vertices) == 0:
        print("The mesh has no vertices")
        return

    # --- Save Custom Split Normals Before Subdivision (cKDTree Version) ---
    orig_normals_per_vertex = {}
    kd = None
    if had_custom_normals:
        # Create a normal list for each vertex in a single pass through each loop
        temp_normals = {i: [] for i in range(len(mesh.vertices))}
        for loop in mesh.loops:
            temp_normals[loop.vertex_index].append(loop.normal)
        for v_idx, normals in temp_normals.items():
            if normals:
                avg = Vector((0.0, 0.0, 0.0))
                for n in normals:
                    avg += n
                if avg.length > 1e-8:
                    avg.normalize()
                orig_normals_per_vertex[v_idx] = avg.copy()
        # Combine the coordinates of each vertex into a NumPy array and construct a cKDTree
        points = np.array([v.co[:] for v in mesh.vertices])
        kd = cKDTree(points)

    try:
        # --- Subdivision Processing Using BMesh ---
        bm = bmesh.new()
        bm.from_mesh(mesh)
        bm.faces.ensure_lookup_table()

        # Convert to world coordinate system
        bm.transform(obj.matrix_world)

        # Constructing a BVHTree
        bvh_tree = BVHTree.FromBMesh(bm)

        # Acquire the initial target surface
        initial_faces = {f for f in bm.faces if f.index in face_indices}

        # Set of target surfaces for searching within a specified distance
        faces_within_distance = set(initial_faces)

        # Search for faces within distance_threshold from each initial target face
        for f in initial_faces:
            # Calculate the center point of the surface
            face_center = f.calc_center_median()

            # Set the search range considering the surface normal and size
            # Calculate the maximum edge length of the face and add it to the search radius
            max_edge_length = max([e.calc_length() for e in f.edges])
            search_radius = max_edge_length
            if max_edge_length > max_distance:
                search_radius = max_distance

            # Search for nearby faces using BVHTree
            for (location, normal, index, distance) in bvh_tree.find_nearest_range(face_center, search_radius):
                if index is not None and index < len(bm.faces):
                    candidate_face = bm.faces[index]
                    faces_within_distance.add(candidate_face)

        # Convert from world coordinates back to the original coordinate system
        bm.transform(obj.matrix_world.inverted())

        # Only edges belonging to the target surface within the distance are subject to subdivision
        all_edges_candidates = {edge for f in faces_within_distance for edge in f.edges}

        # Exclude those with edge lengths shorter than 0.004
        min_edge_length = 0.004
        edges_to_subdivide = []

        for edge in all_edges_candidates:
            edge_length = edge.calc_length()
            if edge_length >= min_edge_length:
                edges_to_subdivide.append(edge)

        if edges_to_subdivide:
            bmesh.ops.subdivide_edges(
                bm,
                edges=edges_to_subdivide,
                cuts=cuts,
                use_grid_fill=True,
                use_single_edge=False,
                use_only_quads=True
            )

        # Acquire the target surface and its adjacent surfaces, and further acquire those adjacent surfaces in a single pass
        faces_to_check = set(faces_within_distance)
        # Acquire primary adjacent surfaces
        first_level_adjacent = set()
        for f in faces_within_distance:
            for edge in f.edges:
                first_level_adjacent.update(edge.link_faces)
        faces_to_check.update(first_level_adjacent)

        # Acquire secondary adjacent faces
        for f in first_level_adjacent:
            for edge in f.edges:
                faces_to_check.update(edge.link_faces)

        # Triangulate polygons with five or more sides
        ngons = [f for f in faces_to_check if len(f.verts) > 4]
        if ngons:
            bmesh.ops.triangulate(
                bm,
                faces=ngons,
                quad_method='BEAUTY',
                ngon_method='BEAUTY'
            )

        # Reflect BMesh content onto the mesh
        bm.to_mesh(mesh)
        mesh.update()
        bm.free()
    except Exception as e:
        print(f"An error occurred during subdivision: {e}")

    # --- After subdivision, reset Custom Split Normals (using cKDTree) ---
    if had_custom_normals and kd is not None:
        new_loop_normals = [None] * len(mesh.loops)
        for i, loop in enumerate(mesh.loops):
            v_index = loop.vertex_index
            v_co = mesh.vertices[v_index].co
            # Search for the nearest vertex in cKDTree (returns distance and index)
            dist, orig_index = kd.query(v_co)
            # Retrieve the saved original normal (if not available, use the current normal)
            new_loop_normals[i] = orig_normals_per_vertex.get(orig_index, mesh.vertices[v_index].normal)
        mesh.use_auto_smooth = True
        mesh.normals_split_custom_set(new_loop_normals)
        mesh.update()

# ① Global Dictionary and Helper Functions for Caching Deformation Fields
_deformation_field_cache = {}

def get_deformation_field(field_data_path: str) -> dict:
    """
    Load the Deformation Field data from the specified path, construct a KDTree, and cache it.
    If it has already been loaded, return it from the cache.
    """
    global _deformation_field_cache
    if field_data_path in _deformation_field_cache:
        return _deformation_field_cache[field_data_path]

    # Loading Deformation Field Data
    data = np.load(field_data_path, allow_pickle=True)
    field_points = data['field_points']
    delta_positions = data['delta_positions']

    # If weights do not exist, use all ones
    if 'weights' in data:
        field_weights = data['weights']
    else:
        field_weights = np.ones(len(field_points))

    world_matrix = Matrix(data['world_matrix'])
    world_matrix_inv = world_matrix.inverted()

    # Retrieve the value of kdtree_query_k (use the default value 64 if it does not exist)
    k_neighbors = 64
    if 'kdtree_query_k' in data:
        try:
            k_value = data['kdtree_query_k']
            k_neighbors = int(k_value)
            print(f"kdtree_query_k value: {k_neighbors}")
        except Exception as e:
            print(f"Warning: Could not process kdtree_query_k value: {e}")

    # Building a KDTree
    kdtree = cKDTree(field_points, balanced_tree=False, compact_nodes=False)

    field_info = {
        'data': data,
        'field_points': field_points,
        'delta_positions': delta_positions,
        'field_weights': field_weights,
        'world_matrix': world_matrix,
        'world_matrix_inv': world_matrix_inv,
        'kdtree': kdtree,
        'kdtree_query_k': k_neighbors,
    }
    _deformation_field_cache[field_data_path] = field_info
    return field_info

def get_deformation_field_multi_step(field_data_path: str) -> dict:
    """
    Load multi-stage Deformation Field data from the specified path, construct a KDTree, and cache it.
    Supports multi-stage data processing similar to the apply_field_data function in SaveAndApplyFieldAuto.py.
    """
    global _deformation_field_cache
    multi_step_key = field_data_path + "_multi_step"
    if multi_step_key in _deformation_field_cache:
        return _deformation_field_cache[multi_step_key]

    # Loading Deformation Field Data
    data = np.load(field_data_path, allow_pickle=True)

    # Data Format Verification and Loading
    if 'all_field_points' in data:
        # New format: When coordinates for each step are saved
        all_field_points = data['all_field_points']
        all_delta_positions = data['all_delta_positions']
        num_steps = int(data.get('num_steps', len(all_delta_positions)))
        print(f"Detected multi-step data (new format): {num_steps} steps")

        # Check mirror settings (if not included in the data, use as is)
        enable_x_mirror = data.get('enable_x_mirror', False)
        print(f"X-axis mirror setting: {'Enabled' if enable_x_mirror else 'Disabled'}")

        if enable_x_mirror:
            # X-axis mirroring: Add mirrored data by inverting data with X-coordinates greater than 0
            mirrored_field_points = []
            mirrored_delta_positions = []

            for step in range(num_steps):
                field_points = all_field_points[step].copy()
                delta_positions = all_delta_positions[step].copy()

                if len(field_points) > 0:
                    # Search for data where the X-coordinate is greater than 0
                    x_positive_mask = field_points[:, 0] > 0.0
                    if np.any(x_positive_mask):
                        # Create mirror data
                        mirror_field_points = field_points[x_positive_mask].copy()
                        mirror_delta_positions = delta_positions[x_positive_mask].copy()

                        # Reverse the X-coordinate and X-component displacement
                        mirror_field_points[:, 0] *= -1.0
                        mirror_delta_positions[:, 0] *= -1.0

                        # Combine the original data and mirror data
                        combined_field_points = np.vstack([field_points, mirror_field_points])
                        combined_delta_positions = np.vstack([delta_positions, mirror_delta_positions])

                        mirrored_field_points.append(combined_field_points)
                        mirrored_delta_positions.append(combined_delta_positions)

                        print(f"Step {step+1}: Original number of vertices {len(field_points)} → After mirror application {len(combined_field_points)}")
                    else:
                        mirrored_field_points.append(field_points)
                        mirrored_delta_positions.append(delta_positions)
                        print(f"Step {step+1}: Field vertex count {len(field_points)} (no mirroring)")
                else:
                    mirrored_field_points.append(field_points)
                    mirrored_delta_positions.append(delta_positions)
                    print(f"Step {step+1}: Field vertex count 0")

            # Use data after mirroring
            all_field_points = mirrored_field_points
            all_delta_positions = mirrored_delta_positions
        else:
            # If mirroring is disabled, the original data is used as-is
            print("X-axis mirroring is disabled, so the original data will be used as-is")
            print("field_data_path: ", field_data_path)
            for step in range(num_steps):
                print(f"Step {step+1}: Number of field vertices {len(all_field_points[step])}")

    elif 'field_points' in data and 'all_delta_positions' in data:
        # Old format: When a single coordinate set is saved
        field_points = data['field_points']
        all_delta_positions = data['all_delta_positions']
        num_steps = int(data.get('num_steps', len(all_delta_positions)))

        # In the old format, the same coordinates are used for all steps
        all_field_points = [field_points for _ in range(num_steps)]
        print(f"Detected multi-step data (legacy format): {num_steps} steps")
    else:
        # For backward compatibility, single-step data is also processed
        field_points = data.get('field_points', data.get('delta_positions', []))
        delta_positions = data.get('delta_positions', data.get('all_delta_positions', [[]])[0] if 'all_delta_positions' in data else [])
        all_field_points = [field_points]
        all_delta_positions = [delta_positions]
        num_steps = 1
        print("Detect single-step data")

    # If weights do not exist, use all ones
    if 'weights' in data:
        field_weights = data['weights']
    else:
        field_weights = np.ones(len(all_field_points[0]) if len(all_field_points) > 0 else 0)

    world_matrix = Matrix(data['world_matrix'])
    world_matrix_inv = world_matrix.inverted()

    # Retrieve the value of kdtree_query_k (use the default value 8 if it does not exist)
    k_neighbors = 8
    # if 'kdtree_query_k' in data:
    #     try:
    #         k_value = data['kdtree_query_k']
    #         k_neighbors = int(k_value)
    #         print(f"kdtree_query_k value: {k_neighbors}")
    #     except Exception as e:
    #         print(f"Warning: Could not process kdtree_query_k value: {e}")

    # Loading RBF Parameters
    rbf_epsilon = float(data.get('rbf_epsilon', 0.00001))
    print(f"RBF Interpolation Parameters: Function=multi_quadratic_biharmonic, epsilon={rbf_epsilon}")

    field_info = {
        'data': data,
        'all_field_points': all_field_points,
        'all_delta_positions': all_delta_positions,
        'num_steps': num_steps,
        'field_weights': field_weights,
        'world_matrix': world_matrix,
        'world_matrix_inv': world_matrix_inv,
        'kdtree_query_k': k_neighbors,
        'rbf_epsilon': rbf_epsilon,
        'is_multi_step': num_steps > 1
    }
    _deformation_field_cache[multi_step_key] = field_info
    return field_info

def find_connected_components(mesh_obj):
    """
    Detect unconnected components within a mesh object

    Parameters:
        mesh_obj: Mesh object to be detected

    Returns:
        List[Set[int]]: A list of sets of vertex indices contained in each component
    """
    # Create a BMesh and copy data from the original mesh
    bm = bmesh.new()
    bm.from_mesh(mesh_obj.data)
    bm.verts.ensure_lookup_table()

    # Create vertex index mapping (BMesh indices → original mesh indices)
    vert_indices = {v.index: i for i, v in enumerate(bm.verts)}

    # Tracking unvisited vertices
    unvisited = set(vert_indices.keys())
    components = []

    while unvisited:
        # Start from unvisited vertices
        start_idx = next(iter(unvisited))

        # Detecting connected components using breadth-first search
        component = set()
        queue = [start_idx]

        while queue:
            current = queue.pop(0)
            if current in unvisited:
                unvisited.remove(current)
                component.add(vert_indices[current])  # Convert to the original mesh's indices and add

                # Add adjacent vertices to the queue (only vertices connected by edges)
                for edge in bm.verts[current].link_edges:
                    other = edge.other_vert(bm.verts[current]).index
                    if other in unvisited:
                        queue.append(other)

        # Components with one vertex (isolated vertices) are excluded
        if len(component) > 1:
            components.append(component)

    bm.free()
    return components

def check_uniform_weights(mesh_obj, component_verts, armature_obj):
    """
    Verify that vertices within the specified component have uniform bone weights

    Parameters:
        mesh_obj: Mesh object
        component_verts: A set of vertex indices contained within the component
        armature_obj: Armature for weight verification

    Returns:
        (bool, dict): A flag indicating whether uniform weights are used, and a dictionary of bone names:weight values
    """
    if not armature_obj:
        return False, {}

    # Get all bone names for the armature
    target_bones = {bone.name for bone in armature_obj.data.bones}

    # Get the weight pattern of the first vertex
    first_vert_idx = next(iter(component_verts))
    first_weights = {}

    for group in mesh_obj.vertex_groups:
        if group.name in target_bones:
            weight = 0.0
            try:
                for g in mesh_obj.data.vertices[first_vert_idx].groups:
                    if g.group == group.index:
                        weight = g.weight
                        break
            except RuntimeError:
                pass

            if weight > 0:
                first_weights[group.name] = weight

    # Verify that all other vertices have the same weight pattern
    for vert_idx in component_verts:
        if vert_idx == first_vert_idx:
            continue

        for bone_name, weight in first_weights.items():
            group = mesh_obj.vertex_groups.get(bone_name)
            if not group:
                return False, {}

            current_weight = 0.0
            try:
                for g in mesh_obj.data.vertices[vert_idx].groups:
                    if g.group == group.index:
                        current_weight = g.weight
                        break
            except RuntimeError:
                pass

            # If the weight values differ, it is not uniform
            if abs(current_weight - weight) >= 0.001:
                return False, {}

        # Check for additional bong groups
        for group in mesh_obj.vertex_groups:
            if group.name in target_bones and group.name not in first_weights:
                weight = 0.0
                try:
                    for g in mesh_obj.data.vertices[vert_idx].groups:
                        if g.group == group.index:
                            weight = g.weight
                            break
                except RuntimeError:
                    pass

                if weight > 0:
                    return False, {}

    return True, first_weights

def generate_weight_hash(weights):
    """Generate hash values from the weight dictionary (rounding down any portion less than 0.001)"""
    sorted_items = sorted(weights.items())
    # Round the weight value to the nearest 0.001
    hash_str = "_".join([f"{name}:{round(weight, 3):.3f}" for name, weight in sorted_items])
    return hash_str

def calculate_obb(vertices_world):
    """
    Calculate the optimal orientation bounding box from the vertex's world coordinates

    Parameters:
        vertices_world: List of vertices world coordinates

    Returns:
        (axes, extents): Primary axis direction and half the length in each direction
    """
    if vertices_world is None or len(vertices_world) < 3:
        return None, None

    # Calculate the center of gravity of the point cloud
    centroid = np.mean(vertices_world, axis=0)

    # Move the center of gravity to the origin
    centered = vertices_world - centroid

    # Calculate the covariance matrix
    cov = np.cov(centered, rowvar=False)

    # Calculate Eigenvectors and Eigenvalues
    eigenvalues, eigenvectors = np.linalg.eigh(cov)

    # Eigenvectors become the principal axes
    axes = eigenvectors

    # Calculate the extent in each axis direction
    extents = np.zeros(3)
    for i in range(3):
        axis = axes[:, i]
        projection = np.dot(centered, axis)
        extents[i] = (np.max(projection) - np.min(projection)) / 2.0

    return axes, extents

def separate_and_combine_components(mesh_obj, clothing_armature, do_not_separate_names=None, clustering=True, clothing_avatar_data=None):
    """
    Detect unconnected components within mesh objects,
    Group and separate items with the same bone weight pattern

    Parameters:
        mesh_obj: Mesh object to be processed
        clothing_armature: Clothing Armature Object
        do_not_separate_names: List of object names not to separate (optional)
        clustering: Whether to perform clustering
        clothing_avatar_data: Clothing avatar data (optional)

    Returns:
        (List[bpy.types.Object], List[bpy.types.Object]): A list of separated objects and a list of objects that were not separated
    """
    # If the list of non-separated object names is None, use an empty list
    if do_not_separate_names is None:
        do_not_separate_names = []

    # Get the specified humanoid bone and its auxiliary bones
    allowed_bones = set()
    if clothing_avatar_data:
        # Target humanoid bone name
        target_humanoid_bones = ["Spine", "Chest", "Neck", "LeftBreast", "RightBreast"]

        # Create a mapping from humanoidBones
        humanoid_to_bone = {}
        if "humanoidBones" in clothing_avatar_data:
            for bone_data in clothing_avatar_data["humanoidBones"]:
                humanoid_name = bone_data.get("humanoidBoneName", "")
                bone_name = bone_data.get("boneName", "")
                if humanoid_name and bone_name:
                    humanoid_to_bone[humanoid_name] = bone_name

        # Add the bone name corresponding to the target humanoid bone
        for humanoid_bone in target_humanoid_bones:
            if humanoid_bone in humanoid_to_bone:
                allowed_bones.add(humanoid_to_bone[humanoid_bone])

        # Add related bones from auxiliaryBones
        if "auxiliaryBones" in clothing_avatar_data:
            for aux_bone_data in clothing_avatar_data["auxiliaryBones"]:
                parent_humanoid = aux_bone_data.get("parentHumanoidBoneName", "")
                if parent_humanoid in target_humanoid_bones:
                    bone_name = aux_bone_data.get("boneName", "")
                    if bone_name:
                        allowed_bones.add(bone_name)

        print(f"Allowed bones for separation: {sorted(allowed_bones)}")

    def has_allowed_bone_weights(weights):
        """Check whether the weight pattern contains weights for permitted bones"""
        if not allowed_bones:
            return True  # If there are no restrictions, allow everything

        for bone_name in weights.keys():
            if bone_name in allowed_bones:
                return True
        return False

    # Detect connecting components
    components = find_connected_components(mesh_obj)

    if len(components) <= 1:
        # Do not separate if it is a single connected component
        return [], [mesh_obj]

    print(f"Found {len(components)} connected components in {mesh_obj.name}")

    # Check the weight of each component
    component_data = []
    weight_hash_do_not_separate = []
    for i, component in enumerate(components):
        is_uniform, weights = check_uniform_weights(mesh_obj, component, clothing_armature)

        if is_uniform and weights:
            # Check if it has the weight of a permitted bone
            # if not has_allowed_bone_weights(weights):
            #     print(f"Component {i} in {mesh_obj.name} does not have allowed bone weights, skipping separation")
            #     component_data.append((component, False, {}, "", 0.0))
            #     continue

            # Get the world coordinates of vertices within a component
            vertices_world = []
            for vert_idx in component:
                vert_co = mesh_obj.data.vertices[vert_idx].co.copy()
                vert_world = mesh_obj.matrix_world @ vert_co
                vertices_world.append(np.array([vert_world.x, vert_world.y, vert_world.z]))

            vertices_world = np.array(vertices_world)

            # Calculate OBB
            axes, extents = calculate_obb(vertices_world)

            # Calculate the length of the longest side
            if extents is not None:
                max_extent = np.max(extents) * 2.0  # Since it's half the length, it's twice as long

                # Components with uniform weight
                weight_hash = generate_weight_hash(weights)

                # Exclude components that are too small
                if max_extent < 0.0003:
                    print(f"Component {i} in {mesh_obj.name} is too small (max extent: {max_extent:.4f}), skipping")
                    component_data.append((component, False, {}, "", max_extent))
                else:
                    # Components with names matching patterns included in do_not_separate_names shall not be separated
                    should_separate = True
                    temp_name = f"{mesh_obj.name}_Uniform_{i}"

                    # Object Name Check
                    if should_separate:
                        for name_pattern in do_not_separate_names:
                            if name_pattern in temp_name:
                                should_separate = False
                                print(f"Component {i} in {mesh_obj.name} name matches do_not_separate pattern: {name_pattern}")
                                weight_hash_do_not_separate.append(weight_hash)
                                break

                    if should_separate:
                        for hash_val in weight_hash_do_not_separate:
                            if hash_val == weight_hash:
                                should_separate = False
                                print(f"Component {i} in {mesh_obj.name} weight hash matches do_not_separate pattern: {hash_val}")
                                break

                    if should_separate:
                        print(f"Component {i} in {mesh_obj.name} has uniform weights: {weight_hash} (max extent: {max_extent:.4f})")
                        # Vertex coordinates are also saved
                        component_data.append((component, True, weights, weight_hash, max_extent, vertices_world))
                    else:
                        component_data.append((component, False, {}, "", max_extent))
            else:
                # If OBB calculation fails, do not separate
                print(f"Component {i} in {mesh_obj.name} OBB calculation failed")
                component_data.append((component, False, {}, "", 0.0))
        else:
            # Non-uniform, weightless components
            print(f"Component {i} in {mesh_obj.name} does not have uniform weights")
            component_data.append((component, False, {}, "", 0.0))

    # Group by Weight Hash
    weight_groups = {}
    non_uniform_components = []

    for component, is_uniform, weights, weight_hash, max_extent, *extra_data in component_data:
        if is_uniform:
            if weight_hash not in weight_groups:
                weight_groups[weight_hash] = []
            vertices_world = extra_data[0] if extra_data else None
            weight_groups[weight_hash].append((component, vertices_world))
        else:
            non_uniform_components.append(component)

    # Separate components with uniform weight
    uniform_objects = []

    if clustering:
        # Further clustering of each weight hash component based on spatial distance
        for weight_hash, components_with_coords in weight_groups.items():
            # Calculate the coordinates and size of the component
            component_coords = {}
            component_sizes = {}
            component_indices = {}

            for i, (component, vertices_world) in enumerate(components_with_coords):
                if vertices_world is not None and len(vertices_world) > 0:
                    # Calculate the center of the component
                    center = np.mean(vertices_world, axis=0)
                    # Converting NumPy arrays to Vector
                    vectors = [Vector(v) for v in vertices_world]

                    component_coords[i] = vectors
                    component_sizes[i] = calculate_component_size(vectors)
                    component_indices[i] = component

            # Perform spatial clustering
            clusters = cluster_components_by_adaptive_distance(component_coords, component_sizes)

            print(f"Weight hash {weight_hash} has {len(clusters)} spatial clusters")

            # Create separate objects for each cluster
            for cluster_idx, cluster in enumerate(clusters):
                # Set the name (using the first component ID and spatial cluster ID)
                first_component_id = -1
                for i, (component, is_uniform, weights, hash_val, _, *_) in enumerate(component_data):
                    if is_uniform and hash_val == weight_hash:
                        for comp_idx in cluster:
                            if component == component_indices[comp_idx]:
                                first_component_id = i
                                break
                        if first_component_id >= 0:
                            break

                if first_component_id >= 0:
                    cluster_name = f"{mesh_obj.name}_Uniform_{first_component_id}_Cluster_{cluster_idx}"
                else:
                    cluster_name = f"{mesh_obj.name}_Uniform_Hash_{len(uniform_objects)}_Cluster_{cluster_idx}"

                should_separate = True
                for name_pattern in do_not_separate_names:
                    if name_pattern in cluster_name:
                        print(f"Component {i} in {cluster_name} name matches do_not_separate pattern: {name_pattern}")
                        for (component, vertices_world) in components_with_coords:
                            non_uniform_components.append(component)
                        should_separate = False
                        break
                if not should_separate:
                    continue

                # Saving Active Objects
                original_active = bpy.context.view_layer.objects.active

                # Select the original mesh
                bpy.ops.object.select_all(action='DESELECT')
                mesh_obj.select_set(True)
                bpy.context.view_layer.objects.active = mesh_obj

                # Duplicate the object
                bpy.ops.object.duplicate(linked=False)
                new_obj = bpy.context.active_object
                new_obj.name = cluster_name

                # Collect the vertices of components within the cluster
                keep_vertices = set()
                for comp_idx in cluster:
                    keep_vertices.update(component_indices[comp_idx])

                # Remove all vertices not belonging to this cluster
                # Enter edit mode
                bpy.ops.object.select_all(action='DESELECT')
                new_obj.select_set(True)
                bpy.context.view_layer.objects.active = new_obj
                bpy.ops.object.mode_set(mode='EDIT')
                bpy.ops.mesh.select_mode(type="VERT")

                # Deselect all vertices
                bpy.ops.mesh.select_all(action='DESELECT')

                # Select the vertex to retain
                bpy.ops.object.mode_set(mode='OBJECT')
                for i, vert in enumerate(new_obj.data.vertices):
                    vert.select = i in keep_vertices

                # Delete all vertices except the selected one
                bpy.ops.object.mode_set(mode='EDIT')
                bpy.ops.mesh.select_all(action='INVERT')
                bpy.ops.mesh.delete(type='VERT')
                bpy.ops.object.mode_set(mode='OBJECT')

                # Keep the original shape key on the object
                if mesh_obj.data.shape_keys:
                    for key_block in mesh_obj.data.shape_keys.key_blocks:
                        if key_block.name not in new_obj.data.shape_keys.key_blocks:
                            shape_key = new_obj.shape_key_add(name=key_block.name)
                            # Copy the shape key value
                            shape_key.value = key_block.value

                uniform_objects.append(new_obj)

                # Restore to the original active object
                bpy.context.view_layer.objects.active = original_active

    # If there are components that cannot be separated, duplicate the original mesh
    if non_uniform_components:
        # Saving Active Objects
        original_active = bpy.context.view_layer.objects.active

        # Select the original mesh
        bpy.ops.object.select_all(action='DESELECT')
        mesh_obj.select_set(True)
        bpy.context.view_layer.objects.active = mesh_obj

        # Duplicate the object
        bpy.ops.object.duplicate(linked=False)
        non_uniform_obj = bpy.context.active_object
        non_uniform_obj.name = f"{mesh_obj.name}_NonUniform"

        # Remove all vertices except those belonging to non-separated components
        keep_vertices = set()
        for component in non_uniform_components:
            keep_vertices.update(component)

        # Enter edit mode
        bpy.ops.object.select_all(action='DESELECT')
        non_uniform_obj.select_set(True)
        bpy.context.view_layer.objects.active = non_uniform_obj
        bpy.ops.object.mode_set(mode='EDIT')
        bpy.ops.mesh.select_mode(type="VERT")

        # Deselect all vertices
        bpy.ops.mesh.select_all(action='DESELECT')

        # Select the vertex to retain
        bpy.ops.object.mode_set(mode='OBJECT')
        for i, vert in enumerate(non_uniform_obj.data.vertices):
            vert.select = i in keep_vertices

        # Delete all vertices except the selected one
        bpy.ops.object.mode_set(mode='EDIT')
        bpy.ops.mesh.select_all(action='INVERT')
        bpy.ops.mesh.delete(type='VERT')
        bpy.ops.object.mode_set(mode='OBJECT')

        # Restore to the original active object
        bpy.context.view_layer.objects.active = original_active
    else:
        non_uniform_obj = None

    # Create a list of objects to return
    separated_objects = uniform_objects
    non_separated_objects = [non_uniform_obj] if non_uniform_obj else []

    # Display the number of vertices for objects that were not separated
    if non_uniform_obj:
        print(f"Non-separated object '{non_uniform_obj.name}' vertex count: {len(non_uniform_obj.data.vertices)}")
    else:
        print("No non-separated object.")

    # Display the number of vertices for each separated object
    for sep_obj in uniform_objects:
        print(f"Separated object '{sep_obj.name}' vertex count: {len(sep_obj.data.vertices)}")

    return separated_objects, non_separated_objects

def calculate_optimal_rigid_transform(source_points, target_points):
    """
    Calculate the optimal rigid transformation (rotation and translation) between two point clouds

    Parameters:
        source_points: Source point cloud (Nx3 NumPy array)
        target_points: Destination point cloud (Nx3 NumPy array)

    Returns:
        (R, t): Rotation matrix (3x3) and translation vector (3x1)
    """
    # Calculate the center of gravity of the point cloud
    centroid_source = np.mean(source_points, axis=0)
    centroid_target = np.mean(target_points, axis=0)

    # Move the center of gravity to the origin
    source_centered = source_points - centroid_source
    target_centered = target_points - centroid_target

    # Calculate the covariance matrix
    H = source_centered.T @ target_centered

    # Singular Value Decomposition
    U, S, Vt = np.linalg.svd(H)

    # Calculate the rotation matrix
    R = Vt.T @ U.T

    # Prevent reflection (when the determinant is negative)
    if np.linalg.det(R) < 0:
        Vt[-1, :] *= -1
        R = Vt.T @ U.T

    # Calculate the translation vector
    t = centroid_target - R @ centroid_source

    return R, t

def apply_rigid_transform_to_points(points, R, t):
    """
    Applying rigid body transformation to point clouds

    Parameters:
        points: Point cloud to transform (Nx3 NumPy array)
        R: Rotation Matrix (3x3)
        t: Translation vector (3x1)

    Returns:
        transformed_points: Transformed point cloud (Nx3 NumPy array)
    """
    return (R @ points.T).T + t

def calculate_optimal_similarity_transform(source_points, target_points):
    """
    Calculate the optimal similarity transformation (scale, rotation, translation) between two point clouds

    Parameters:
        source_points: Source point cloud (Nx3 NumPy array)
        target_points: Destination point cloud (Nx3 NumPy array)

    Returns:
        (s, R, t): Scaling factor (scalar), rotation matrix (3x3), translation vector (3x1)
    """
    # Calculate the center of gravity of the point cloud
    centroid_source = np.mean(source_points, axis=0)
    centroid_target = np.mean(target_points, axis=0)

    # Move the center of gravity to the origin
    source_centered = source_points - centroid_source
    target_centered = target_points - centroid_target

    # Calculate the sum of squares of the source point cloud (for scaling factor calculation)
    source_scale = np.sum(source_centered**2)

    # Calculate the covariance matrix
    H = source_centered.T @ target_centered

    # Singular Value Decomposition
    U, S, Vt = np.linalg.svd(H)

    # Calculate the rotation matrix
    R = Vt.T @ U.T

    # Prevent reflection (when the determinant is negative)
    if np.linalg.det(R) < 0:
        Vt[-1, :] *= -1
        R = Vt.T @ U.T

    # Calculate the optimal scaling factor
    trace_RSH = np.sum(S)
    s = trace_RSH / source_scale if source_scale > 0 else 1.0

    # Calculate the translation vector
    t = centroid_target - s * (R @ centroid_source)

    return s, R, t

def apply_similarity_transform_to_points(points, s, R, t):
    """
    Applying a similarity transformation to a point cloud

    Parameters:
        points: Point cloud to transform (Nx3 NumPy array)
        s: Scaling factor (scalar)
        R: Rotation Matrix (3x3)
        t: Translation vector (3x1)

    Returns:
        transformed_points: Transformed point cloud (Nx3 NumPy array)
    """
    return s * (R @ points.T).T + t

def calculate_optimal_similarity_transform_weighted(source_points, target_points, weights):
    """
    Calculate the optimal similarity transformation (scale, rotation, translation) between two weighted point clouds

    Parameters:
        source_points: Source point cloud (Nx3 NumPy array)
        target_points: Destination point cloud (Nx3 NumPy array)
        weights: Weights for each point (Nx1 NumPy array)

    Returns:
        (s, R, t): Scaling factor (scalar), rotation matrix (3x3), translation vector (3x1)
    """
    # Normalize the weights
    weights = weights / np.sum(weights) if np.sum(weights) > 0 else np.ones_like(weights) / len(weights)

    # Calculate the weighted center of gravity
    centroid_source = np.sum(source_points * weights[:, np.newaxis], axis=0)
    centroid_target = np.sum(target_points * weights[:, np.newaxis], axis=0)

    print(f"centroid_source: {centroid_source}")
    print(f"centroid_target: {centroid_target}")
    print(f"centroid_source_original: {np.mean(source_points, axis=0)}")
    print(f"centroid_target_original: {np.mean(target_points, axis=0)}")

    # Move the center of gravity to the origin
    source_centered = source_points - centroid_source
    target_centered = target_points - centroid_target

    # Calculate the sum of squares for weighted source point clouds (for scaling coefficient calculation)
    source_scale = np.sum(weights[:, np.newaxis] * source_centered**2)

    # Calculate the weighted covariance matrix
    H = (source_centered * weights[:, np.newaxis]).T @ target_centered

    # Singular Value Decomposition
    U, S, Vt = np.linalg.svd(H)

    # Calculate the rotation matrix
    R = Vt.T @ U.T

    # Prevent reflection (when the determinant is negative)
    if np.linalg.det(R) < 0:
        Vt[-1, :] *= -1
        R = Vt.T @ U.T

    # Calculate the optimal scaling factor
    trace_RSH = np.sum(S)
    s = trace_RSH / source_scale if source_scale > 0 else 1.0

    # Calculate the translation vector
    t = centroid_target - s * (R @ centroid_source)

    return s, R, t

def get_distance_weight_influence_factors(obj, influence_range=1.0, min_weight_diff_threshold=0.01):
    """
    Get the influence coefficient from the DistanceWeight vertex group

    Parameters:
        obj: Target mesh object
        influence_range: Difference between maximum and minimum influence (0.0-1.0, default 0.5)
        min_weight_diff_threshold: Minimum threshold for the difference between maximum and minimum values (if smaller than this, equal weighting is applied)

    Returns:
        influence_factors: Influence factor coefficients for each vertex (Nx1 NumPy array), or None (if no vertex groups exist)
    """
    # Search for DistanceWeight vertex groups
    distance_weight_group = None
    for group in obj.vertex_groups:
        if group.name == "DistanceWeight":
            distance_weight_group = group
            break

    if distance_weight_group is None:
        return None

    # Get the weight value for each vertex
    num_vertices = len(obj.data.vertices)
    weights = np.zeros(num_vertices)

    for i in range(num_vertices):
        try:
            weight = 0.0
            for g in obj.data.vertices[i].groups:
                if g.group == distance_weight_group.index:
                    weight = g.weight
                    break
            weights[i] = weight
        except RuntimeError:
            weights[i] = 0.0  # If the vertex does not belong to a group

    # Calculate the maximum and minimum values
    max_weight = np.max(weights)
    min_weight = np.min(weights)
    weight_range = max_weight - min_weight

    # If the range is smaller than the threshold, it returns an equal impact
    if weight_range < min_weight_diff_threshold:
        print(f"DistanceWeight range ({weight_range:.4f}) is below threshold ({min_weight_diff_threshold:.4f}), using uniform influence")
        return np.ones(num_vertices)

    # Calculate the impact
    # The vertex with the maximum weight has an influence of 1.0, while the vertex with the minimum weight has an influence of (1.0 - influence_range)
    min_influence = 1.0 - influence_range
    max_influence = 1.0

    # Normalize weight values to 0-1 and map them to the influence range
    normalized_weights = (weights - min_weight) / weight_range
    influence_factors = min_influence + normalized_weights * influence_range

    print(f"DistanceWeight influence: min={min_influence:.3f}, max={max_influence:.3f}, range={influence_range:.3f}")
    print(f"DistanceWeight: min={min_weight:.3f}, max={max_weight:.3f}, range={weight_range:.3f}")

    return influence_factors

def join_objects(objects, target_name=None):
    """
    Combine multiple objects

    Parameters:
        objects: List of objects to merge
        target_name: Name of the combined object (optional)

    Returns:
        bpy.types.Object: Combined object (on success) or None (on failure)
    """
    if not objects:
        return None

    if len(objects) == 1:
        if target_name and objects[0].name != target_name:
            objects[0].name = target_name
        return objects[0]

    # Save active objects
    original_active = bpy.context.view_layer.objects.active

    # Clear all selections
    bpy.ops.object.select_all(action='DESELECT')

    # Select the objects to combine
    for obj in objects:
        obj.select_set(True)

    # Set the first object as active
    bpy.context.view_layer.objects.active = objects[0]

    # Combine objects
    bpy.ops.object.join()

    # Retrieve the combined object
    joined_obj = bpy.context.view_layer.objects.active

    # Set name
    if target_name:
        joined_obj.name = target_name

    # Restore the original active object
    bpy.context.view_layer.objects.active = original_active

    return joined_obj

def batch_process_vertices_simple(vertices, kdtree, field_points, delta_positions, field_weights,
                         field_matrix, field_matrix_inv, target_matrix, target_matrix_inv, batch_size=1000):
    """
    Process vertices in batches
    """
    num_vertices = len(vertices)
    results = np.zeros((num_vertices, 3))

    for start_idx in range(0, num_vertices, batch_size):
        end_idx = min(start_idx + batch_size, num_vertices)
        batch_vertices = vertices[start_idx:end_idx]

        # Convert all vertices in the batch to field space
        batch_world = np.array([target_matrix @ Vector(v) for v in batch_vertices])
        batch_field = np.array([field_matrix_inv @ Vector(v) for v in batch_world])

        # Search for Recent Contacts (Batch Processing)
        distances, indices = kdtree.query(batch_field, k=64)

        # Calculate the displacement of each vertex
        for i, (vert_field, dist, idx) in enumerate(zip(batch_field, distances, indices)):
            x_sign = 1 if vert_field[0] >= 0 else -1

            # Calculate weighted displacement
            weights = 1.0 / (dist + 0.0001)
            if weights.sum() > 0.0:
                weights /= weights.sum()
            else:
                weights *= 0

            deltas = delta_positions[idx]

            displacement = (deltas * weights[:, np.newaxis]).sum(axis=0)

            # Calculate displacement in world space
            world_displacement = field_matrix.to_3x3() @ Vector(displacement)

            results[start_idx + i] = batch_world[i] + world_displacement

    return results

def process_field_deformation_simple(target_obj, field_data_path, blend_shape_labels=None, clothing_avatar_data=None, shape_key_name="SymmetricDeformed", ignore_blendshape=None, target_shape_key=None, base_shape_key=None):
    # Retrieve vertex positions (original state) from the evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = target_obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data

    # Preparing Shape Keys
    if target_obj.data.shape_keys is None:
        target_obj.shape_key_add(name='Basis')
    shape_key = target_obj.shape_key_add(name=shape_key_name)
    shape_key.value = 1.0

    data = np.load(field_data_path, allow_pickle=True)
    field_points = data['field_points']
    delta_positions = data['delta_positions']
    field_weights = data['weights']
    field_matrix = Matrix(data['world_matrix'])
    field_matrix_inv = field_matrix.inverted()
    kdtree = cKDTree(field_points, balanced_tree=False, compact_nodes=False)

    original_positions = np.array([v.co for v in eval_mesh.vertices])

    chosen_positions = batch_process_vertices_simple(
        original_positions,
        kdtree,
        field_points,
        delta_positions,
        field_weights,
        field_matrix,
        field_matrix_inv,
        target_obj.matrix_world,
        target_obj.matrix_world.inverted(),
        batch_size=1000
    )

    blendshape_ignored = True

    armature_obj = get_armature_from_modifier(target_obj)
    if not armature_obj:
        raise ValueError("Armature modifier not found")

    matrix_armature_inv_fallback = Matrix.Identity(4)
    for i, world_pos in enumerate(chosen_positions):
        matrix_armature_inv = calculate_inverse_pose_matrix(target_obj, armature_obj, i)
        if matrix_armature_inv is None:
            matrix_armature_inv = matrix_armature_inv_fallback
        undeformed_world_pos = matrix_armature_inv @ Vector(world_pos)
        local_pos = target_obj.matrix_world.inverted() @ undeformed_world_pos
        shape_key.data[i].co = local_pos
        matrix_armature_inv_fallback = matrix_armature_inv

    return shape_key, blendshape_ignored

def apply_blendshape_deformation_fields(target_obj, field_data_path, blend_shape_labels=None, clothing_avatar_data=None, blend_shape_values=None):
    """
    Apply the Deformation Field for BlendShape and save the result as a shape key with the name _BaseShape appended

    Parameters:
        target_obj: Target mesh object
        field_data_path: Deformation Field path
        blend_shape_labels: List of blend shape labels to apply
        clothing_avatar_data: Clothing avatar data
        blend_shape_values: List of blend shape values
    """
    if not blend_shape_labels or not clothing_avatar_data:
        return

    # Create a dictionary of blend shape values
    blend_shape_value_dict = {}
    if blend_shape_values:
        for i, label in enumerate(blend_shape_labels):
            if i < len(blend_shape_values):
                blend_shape_value_dict[label] = blend_shape_values[i]
            else:
                blend_shape_value_dict[label] = 1.0  # If insufficient, 1.0
    else:
        # If blend_shape_values is None, all values are 1.0
        for label in blend_shape_labels:
            blend_shape_value_dict[label] = 1.0

    print(f"Blend Shape Value: {blend_shape_value_dict}")

    # Get the original vertex position
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = target_obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data
    original_positions = np.array([v.co for v in eval_mesh.vertices])
    armature_obj = get_armature_from_modifier(target_obj)

    label_to_filepath = {}
    label_to_mask_bones = {}
    for field in clothing_avatar_data.get("invertedBlendShapeFields", []):
        label_to_filepath[field["label"]] = field["filePath"]
        if "maskBones" in field:
            label_to_mask_bones[field["label"]] = field["maskBones"]

    label_to_filepath_normal = {}
    label_to_mask_bones_normal = {}
    for field in clothing_avatar_data.get("blendShapeFields", []):
        label_to_filepath_normal[field["label"]] = field["filePath"]
        if "maskBones" in field:
            label_to_mask_bones_normal[field["label"]] = field["maskBones"]

    # Processing for each blend shape label
    for label in blend_shape_labels:
        if label in label_to_filepath and (target_obj.data.shape_keys is None or label not in target_obj.data.shape_keys.key_blocks):
            blend_field_path = os.path.join(os.path.dirname(field_data_path), label_to_filepath[label])
            if os.path.exists(blend_field_path):
                start_value = 1.0 - blend_shape_value_dict[label]
                if start_value < 0.00001:
                    start_value = 0.0
                end_value = 1.0  # The ending value is always 1.0

                print(f"Applying inverted blend shape field for {label}: {start_value} -> {end_value}")
                field_info_blend = get_deformation_field_multi_step(blend_field_path)
                blend_points = field_info_blend['all_field_points']
                blend_deltas = field_info_blend['all_delta_positions']
                blend_field_weights = field_info_blend['field_weights']
                blend_matrix = field_info_blend['world_matrix']
                blend_matrix_inv = field_info_blend['world_matrix_inv']
                blend_k_neighbors = field_info_blend['kdtree_query_k']
                mask_weights = None
                if label in label_to_mask_bones:
                    mask_weights = create_blendshape_mask(target_obj, label_to_mask_bones[label], clothing_avatar_data, field_name=label, store_debug_mask=True)
                    # If all mask_weights are 0, skip processing
                    if mask_weights is not None and np.all(mask_weights == 0):
                        print(f"Skipping {label} - all mask weights are zero")
                        continue

                # Use the new custom range processing
                world_positions = batch_process_vertices_with_custom_range(
                    original_positions,
                    blend_points,
                    blend_deltas,
                    blend_field_weights,
                    blend_matrix,
                    blend_matrix_inv,
                    target_obj.matrix_world,
                    target_obj.matrix_world.inverted(),
                    start_value,
                    end_value,
                    deform_weights=mask_weights,
                    batch_size=1000,
                    k=blend_k_neighbors
                )

                # Save as Shape Key
                shape_key_name = f"{label}_BaseShape"
                if target_obj.data.shape_keys is None:
                    target_obj.shape_key_add(name="Basis", from_mix=False)

                shape_key = target_obj.shape_key_add(name=shape_key_name, from_mix=False)
                matrix_armature_inv_fallback = Matrix.Identity(4)
                for i in range(len(world_positions)):
                    matrix_armature_inv = calculate_inverse_pose_matrix(target_obj, armature_obj, i)
                    if matrix_armature_inv is None:
                        matrix_armature_inv = matrix_armature_inv_fallback
                    undeformed_world_pos = matrix_armature_inv @ Vector(world_positions[i])
                    local_pos = target_obj.matrix_world.inverted() @ undeformed_world_pos
                    shape_key.data[i].co = local_pos
                    matrix_armature_inv_fallback = matrix_armature_inv

                # Create a negating shape key
                inv_shape_key = target_obj.shape_key_add(name=f"{label}_temp", from_mix=False)
                # Calculate and apply a displacement to counteract the generated shape key
                basis_key = target_obj.data.shape_keys.key_blocks["Basis"]
                if start_value < 0.00001:
                    for i in range(len(world_positions)):
                        # Calculate the displacement of BaseShape (current position - original position)
                        base_displacement = Vector(shape_key.data[i].co) - Vector(basis_key.data[i].co)
                        # Set displacement (opposite direction) for cancellation
                        inv_shape_key.data[i].co = Vector(basis_key.data[i].co) - base_displacement
                else:
                    blend_field_path = os.path.join(os.path.dirname(field_data_path), label_to_filepath_normal[label])
                    if os.path.exists(blend_field_path):
                        start_value = blend_shape_value_dict[label]
                        end_value = 1.0  # The ending value is always 1.0

                        print(f"Applying blend shape field for {label}: {start_value} -> {end_value}")
                        field_info_blend = get_deformation_field_multi_step(blend_field_path)
                        blend_points = field_info_blend['all_field_points']
                        blend_deltas = field_info_blend['all_delta_positions']
                        blend_field_weights = field_info_blend['field_weights']
                        blend_matrix = field_info_blend['world_matrix']
                        blend_matrix_inv = field_info_blend['world_matrix_inv']
                        blend_k_neighbors = field_info_blend['kdtree_query_k']
                        mask_weights = None
                        if label in label_to_mask_bones_normal:
                            mask_weights = create_blendshape_mask(target_obj, label_to_mask_bones_normal[label], clothing_avatar_data, field_name=label, store_debug_mask=True)
                            # If all mask_weights are 0, skip processing
                            if mask_weights is not None and np.all(mask_weights == 0):
                                print(f"Skipping {label} - all mask weights are zero")
                                continue

                        # Use the new custom range processing
                        world_positions = batch_process_vertices_with_custom_range(
                            original_positions,
                            blend_points,
                            blend_deltas,
                            blend_field_weights,
                            blend_matrix,
                            blend_matrix_inv,
                            target_obj.matrix_world,
                            target_obj.matrix_world.inverted(),
                            start_value,
                            end_value,
                            deform_weights=mask_weights,
                            batch_size=1000,
                            k=blend_k_neighbors
                        )

                        matrix_armature_inv_fallback = Matrix.Identity(4)
                        for i in range(len(world_positions)):
                            matrix_armature_inv = calculate_inverse_pose_matrix(target_obj, armature_obj, i)
                            if matrix_armature_inv is None:
                                matrix_armature_inv = matrix_armature_inv_fallback
                            undeformed_world_pos = matrix_armature_inv @ Vector(world_positions[i])
                            local_pos = target_obj.matrix_world.inverted() @ undeformed_world_pos
                            base_displacement = Vector(shape_key.data[i].co) - Vector(basis_key.data[i].co)
                            inv_shape_key.data[i].co = local_pos - base_displacement
                            matrix_armature_inv_fallback = matrix_armature_inv

                print(f"Created shape key: {shape_key_name}")
                print(f"Created inverse shape key: {label}_temp")
            else:
                print(f"Warning: Field file not found for blend shape {label}")
        else:
            print(f"Warning: No field data found for blend shape {label}")

def process_field_deformation(target_obj, field_data_path, blend_shape_labels=None, clothing_avatar_data=None, shape_key_name="SymmetricDeformed", ignore_blendshape=None, target_shape_key=None, base_shape_key=None):
    # ① Retrieve vertex positions (original state) from the evaluated mesh
    if target_shape_key is not None:
        # Set all shape key values to 0
        for sk in target_obj.data.shape_keys.key_blocks:
            sk.value = 0.0
        # Set the value of the target shape key to 1
        target_shape_key.value = 1.0

    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj_original = target_obj.evaluated_get(depsgraph)
    eval_mesh_original = eval_obj_original.data
    original_positions = np.array([v.co for v in eval_mesh_original.vertices])

    used_shape_keys = []
    if ignore_blendshape is None or ignore_blendshape is False:
        if blend_shape_labels and clothing_avatar_data:
            # Retrieve vertex positions from pre-created shape keys
            for label in blend_shape_labels:
                # If ignore_blendshape is None, automatic detection is performed. If a shape key with the same name exists on the clothing model, it is not applied
                if ignore_blendshape is None and target_obj.data.shape_keys and label in target_obj.data.shape_keys.key_blocks:
                    print(f"Skipping {label} - already has shape key")
                    continue
                target_avatar_base_shape_key_name = f"{label}_BaseShape"
                if target_obj.data.shape_keys and target_avatar_base_shape_key_name in target_obj.data.shape_keys.key_blocks:
                    target_avatar_base_shape_key = target_obj.data.shape_keys.key_blocks[target_avatar_base_shape_key_name]
                    target_avatar_base_shape_key.value = 1.0
                    print(f"Using shape key {target_avatar_base_shape_key_name} for BlendShape deformation")
                    used_shape_keys.append(target_avatar_base_shape_key_name)
                else:
                    print(f"Warning: Shape key {target_avatar_base_shape_key_name} not found")

    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = target_obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data
    # blend_positions contains the vertex positions after BlendShape application
    blend_positions = np.array([v.co for v in eval_mesh.vertices])

    # ③ Obtain the main Deformation Field information
    field_info = get_deformation_field_multi_step(field_data_path)
    field_points = field_info['all_field_points']
    delta_positions = field_info['all_delta_positions']
    field_weights = field_info['field_weights']
    field_matrix = field_info['world_matrix']
    field_matrix_inv = field_info['world_matrix_inv']
    k_neighbors = field_info['kdtree_query_k']

    final_positions = batch_process_vertices_multi_step(
        blend_positions,
        field_points,
        delta_positions,
        field_weights,
        field_matrix,
        field_matrix_inv,
        target_obj.matrix_world,
        target_obj.matrix_world.inverted(),
        None,
        batch_size=1000,
        k=k_neighbors
    )

    for label in used_shape_keys:
        target_obj.data.shape_keys.key_blocks[label].value = 0.0

    armature_obj = get_armature_from_modifier(target_obj)
    if not armature_obj:
        raise ValueError("Armature modifier not found")

    # ⑩ Save Shape Key or Calculate Difference
    if target_shape_key is not None and base_shape_key is not None:
        # Difference calculation mode: Calculate the difference from base_shape_key and store it in target_shape_key
        matrix_armature_inv_fallback = Matrix.Identity(4)
        for i in range(len(original_positions)):
            matrix_armature_inv = calculate_inverse_pose_matrix(target_obj, armature_obj, i)
            if matrix_armature_inv is None:
                matrix_armature_inv = matrix_armature_inv_fallback
            undeformed_world_pos = matrix_armature_inv @ Vector(final_positions[i])
            local_pos = target_obj.matrix_world.inverted() @ undeformed_world_pos

            # Calculate the difference from base_shape_key
            base_pos = base_shape_key.data[i].co
            delta = local_pos - base_pos

            # Save the difference to target_shape_key
            target_shape_key.data[i].co = target_obj.data.vertices[i].co + delta
            matrix_armature_inv_fallback = matrix_armature_inv
        return target_shape_key
    else:
        # Normal Mode: Create and save a new shape key
        matrix_armature_inv_fallback = Matrix.Identity(4)
        if target_obj.data.shape_keys is None:
            target_obj.shape_key_add(name='Basis')
        shape_key_a = target_obj.shape_key_add(name=shape_key_name)
        shape_key_a.value = 1.0

        for i in range(len(original_positions)):
            matrix_armature_inv = calculate_inverse_pose_matrix(target_obj, armature_obj, i)
            if matrix_armature_inv is None:
                matrix_armature_inv = matrix_armature_inv_fallback
            undeformed_world_pos = matrix_armature_inv @ Vector(final_positions[i])
            local_pos = target_obj.matrix_world.inverted() @ undeformed_world_pos
            shape_key_a.data[i].co = local_pos
            matrix_armature_inv_fallback = matrix_armature_inv
        return shape_key_a

class TransitionCache:
    """Class that caches the execution results of Transition"""
    def __init__(self):
        self.cache = {}  # {blend_shape_combination_hash: {vertices: np.array, blendshape_values: dict}}

    def get_cache_key(self, blendshape_values):
        """Generate cache keys from BlendShape values"""
        sorted_items = sorted(blendshape_values.items())
        return hash(tuple(sorted_items))

    def store_result(self, blendshape_values, vertices, all_blendshape_values):
        """Store execution results in the cache"""
        cache_key = self.get_cache_key(blendshape_values)

        # If the same key already exists, do not overwrite it
        if cache_key in self.cache:
            print(f"Cache key already exists, keeping existing entry: {cache_key}")
            return

        self.cache[cache_key] = {
            'vertices': vertices.copy(),
            'blendshape_values': all_blendshape_values.copy()
        }
        print(f"Stored new cache entry: {cache_key}")

    def find_interpolation_candidates(self, target_blendshape_values, changing_blendshape, blendshape_groups=None):
        """Search for linear interpolation candidates"""
        candidates = []

        # Identify the BlendShapeGroup to which changing_blendshape belongs
        changing_blendshape_group = None
        group_blendshapes = set()

        if blendshape_groups:
            for group in blendshape_groups:
                blendshapes_in_group = group.get('blendShapeFields', [])
                if changing_blendshape in blendshapes_in_group:
                    changing_blendshape_group = group
                    group_blendshapes = set(blendshapes_in_group)
                    break

        print(f"target_blendshape_values: {target_blendshape_values}")

        for cache_key, cached_data in self.cache.items():
            cached_values = cached_data['blendshape_values']

            print(f"cached_values: {cached_values}")

            values_match = True

            # When belonging to a BlendShapeGroup: Check if the values of other BlendShape in the same group are the same
            if changing_blendshape_group:
                for name in group_blendshapes:
                    if name != changing_blendshape and abs(cached_values.get(name, 0.0) - target_blendshape_values.get(name, 0.0)) > 1e-6:
                        values_match = False
                        break

            if values_match:
                cached_changing_value = cached_values.get(changing_blendshape, 0.0)
                target_changing_value = target_blendshape_values.get(changing_blendshape, 0.0)

                print(f"cached_changing_value: {cached_changing_value}, target_changing_value: {target_changing_value}")

                candidates.append({
                    'cached_value': cached_changing_value,
                    'target_value': target_changing_value,
                    'vertices': cached_data['vertices'],
                    'distance': abs(cached_changing_value - target_changing_value)
                })

        return candidates

    def interpolate_result(self, target_blendshape_values, changing_blendshape, blendshape_groups=None):
        """Calculate the result using linear interpolation"""
        candidates = self.find_interpolation_candidates(target_blendshape_values, changing_blendshape, blendshape_groups)

        if len(candidates) < 2:
            return None

        target_value = target_blendshape_values.get(changing_blendshape, 0.0)

        # Find all candidate pairs that sandwich the target value
        valid_pairs = []
        for i in range(len(candidates)):
            for j in range(i + 1, len(candidates)):
                val1, val2 = candidates[i]['cached_value'], candidates[j]['cached_value']

                if (val1 <= target_value <= val2) or (val2 <= target_value <= val1):
                    if abs(val2 - val1) < 1e-6:
                        continue  # If the values are the same, skip

                    interval_size = abs(val2 - val1)
                    valid_pairs.append({
                        'interval_size': interval_size,
                        'candidate1': candidates[i],
                        'candidate2': candidates[j],
                        'val1': val1,
                        'val2': val2
                    })

        if not valid_pairs:
            return None

        # Select the pair with the smallest interval
        best_pair = min(valid_pairs, key=lambda x: x['interval_size'])

        # Perform linear interpolation
        val1, val2 = best_pair['val1'], best_pair['val2']
        t = (target_value - val1) / (val2 - val1)
        vertices1 = best_pair['candidate1']['vertices']
        vertices2 = best_pair['candidate2']['vertices']

        interpolated_vertices = vertices1 + t * (vertices2 - vertices1)
        print(f"Using linear interpolation with interval size {best_pair['interval_size']:.6f} for {changing_blendshape}")
        return interpolated_vertices


def execute_transitions_with_cache(deferred_transitions, transition_cache, target_obj, rigid_transformation=False):
    """Execute delayed Transition using the cache system"""
    print(f"Executing {len(deferred_transitions)} deferred transitions with caching...")

    # Retrieve information from BlendShapeGroups (from the first transition_data)
    blendshape_groups = None
    if deferred_transitions:
        base_avatar_data = deferred_transitions[0].get('base_avatar_data')
        if base_avatar_data:
            blendshape_groups = base_avatar_data.get('blendShapeGroups', [])

    # Get the initial shape key name shape_key_name corresponding to label
    # Save the initial state to the cache
    target_shape_key_label_to_name = {}
    for transition_data in deferred_transitions:
        config_data = transition_data['config_data']
        shape_key_name = transition_data['target_shape_key_name']
        shape_key_label = transition_data['target_label']
        target_shape_key_label_to_name[shape_key_label] = shape_key_name

        target_shape_key = None
        if target_obj.data.shape_keys and shape_key_name and shape_key_name in target_obj.data.shape_keys.key_blocks:
            target_shape_key = target_obj.data.shape_keys.key_blocks.get(shape_key_name)

        if target_shape_key is None:
            print(f"Target shape key {shape_key_name} / {target_shape_key.name} not found")
            continue

        print(f"Target shape key {shape_key_name} / {target_shape_key.name} found")

        # Get the current position of the target_shape_key
        initial_vertices = np.array([v.co for v in target_shape_key.data])

        initial_settings = []
        if shape_key_label == 'Basis':
            initial_settings = config_data.get('targetBlendShapeSettings', [])
        else:
            blend_fields = config_data.get('blendShapeFields', [])
            for blend_field in blend_fields:
                if blend_field['label'] == shape_key_label:
                    initial_settings = blend_field.get('targetBlendShapeSettings', [])
                    break
            if not initial_settings:
                initial_settings = config_data.get('targetBlendShapeSettings', [])

        initial_blendshape_values = {}
        for setting in initial_settings:
            blend_shape_name = setting.get('name', '')
            blend_shape_value = setting.get('value', 0.0)
            if blend_shape_name:
                initial_blendshape_values[blend_shape_name] = blend_shape_value

        # Save the initial state to the cache
        transition_cache.store_result(initial_blendshape_values, initial_vertices, initial_blendshape_values)
        print(f"Cached initial state for {shape_key_label} with {len(initial_blendshape_values)} BlendShape values")

    initial_vertices_dict = {}

    # Collect the operations and transition_set for each transition_data in advance
    transition_operations = []
    for transition_data in deferred_transitions:
        config_data = transition_data['config_data']
        target_label = transition_data['target_label']
        target_shape_key_name = transition_data['target_shape_key_name']
        clothing_avatar_data = transition_data['clothing_avatar_data']

        # Get Transition Details
        transition_sets = config_data.get('blend_shape_transition_sets', [])
        target_transition_set = None
        target_shape_key_name = None
        for transition_set in transition_sets:
            source_label = transition_set.get('source_label', '')
            target_shape_key_name = target_shape_key_label_to_name.get(source_label, '')
            print(f"source_label: {source_label}, target_shape_key_name: {target_shape_key_name}")
            if transition_set.get('label', '') == target_label and target_shape_key_name in target_obj.data.shape_keys.key_blocks:
                target_transition_set = transition_set
                print(f"Found transition set for {target_label} with source label {source_label}")
                break

        if not target_transition_set:
            # Test the default Transition
            default_transition_sets = config_data.get('blend_shape_default_transition_sets', [])
            for default_transition_set in default_transition_sets:
                source_label = default_transition_set.get('source_label', '')
                target_shape_key_name = target_shape_key_label_to_name.get(source_label, '')
                print(f"source_label: {source_label}, target_shape_key_name: {target_shape_key_name}")
                if default_transition_set.get('label', '') == target_label and target_shape_key_name in target_obj.data.shape_keys.key_blocks:
                    target_transition_set = default_transition_set
                    print(f"Found default transition set for {target_label} with source label {source_label}")
                    break

        if not target_transition_set:
            print(f"No transition set found for {target_label}")
            continue

        # Retrieve the initial BlendShape values from the current_settings of the transition_set
        current_settings = target_transition_set.get('current_settings', [])
        source_label = target_transition_set['source_label']
        initial_blendshape_values = {}

        # Set the BlendShape value from current_settings
        for setting in current_settings:
            blend_shape_name = setting.get('name', '')
            blend_shape_value = setting.get('value', 0.0)
            if blend_shape_name:
                initial_blendshape_values[blend_shape_name] = blend_shape_value

        # Get the ShapeKey closest to the state after the selected Transition
        target_shape_key_name = target_shape_key_label_to_name.get(source_label, None)
        target_shape_key = None
        if target_obj.data.shape_keys and target_shape_key_name and target_shape_key_name in target_obj.data.shape_keys.key_blocks:
            target_shape_key = target_obj.data.shape_keys.key_blocks.get(target_shape_key_name)

        if target_shape_key is None:
            print(f"Target shape key {target_shape_key_name} not found")
            continue
        else:
            print(f"Target shape key {target_shape_key_name} / {target_shape_key.name} found")

        # Get the current position of the target_shape_key
        initial_vertices = np.array([v.co for v in target_shape_key.data])

        print(f"target_transition_set: {target_transition_set}")

        initial_vertices_dict[transition_data['target_shape_key_name']] = initial_vertices.copy()

        for transition in target_transition_set.get('transitions', []):
            operations = transition.get('operations', [])
            if not operations:
                print(f"No operations found for {target_label}")
                continue

            print(f"number of operations: {len(operations)}")
            print(f"operations: {operations}")

            # Save operations and transition_data as a set (including initial BlendShape values)
            transition_operations.append({
                'operations': operations,
                'transition_data': transition_data,
                'current_blendshape_values': initial_blendshape_values.copy(),
                'initial_vertices': initial_vertices.copy(),
                'current_vertices': initial_vertices.copy(),
                'mask_bones': target_transition_set.get("maskBones", [])
            })

        if target_transition_set.get('transitions', []) is None or len(target_transition_set.get('transitions', [])) == 0:
            print(f"No transitions found for {target_label}")
            # Save empty operations and transition_data (including initial BlendShape values)
            transition_operations.append({
                'operations': [],
                'transition_data': transition_data,
                'current_blendshape_values': initial_blendshape_values.copy(),
                'initial_vertices': initial_vertices.copy(),
                'current_vertices': initial_vertices.copy(),
                'mask_bones': target_transition_set.get("maskBones", [])
            })

    # Get the maximum number of operation
    max_operations = 0
    for item in transition_operations:
        max_operations = max(max_operations, len(item['operations']))

    # Execute in order of operation
    for operation_index in range(max_operations):
        print(f"Executing operation index {operation_index + 1}")

        for item in transition_operations:
            operations = item['operations']
            transition_data = item['transition_data']
            target_label = transition_data['target_label']
            target_shape_key_name = transition_data['target_shape_key_name']
            clothing_avatar_data = transition_data['clothing_avatar_data']
            base_avatar_data = transition_data['base_avatar_data']
            current_blendshape_values = item['current_blendshape_values']

            print(f"target_label: {target_label}")

            # Check if the current operation_index exists
            if operation_index >= len(operations):
                continue

            operation = operations[operation_index]
            changing_shape_key = operation.get('blend_shape', '')
            if not changing_shape_key:
                print(f"Warning: No target blend_shape found in operation for {operation_index}")
                continue

            # Retrieve the to_value from the operation and update the BlendShape value
            target_blendshape_values = current_blendshape_values.copy()
            if 'to_value' in operation:
                target_blendshape_values[changing_shape_key] = operation['to_value']
            else:
                print(f"Warning: No to_value found in operation for {changing_shape_key}")
                continue

            # Get Target BlendShape
            target_shape_key = None
            if target_obj.data.shape_keys and target_shape_key_name in target_obj.data.shape_keys.key_blocks:
                target_shape_key = target_obj.data.shape_keys.key_blocks.get(target_shape_key_name)

            if target_shape_key is None:
                print(f"Target shape key {target_shape_key_name} not found")
                continue

            operation_label = operation['blend_shape']
            blendshape_fields = base_avatar_data.get('blendShapeFields', [])
            mask_weights = None
            for blend_field in blendshape_fields:
                if blend_field['label'] == operation_label:
                    mask_bones = blend_field.get("maskBones", [])
                    if mask_bones:
                        print(f"mask_bones is found for {operation_label} : {mask_bones}")
                        mask_weights = create_blendshape_mask(target_obj, mask_bones, clothing_avatar_data, field_name=operation_label, store_debug_mask=False)
                        break
            # If mask_weights is None, set mask_weights to 1.0
            if mask_weights is None:
                print(f"mask_weights is None for {operation_label}")
                mask_weights = [1.0] * len(target_obj.data.vertices)

            if mask_weights is not None and np.all(mask_weights == 0):
                print(f"Skipping operation for {operation_label} - all mask weights are zero")
                item['current_blendshape_values'] = target_blendshape_values.copy()
                continue

            # Attempt to retrieve results from cache using linear interpolation
            interpolated_vertices = transition_cache.interpolate_result(target_blendshape_values, changing_shape_key, blendshape_groups)

            if interpolated_vertices is not None:
                print(f"Using cached interpolation for {changing_shape_key} = {target_blendshape_values[changing_shape_key]} (label: {transition_data['target_label']})")

                #Update the position of current_vertices
                current_vertices = item['current_vertices']
                for i in range(len(target_obj.data.vertices)):
                    current_vertices[i] = (1.0 - mask_weights[i]) * current_vertices[i] + mask_weights[i] * interpolated_vertices[i]

                item['current_blendshape_values'] = target_blendshape_values.copy()
                continue

            # If not in cache, actually execute the operation and store it in cache
            print(f"Executing and caching operation for {changing_shape_key} = {target_blendshape_values[changing_shape_key]} (label: {transition_data['target_label']})")

            current_vertices = item['current_vertices']

            # Create a temporary shape key from current_vertices
            temp_shape_key_name = f"{changing_shape_key}_transition_operation"
            temp_shape_key = None
            if temp_shape_key_name in target_obj.data.shape_keys.key_blocks:
                temp_shape_key = target_obj.data.shape_keys.key_blocks[temp_shape_key_name]
            else:
                temp_shape_key = target_obj.shape_key_add(name=temp_shape_key_name)
            for i in range(len(target_obj.data.vertices)):
                temp_shape_key.data[i].co = current_vertices[i]

            # Apply BlendShapeSettings
            # At this point, the position of temp_shape_key.data has been changed
            apply_blendshape_operation_with_shape_key_name(target_obj, operation, temp_shape_key_name, rigid_transformation)

            # Update the position of current_vertices
            for i in range(len(target_obj.data.vertices)):
                current_vertices[i] = (1.0 - mask_weights[i]) * current_vertices[i] + mask_weights[i] * temp_shape_key.data[i].co

            # Delete temporary shape keys
            target_obj.shape_key_remove(temp_shape_key)

            # Save the results to the cache
            transition_cache.store_result(target_blendshape_values, current_vertices, target_blendshape_values)

            item['current_blendshape_values'] = target_blendshape_values.copy()
            print(f"Updated BlendShape values for {transition_data['target_label']}: {changing_shape_key} = {target_blendshape_values[changing_shape_key]}")

    for target_shape_key_name, initial_vertices in initial_vertices_dict.items():
        target_shape_key = None
        if target_obj.data.shape_keys and target_shape_key_name in target_obj.data.shape_keys.key_blocks:
            target_shape_key = target_obj.data.shape_keys.key_blocks.get(target_shape_key_name)

        if target_shape_key is None:
            print(f"Initialize: Target shape key {target_shape_key_name} / {target_shape_key.name} not found")
            continue
        else:
            print(f"Initialize: Target shape key {target_shape_key_name} / {target_shape_key.name} found")

        for i in range(len(target_obj.data.vertices)):
            target_shape_key.data[i].co = initial_vertices[i]

    #Retrieve the last current_vertices from transition_operations and apply it to the shape key with the same name as target_label
    #At that time, apply the mask_weights from transition_set
    used_shape_key_names = set()
    created_shape_key_names = []
    created_shape_key_mask_weights = {}

    for item in transition_operations:
        target_shape_key_name = item['transition_data']['target_shape_key_name']
        clothing_avatar_data = item['transition_data']['clothing_avatar_data']
        target_label = item['transition_data']['target_label']

        # If target_label is Basis, use target_shape_key_name, otherwise use target_label
        shape_key_to_use = target_shape_key_name if target_label == 'Basis' else target_label
        shape_key_created = False

        # Acquire or create a shape key
        # If target_obj.data.shape_keys exists and shape_key_to_use exists in target_obj.data.shape_keys.key_blocks, then obtain target_shape_key
        # However, if the end contains {shape_key_to_use}_generated, that takes precedence
        target_shape_key = None
        generated_shape_key_name = f"{shape_key_to_use}_generated"
        if target_obj.data.shape_keys and generated_shape_key_name in target_obj.data.shape_keys.key_blocks:
            target_shape_key = target_obj.data.shape_keys.key_blocks.get(generated_shape_key_name)
            print(f"Generated target shape key {generated_shape_key_name} found")
        elif target_obj.data.shape_keys and shape_key_to_use in target_obj.data.shape_keys.key_blocks:
            target_shape_key = target_obj.data.shape_keys.key_blocks.get(shape_key_to_use)
            print(f"Target shape key {shape_key_to_use} found")
        else:
            # If no shape key exists, create a new one (do not create a Basis)
            if target_label != 'Basis':
                if not target_obj.data.shape_keys:
                    # If there is no Basis shape key, create one
                    target_obj.shape_key_add(name='Basis', from_mix=False)

                target_shape_key = target_obj.shape_key_add(name=shape_key_to_use, from_mix=False)
                print(f"Created new shape key: {shape_key_to_use}")
                created_shape_key_names.append(shape_key_to_use)
                shape_key_created = True
            else:
                print(f"Warning: Basis shape key {shape_key_to_use} not found")

        if target_shape_key is None:
            print(f"Failed to get or create shape key: {shape_key_to_use}")
            continue

        used_shape_key_names.add(target_shape_key.name)

        initial_vertices = item['initial_vertices']
        current_vertices = item['current_vertices']
        mask_bones = item['mask_bones']
        mask_weights = None
        if mask_bones:
            mask_weights = create_blendshape_mask(target_obj, mask_bones, clothing_avatar_data, field_name=target_label, store_debug_mask=False)
        if mask_weights is None:
            mask_weights = [1.0] * len(target_obj.data.vertices)

        if shape_key_created:
            created_shape_key_mask_weights[target_shape_key.name] = mask_weights

        for i in range(len(target_obj.data.vertices)):
            target_shape_key.data[i].co = mask_weights[i] * (current_vertices[i] - initial_vertices[i]) + target_shape_key.data[i].co

    print("Finished executing deferred transitions")
    print(f"Created shape keys: {created_shape_key_names}")

    return transition_operations, created_shape_key_mask_weights, used_shape_key_names

def subdivide_breast_faces(target_obj, clothing_avatar_data):
    # If subdivision is True, pre-subdivide the faces associated with the chest bone
    if clothing_avatar_data:
        breast_related_faces = set()

        # Get the bone names for LeftBreast and RightBreast
        breast_bone_names = []
        for bone_mapping in clothing_avatar_data.get("humanoidBones", []):
            if bone_mapping["humanoidBoneName"] in ["LeftBreast", "RightBreast"]:
                breast_bone_names.append(bone_mapping["boneName"])

        # Also acquire auxiliary bones
        for aux_bone_group in clothing_avatar_data.get("auxiliaryBones", []):
            if aux_bone_group["humanoidBoneName"] in ["LeftBreast", "RightBreast"]:
                breast_bone_names.extend(aux_bone_group["auxiliaryBones"])

        # Identify vertices related to the chest bone
        breast_vertices = set()
        for bone_name in breast_bone_names:
            if bone_name in target_obj.vertex_groups:
                vertex_group = target_obj.vertex_groups[bone_name]
                for vertex in target_obj.data.vertices:
                    for group in vertex.groups:
                        if group.group == vertex_group.index and group.weight > 0.001:
                            breast_vertices.add(vertex.index)

        # Identify the plane containing the apex of the chest
        if breast_vertices:
            for face in target_obj.data.polygons:
                if any(vertex_idx in breast_vertices for vertex_idx in face.vertices):
                    breast_related_faces.add(face.index)

            if breast_related_faces:
                print(f"Subdividing {len(breast_related_faces)} breast-related faces...")
                subdivide_faces(target_obj, list(breast_related_faces), cuts=1)

def apply_symmetric_field_delta(target_obj, field_data_path, blend_shape_labels=None, clothing_avatar_data=None, base_avatar_data=None, subdivision=True, shape_key_name="SymmetricDeformed", skip_blend_shape_generation=False, config_data=None, ignore_blendshape=None):
    """
    Load saved symmetric Deformation Field difference data and apply it to the mesh (optimized version, multi-stage compatible).
    ※The ratio of intersecting surfaces when applying the Deformation Field for BlendShape first versus applying only the main field
      It performs processing that ignores BlendShape displacement under specified conditions.
    """

    # Initialize the Transition cache
    transition_cache = TransitionCache()
    deferred_transitions = []  # List of Transition to execute with delay

    MAX_ITERATIONS = 0  # Maximum number of repetitions

    # Main processing loop (conventional single-step processing)
    iteration = 0
    shape_key = None
    basis_field_path = os.path.join(os.path.dirname(field_data_path), field_data_path)
    while iteration <= MAX_ITERATIONS:

        original_shape_key_state = save_shape_key_state(target_obj)

        print(f"selected field_data_path: {basis_field_path}")

        # Create a shape key and apply the transformation
        if shape_key:
            target_obj.shape_key_remove(shape_key)
        shape_key = process_field_deformation(target_obj, basis_field_path, blend_shape_labels, clothing_avatar_data, shape_key_name, ignore_blendshape)

        restore_shape_key_state(target_obj, original_shape_key_state)

        # Add the Basis transition to the delayed execution list
        if config_data:
            deferred_transitions.append({
                'target_obj': target_obj,
                'config_data': config_data,
                'target_label': 'Basis',
                'target_shape_key_name': shape_key_name,
                'base_avatar_data': base_avatar_data,
                'clothing_avatar_data': clothing_avatar_data,
                'save_original_shape_key': False
            })

        # Detect new intersections
        intersections = find_intersecting_faces_bvh(target_obj)
        print(f"Iteration {iteration + 1}: Intersecting faces: {len(intersections)}")

        if not subdivision:
            print("Subdivision skipped")
            break

        if not intersections:
            print("No intersections detected")
            break

        if iteration == MAX_ITERATIONS:
            print("Maximum iterations reached")
            break
        # When a new intersection is detected, subdivide those faces
        # subdivide_faces(target_obj, intersections)

        iteration += 1

    # Create a label set for processing the blendShapeFields in the config file
    config_blend_shape_labels = set()
    config_generated_shape_keys = {}  # Save the shape key name to exclude from subsequent processing
    additional_shape_keys = set() # Save the shape key name to be processed additionally
    non_relative_shape_keys = set() # Save shape key names without relative displacement

    skipped_shape_keys = set()
    label_to_target_shape_key_name = {'Basis': shape_key_name}

    # 1. Process the blendShapeFields in the config file first
    if config_data and "blendShapeFields" in config_data:
        print("Processing config blendShapeFields...")

        for blend_field in config_data["blendShapeFields"]:
            label = blend_field["label"]
            source_label = blend_field["sourceLabel"]
            field_path = os.path.join(os.path.dirname(field_data_path), blend_field["path"])

            print(f"selected field_path: {field_path}")
            source_blend_shape_settings = blend_field.get("sourceBlendShapeSettings", [])

            if (blend_shape_labels is None or source_label not in blend_shape_labels) and source_label not in target_obj.data.shape_keys.key_blocks:
                print(f"Skipping {label} - source label {source_label} not in shape keys")
                skipped_shape_keys.add(label)
                continue

            # Get mask weight
            mask_bones = blend_field.get("maskBones", [])
            mask_weights = None
            if mask_bones:
                mask_weights = create_blendshape_mask(target_obj, mask_bones, clothing_avatar_data, field_name=label, store_debug_mask=True)

            if mask_weights is not None and np.all(mask_weights == 0):
                print(f"Skipping {label} - all mask weights are zero")
                continue

            # Save the original shape key settings for the target mesh object
            original_shape_key_state = save_shape_key_state(target_obj)

            # Set all shape key values to 0
            if target_obj.data.shape_keys:
                for key_block in target_obj.data.shape_keys.key_blocks:
                    key_block.value = 0.0

            # Set the target shape key value to 1 in the first Config Pair (assuming 1 is the baseline) or set the shape key value after the Transition in the previous Config Pair to 1
            if clothing_avatar_data["name"] == "Template":
                if target_obj.data.shape_keys:
                    if source_label in target_obj.data.shape_keys.key_blocks:
                        source_shape_key = target_obj.data.shape_keys.key_blocks.get(source_label)
                        source_shape_key.value = 1.0
                        print(f"source_label: {source_label} is found in shape keys")
                    else:
                        temp_shape_key_name = f"{source_label}_temp"
                        if temp_shape_key_name in target_obj.data.shape_keys.key_blocks:
                            target_obj.data.shape_keys.key_blocks[temp_shape_key_name].value = 1.0
                            print(f"temp_shape_key_name: {temp_shape_key_name} is found in shape keys")
            else:
                # Apply source_blend_shape_settings
                for source_blend_shape_setting in source_blend_shape_settings:
                    source_blend_shape_name = source_blend_shape_setting.get("name", "")
                    source_blend_shape_value = source_blend_shape_setting.get("value", 0.0)
                    if source_blend_shape_name in target_obj.data.shape_keys.key_blocks:
                        source_blend_shape_key = target_obj.data.shape_keys.key_blocks.get(source_blend_shape_name)
                        source_blend_shape_key.value = source_blend_shape_value
                        print(f"source_blend_shape_name: {source_blend_shape_name} is found in shape keys")
                    else:
                        temp_blend_shape_key_name = f"{source_blend_shape_name}_temp"
                        if temp_blend_shape_key_name in target_obj.data.shape_keys.key_blocks:
                            target_obj.data.shape_keys.key_blocks[temp_blend_shape_key_name].value = source_blend_shape_value
                            print(f"temp_blend_shape_key_name: {temp_blend_shape_key_name} is found in shape keys")

            # Set the blend_shape_key_name (if a shape key with the same name exists, append _generated)
            blend_shape_key_name = label
            if target_obj.data.shape_keys and label in target_obj.data.shape_keys.key_blocks:
                blend_shape_key_name = f"{label}_generated"

            # Execute process_field_deformation
            if os.path.exists(field_path):
                print(f"Processing config blend shape field: {label} -> {blend_shape_key_name}")
                generated_shape_key = process_field_deformation(target_obj, field_path, blend_shape_labels, clothing_avatar_data, blend_shape_key_name, ignore_blendshape)

                # Add the transition for the corresponding label to the delayed execution list
                if config_data and generated_shape_key:
                    deferred_transitions.append({
                        'target_obj': target_obj,
                        'config_data': config_data,
                        'target_label': label,
                        'target_shape_key_name': generated_shape_key.name,
                        'base_avatar_data': base_avatar_data,
                        'clothing_avatar_data': clothing_avatar_data,
                        'save_original_shape_key': False
                    })

                # Set the generated shape key value to 0
                if generated_shape_key:
                    generated_shape_key.value = 0.0
                    config_generated_shape_keys[generated_shape_key.name] = mask_weights
                    non_relative_shape_keys.add(generated_shape_key.name)

                config_blend_shape_labels.add(label)

                label_to_target_shape_key_name[label] = generated_shape_key.name
            else:
                print(f"Warning: Config blend shape field file not found: {field_path}")

            # Restore original shape key settings
            restore_shape_key_state(target_obj, original_shape_key_state)

    # Processing for shape keys included in transition_sets but not in config_blend_shape_labels
    if config_data and config_data.get('blend_shape_transition_sets', []):
        transition_sets = config_data.get('blend_shape_transition_sets', [])
        print("Processing skipped config blendShapeFields...")

        for transition_set in transition_sets:
            label = transition_set["label"]
            if label in config_blend_shape_labels or label == 'Basis':
                continue

            source_label = get_source_label(label, config_data)
            if source_label not in label_to_target_shape_key_name:
                print(f"Skipping {label} - source label {source_label} not in label_to_target_shape_key_name")
                continue

            print(f"Processing skipped config blendShapeField: {label}")

            # Get mask weight
            mask_bones = transition_set.get("mask_bones", [])
            print(f"mask_bones: {mask_bones}")
            mask_weights = None
            if mask_bones:
                mask_weights = create_blendshape_mask(target_obj, mask_bones, clothing_avatar_data, field_name=label, store_debug_mask=True)

            if mask_weights is not None and np.all(mask_weights == 0):
                print(f"Skipping {label} - all mask weights are zero")
                continue

            target_shape_key_name = label_to_target_shape_key_name[source_label]
            target_shape_key = target_obj.data.shape_keys.key_blocks.get(target_shape_key_name)

            if not target_shape_key:
                print(f"Skipping {label} - target shape key {target_shape_key_name} not found")
                continue

            # Create a copy of the shape key specified by target_shape_key_name
            blend_shape_key_name = label
            if target_obj.data.shape_keys and label in target_obj.data.shape_keys.key_blocks:
                blend_shape_key_name = f"{label}_generated"

            skipped_blend_shape_key = target_obj.shape_key_add(name=blend_shape_key_name)

            for i in range(len(skipped_blend_shape_key.data)):
                skipped_blend_shape_key.data[i].co = target_shape_key.data[i].co.copy()

            print(f"skipped_blend_shape_key: {skipped_blend_shape_key.name}")

            if config_data and skipped_blend_shape_key:
                deferred_transitions.append({
                    'target_obj': target_obj,
                    'config_data': config_data,
                    'target_label': label,
                    'target_shape_key_name': skipped_blend_shape_key.name,
                    'base_avatar_data': base_avatar_data,
                    'clothing_avatar_data': clothing_avatar_data,
                    'save_original_shape_key': False
                })

                print(f"Added deferred transition: {label} -> {skipped_blend_shape_key.name}")

                config_generated_shape_keys[skipped_blend_shape_key.name] = mask_weights
                non_relative_shape_keys.add(skipped_blend_shape_key.name)
                config_blend_shape_labels.add(label)
                label_to_target_shape_key_name[label] = skipped_blend_shape_key.name


    # 2. Processing for shape keys not included in the blendshapes of clothing_avatar_data
    if target_obj.data.shape_keys:
        # Create a list of blend shapes from clothing_avatar_data
        clothing_blendshapes = set()
        if clothing_avatar_data and "blendshapes" in clothing_avatar_data:
            for blendshape in clothing_avatar_data["blendshapes"]:
                clothing_blendshapes.add(blendshape["name"])

        # Processing for each shape key
        for key_block in target_obj.data.shape_keys.key_blocks:
            if (key_block.name == "Basis" or
                key_block.name in clothing_blendshapes or
                key_block == shape_key or
                key_block.name.endswith("_BaseShape") or
                key_block.name in config_generated_shape_keys.keys() or
                key_block.name in config_blend_shape_labels or
                key_block.name.endswith("_original") or
                key_block.name.endswith("_generated") or
                key_block.name.endswith("_temp")):
                continue  # Skip those contained in the blendshapes of Basis or clothing_avatar_data, those ending with _BaseShape, or those generated by config

            print(f"Processing additional shape key: {key_block.name}")

            original_shape_key_state = save_shape_key_state(target_obj)

            # Set all shape key values to 0
            for sk in target_obj.data.shape_keys.key_blocks:
                sk.value = 0.0

            basis_field_path2 = os.path.join(os.path.dirname(field_data_path), field_data_path)
            source_label = get_source_label('Basis', config_data)
            if source_label is not None and source_label != 'Basis' and target_obj.data.shape_keys:
                source_field_path = None
                source_shape_name = None
                if config_data and "blendShapeFields" in config_data:
                    for blend_field in config_data["blendShapeFields"]:
                        if blend_field["label"] == source_label:
                            source_field_path = os.path.join(os.path.dirname(field_data_path), blend_field["path"])
                            source_shape_name = blend_field["sourceLabel"]
                            break
                if source_field_path is not None and source_shape_name is not None:
                    if source_shape_name in target_obj.data.shape_keys.key_blocks:
                        source_shape_key = target_obj.data.shape_keys.key_blocks.get(source_shape_name)
                        source_shape_key.value = 1.0
                        basis_field_path2 = source_field_path
                        print(f"source_label: {source_shape_name} is found in shape keys")
                    else:
                        temp_shape_key_name = f"{source_shape_name}_temp"
                        if temp_shape_key_name in target_obj.data.shape_keys.key_blocks:
                            target_obj.data.shape_keys.key_blocks[temp_shape_key_name].value = 1.0
                            basis_field_path2 = source_field_path
                            print(f"temp_shape_key_name: {temp_shape_key_name} is found in shape keys")

            print(f"basis_field_path2: {basis_field_path2}")

            # Set the value of the target shape key to 1
            key_block.value = 1.0

            temp_blend_shape_key_name = f"{key_block.name}_generated"

            temp_shape_key = process_field_deformation(target_obj, basis_field_path2, blend_shape_labels, clothing_avatar_data, temp_blend_shape_key_name, ignore_blendshape)

            additional_shape_keys.add(temp_shape_key.name)
            non_relative_shape_keys.add(temp_shape_key.name)

            # Reset the shape key value
            key_block.value = 0.0

            restore_shape_key_state(target_obj, original_shape_key_state)

    # Execute delayed Transition with the cache system
    non_transitioned_shape_vertices = None
    created_shape_key_mask_weights = {}
    shape_keys_to_remove = []
    if deferred_transitions:
        transition_operations, created_shape_key_mask_weights, used_shape_key_names = execute_transitions_with_cache(deferred_transitions, transition_cache, target_obj)
        for transition_operation in transition_operations:
            if transition_operation['transition_data']['target_label'] == 'Basis':
                non_transitioned_shape_vertices = [Vector(v) for v in transition_operation['initial_vertices']]
                break
        if used_shape_key_names:
            for config_shape_key_name in config_generated_shape_keys:
                if config_shape_key_name not in used_shape_key_names and config_shape_key_name in target_obj.data.shape_keys.key_blocks:
                    shape_keys_to_remove.append(config_shape_key_name)

    for created_shape_key_name, mask_weights in created_shape_key_mask_weights.items():
        if created_shape_key_name in target_obj.data.shape_keys.key_blocks:
            config_generated_shape_keys[created_shape_key_name] = mask_weights
            non_relative_shape_keys.add(created_shape_key_name)
            config_blend_shape_labels.add(created_shape_key_name)
            label_to_target_shape_key_name[created_shape_key_name] = created_shape_key_name
            print(f"Added created shape key: {created_shape_key_name}")

    shape_key.value = 1.0

    # Preparation before processing the blendShapeFields of base_avatar_data
    basis_name = 'Basis'
    basis_index = target_obj.data.shape_keys.key_blocks.find(basis_name)

    bpy.ops.object.mode_set(mode='OBJECT')
    bpy.ops.object.select_all(action='DESELECT')
    bpy.context.view_layer.objects.active = target_obj
    target_obj.select_set(True)

    if non_transitioned_shape_vertices:
        for additionalshape_key_name in additional_shape_keys:
            if additionalshape_key_name in target_obj.data.shape_keys.key_blocks:
                additional_shape_key = target_obj.data.shape_keys.key_blocks.get(additionalshape_key_name)
                # Add the difference between the shape_key and the pre-transition Basis shape to each vertex of the additional_shape_key
                for i, vert in enumerate(additional_shape_key.data):
                    # Calculate the difference between the shape_key and Basis
                    shape_diff = shape_key.data[i].co - non_transitioned_shape_vertices[i]
                    # Add the difference to the vertex coordinates of additional_shape_key
                    additional_shape_key.data[i].co += shape_diff

            else:
                print(f"Warning: {additionalshape_key_name} is not found in shape keys")

    bpy.ops.object.mode_set(mode='EDIT')
    bpy.ops.mesh.select_all(action='SELECT')

    print(f"Shape keys in {target_obj.name}:")
    for key_block in target_obj.data.shape_keys.key_blocks:
        print(f"- {key_block.name} (value: {key_block.value})")

    original_shape_key_name = f"{shape_key_name}_original"
    for sk in target_obj.data.shape_keys.key_blocks:
        if sk.name in non_relative_shape_keys and sk.name != basis_name:
            if shape_key_name in target_obj.data.shape_keys.key_blocks:
                target_obj.active_shape_key_index = target_obj.data.shape_keys.key_blocks.find(sk.name)
                bpy.ops.mesh.blend_from_shape(shape=shape_key_name, blend=-1, add=True)
            else:
                print(f"Warning: {shape_key_name} or {shape_key_name}_original is not found in shape keys")

    bpy.context.object.active_shape_key_index = basis_index
    bpy.ops.mesh.blend_from_shape(shape=shape_key_name, blend=1, add=True)

    bpy.ops.object.mode_set(mode='OBJECT')

    if original_shape_key_name in target_obj.data.shape_keys.key_blocks:
        original_shape_key = target_obj.data.shape_keys.key_blocks.get(original_shape_key_name)
        target_obj.shape_key_remove(original_shape_key)
        print(f"Removed shape key: {original_shape_key_name} from {target_obj.name}")

    # # Delete unnecessary shape keys
    if shape_key:
       target_obj.shape_key_remove(shape_key)

    for unused_shape_key_name in shape_keys_to_remove:
        if unused_shape_key_name in target_obj.data.shape_keys.key_blocks:
            unused_shape_key = target_obj.data.shape_keys.key_blocks.get(unused_shape_key_name)
            if unused_shape_key:
                target_obj.shape_key_remove(unused_shape_key)
                print(f"Removed shape key: {unused_shape_key_name} from {target_obj.name}")
            else:
                print(f"Warning: {unused_shape_key_name} is not found in shape keys")
        else:
            print(f"Warning: {unused_shape_key_name} is not found in shape keys")

    # Apply mask_weights to the displacement of shape keys generated by the blendShapeFields in the config file
    if config_generated_shape_keys:
        print(f"Applying mask weights to generated shape keys: {list(config_generated_shape_keys.keys())}")

        # Get the vertex positions of the base shape
        basis_shape_key = target_obj.data.shape_keys.key_blocks.get(basis_name)
        if basis_shape_key:
            basis_positions = np.array([v.co for v in basis_shape_key.data])

            # Apply a mask to each generated shape key
            for shape_key_name_to_mask, mask_weights in config_generated_shape_keys.items():
                if shape_key_name_to_mask == basis_name:
                    continue

                shape_key_to_mask = target_obj.data.shape_keys.key_blocks.get(shape_key_name_to_mask)
                if shape_key_to_mask:
                    # Get the vertex positions of the current shape key
                    shape_positions = np.array([v.co for v in shape_key_to_mask.data])

                    # Calculate displacement
                    displacement = shape_positions - basis_positions

                    # Apply mask (multiply displacement by mask_weights)
                    if mask_weights is not None:
                        masked_displacement = displacement * mask_weights[:, np.newaxis]
                    else:
                        masked_displacement = displacement

                    # Calculate the position after applying the mask
                    new_positions = basis_positions + masked_displacement

                    # Update shape key vertex positions
                    for i, vertex in enumerate(shape_key_to_mask.data):
                        vertex.co = new_positions[i]

                    print(f"Applied mask weights to shape key: {shape_key_name_to_mask}")

    # 4. Process the blendShapeFields in base_avatar_data (skip those matching the config label)
    if base_avatar_data and "blendShapeFields" in base_avatar_data and not skip_blend_shape_generation:
        # Acquisition of the Armature
        armature_obj = get_armature_from_modifier(target_obj)
        if not armature_obj:
            raise ValueError("Armature modifier not found")

        # Save the original shape key settings for the target mesh object
        original_shape_key_state = save_shape_key_state(target_obj)

        # Set all shape key values to 0
        if target_obj.data.shape_keys:
            for key_block in target_obj.data.shape_keys.key_blocks:
                key_block.value = 0.0

        # Get the vertex positions of the evaluated mesh (after applying Shape Key A)
        depsgraph = bpy.context.evaluated_depsgraph_get()
        depsgraph.update()
        eval_obj = target_obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data
        vertices = np.array([v.co for v in target_obj.data.vertices])  # Original vertex array
        deformed_vertices = np.array([v.co for v in eval_mesh.vertices])

        # Process each blendShapeField
        for blend_field in base_avatar_data["blendShapeFields"]:
            label = blend_field["label"]

            # If it matches the label in the blendShapeFields section of the config file, skip it
            if label in config_blend_shape_labels:
                print(f"Skipping base avatar blend shape field '{label}' (already processed from config)")
                continue

            field_path = os.path.join(os.path.dirname(field_data_path), blend_field["filePath"])

            if os.path.exists(field_path):
                print(f"Applying blend shape field for {label}")
                # Reading Field Data
                field_info_blend = get_deformation_field_multi_step(field_path)
                blend_points = field_info_blend['all_field_points']
                blend_deltas = field_info_blend['all_delta_positions']
                blend_field_weights = field_info_blend['field_weights']
                blend_matrix = field_info_blend['world_matrix']
                blend_matrix_inv = field_info_blend['world_matrix_inv']
                blend_k_neighbors = field_info_blend['kdtree_query_k']

                # Get mask weight
                mask_weights = None
                if "maskBones" in blend_field:
                    mask_weights = create_blendshape_mask(target_obj, blend_field["maskBones"], clothing_avatar_data, field_name=label, store_debug_mask=True)

                # Calculate the position after deformation
                deformed_positions = batch_process_vertices_multi_step(
                    deformed_vertices,
                    blend_points,
                    blend_deltas,
                    blend_field_weights,
                    blend_matrix,
                    blend_matrix_inv,
                    target_obj.matrix_world,
                    target_obj.matrix_world.inverted(),
                    mask_weights,
                    batch_size=1000,
                    k=blend_k_neighbors
                )

                # Check whether the displacement is zero in world coordinates
                has_displacement = False
                for i in range(len(deformed_vertices)):
                    displacement = deformed_positions[i] - (target_obj.matrix_world @ Vector(deformed_vertices[i]))
                    if np.any(np.abs(displacement) > 1e-5):  # Neglect minor displacements
                        print(f"blendShapeFields {label} world_displacement: {displacement}")
                        has_displacement = True
                        break

                # Create shape keys only when displacement exists
                if has_displacement:

                    blend_shape_key_name = label
                    if target_obj.data.shape_keys and label in target_obj.data.shape_keys.key_blocks:
                        blend_shape_key_name = f"{label}_generated"

                    # Create a shape key
                    shape_key_b = target_obj.shape_key_add(name=blend_shape_key_name)
                    shape_key_b.value = 0.0  # The initial value is 0

                    # Save vertex positions to shape keys
                    matrix_armature_inv_fallback = Matrix.Identity(4)
                    for i in range(len(vertices)):
                        matrix_armature_inv = calculate_inverse_pose_matrix(target_obj, armature_obj, i)
                        if matrix_armature_inv is None:
                            matrix_armature_inv = matrix_armature_inv_fallback
                        # Convert the transformed position to local coordinates
                        deformed_world_pos = matrix_armature_inv @ Vector(deformed_positions[i])
                        deformed_local_pos = target_obj.matrix_world.inverted() @ deformed_world_pos
                        shape_key_b.data[i].co = deformed_local_pos
                        matrix_armature_inv_fallback = matrix_armature_inv
                else:
                    print(f"Skipping creation of shape key '{label}' as it has no displacement")

            else:
                print(f"Warning: Field file not found for blend shape {label}")
        # Restore original shape key settings
        restore_shape_key_state(target_obj, original_shape_key_state)

    # Reset all shape key values
    for sk in target_obj.data.shape_keys.key_blocks:
        sk.value = 0.0

def apply_field_delta_with_rigid_transform_single(obj, field_data_path, blend_shape_labels=None, clothing_avatar_data=None, shape_key_name="RigidTransformed"):
    used_shape_keys = []
    if blend_shape_labels and clothing_avatar_data:
        # Retrieve vertex positions from pre-created shape keys
        for label in blend_shape_labels:
            # If a clothing model has a shape key with the same name, do not apply it
            if obj.data.shape_keys and label in obj.data.shape_keys.key_blocks:
                print(f"Skipping {label} - already has shape key")
                continue
            target_avatar_base_shape_key_name = f"{label}_BaseShape"
            if obj.data.shape_keys and target_avatar_base_shape_key_name in obj.data.shape_keys.key_blocks:
                target_avatar_base_shape_key = obj.data.shape_keys.key_blocks[target_avatar_base_shape_key_name]
                target_avatar_base_shape_key.value = 1.0
                print(f"Using shape key {target_avatar_base_shape_key_name} for BlendShape deformation")
                used_shape_keys.append(target_avatar_base_shape_key_name)
            else:
                print(f"Warning: Shape key {target_avatar_base_shape_key_name} not found")

    # Retrieve vertex positions (original state) from the evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data
    original_positions = np.array([v.co for v in eval_mesh.vertices])
    current_positions = original_positions.copy()

    # Apply the main Deformation Field
    field_info = get_deformation_field_multi_step(field_data_path)
    field_points = field_info['all_field_points']
    delta_positions = field_info['all_delta_positions']
    field_weights = field_info['field_weights']
    field_matrix = field_info['world_matrix']
    field_matrix_inv = field_info['world_matrix_inv']
    k_neighbors = field_info['kdtree_query_k']

    # Calculate deformation positions based on the Deformation Field
    deformed_positions = batch_process_vertices_multi_step(
        current_positions,
        field_points,
        delta_positions,
        field_weights,
        field_matrix,
        field_matrix_inv,
        obj.matrix_world,
        obj.matrix_world.inverted(),
        None,
        batch_size=1000,
        k=k_neighbors
    )

    # Convert to a numpy array
    source_points = np.array([obj.matrix_world @ Vector(v) for v in current_positions])
    target_points = np.array(deformed_positions)

    # # DistanceWeight Get influence from vertex group
    #influence_factors = get_distance_weight_influence_factors(obj, 0.5)
    #s, R, t = calculate_optimal_similarity_transform_weighted(source_points, target_points, influence_factors)

    s, R, t = calculate_optimal_similarity_transform(source_points, target_points)

    # Calculate the result after applying the similarity transformation
    similarity_transformed = apply_similarity_transform_to_points(source_points, s, R, t)

    for label in used_shape_keys:
        obj.data.shape_keys.key_blocks[label].value = 0.0

    # Create a shape key
    if obj.data.shape_keys is None:
        obj.shape_key_add(name='Basis')
    if obj.data.shape_keys and shape_key_name in obj.data.shape_keys.key_blocks:
        shape_key = obj.data.shape_keys.key_blocks[shape_key_name]
    else:
        shape_key = obj.shape_key_add(name=shape_key_name)
    shape_key.value = 1.0

    # Acquire the armature
    armature_obj = get_armature_from_modifier(obj)
    if not armature_obj:
        raise ValueError("Armature modifier not found")

    # Set vertex positions for shape keys
    matrix_armature_inv_fallback = Matrix.Identity(4)
    for i in range(len(current_positions)):
        matrix_armature_inv = calculate_inverse_pose_matrix(obj, armature_obj, i)
        if matrix_armature_inv is None:
            matrix_armature_inv = matrix_armature_inv_fallback
        undeformed_world_pos = matrix_armature_inv @ Vector(similarity_transformed[i])
        local_pos = obj.matrix_world.inverted() @ undeformed_world_pos
        shape_key.data[i].co = local_pos
        matrix_armature_inv_fallback = matrix_armature_inv
    return shape_key


def apply_field_delta_with_rigid_transform(obj, field_data_path, blend_shape_labels=None, base_avatar_data=None, clothing_avatar_data=None, shape_key_name="RigidTransformed", influence_range=1.0, config_data=None, overwrite_base_shape_key=True):
    """
    Load saved symmetric Deformation Field difference data and apply it as the optimal rigid body transformation (multi-stage compatible)

    Parameters:
        obj: Target mesh object
        field_data_path: Deformation Field path
        blend_shape_labels: List of blend shape labels to apply (optional)
        base_avatar_data: Base avatar data (optional)
        clothing_avatar_data: Clothing avatar data (Optional)
        shape_key_name: The name of the shape key to be created
        influence_range: DistanceWeight influence range by vertex group (0.0-1.0, default 0.5)

    Returns:
        Shape Key
    """
    # Initialize the Transition cache
    transition_cache = TransitionCache()
    deferred_transitions = []  # List of Transition to execute with delay

    original_shape_key_state = save_shape_key_state(obj)

    if obj.data.shape_keys:
        for sk in obj.data.shape_keys.key_blocks:
            sk.value = 0.0

    basis_field_path = os.path.join(os.path.dirname(field_data_path), field_data_path)
    print(f"selected field_data_path: {basis_field_path}")

    shape_key = apply_field_delta_with_rigid_transform_single(obj, basis_field_path, blend_shape_labels, clothing_avatar_data, shape_key_name)

    # Add the Basis transition to the delayed execution list
    if config_data:
        deferred_transitions.append({
            'target_obj': obj,
            'config_data': config_data,
            'target_label': 'Basis',
            'target_shape_key_name': shape_key_name,
            'base_avatar_data': base_avatar_data,
            'clothing_avatar_data': clothing_avatar_data,
            'base_avatar_data': base_avatar_data,
            'save_original_shape_key': True
        })

    restore_shape_key_state(obj, original_shape_key_state)

    # Create a label set for processing the blendShapeFields in the config file
    config_blend_shape_labels = set()
    config_generated_shape_keys = {}  # Save the shape key name to exclude from subsequent processing
    non_relative_shape_keys = set() # Save shape key names without relative displacement

    skipped_shape_keys = set()
    label_to_target_shape_key_name = {'Basis': shape_key_name}

    # 1. Process the blendShapeFields in the config file first
    if config_data and "blendShapeFields" in config_data:
        print("Processing config blendShapeFields (rigid transform)...")

        for blend_field in config_data["blendShapeFields"]:
            label = blend_field["label"]
            source_label = blend_field["sourceLabel"]
            field_path = os.path.join(os.path.dirname(field_data_path), blend_field["path"])

            print(f"selected field_path: {field_path}")
            source_blend_shape_settings = blend_field.get("sourceBlendShapeSettings", [])

            if (blend_shape_labels is None or source_label not in blend_shape_labels) and source_label not in obj.data.shape_keys.key_blocks:
                print(f"Skipping {label} - source label {source_label} not in shape keys")
                skipped_shape_keys.add(label)
                continue

            # Get mask weight
            mask_bones = blend_field.get("maskBones", [])
            mask_weights = None
            if mask_bones:
                mask_weights = create_blendshape_mask(obj, mask_bones, clothing_avatar_data, field_name=label, store_debug_mask=True)

            if mask_weights is not None and np.all(mask_weights == 0):
                print(f"Skipping {label} - all mask weights are zero")
                continue

            # Save the original shape key settings for the target mesh object
            original_shape_key_state = save_shape_key_state(obj)

            # Set all shape key values to 0
            if obj.data.shape_keys:
                for key_block in obj.data.shape_keys.key_blocks:
                    key_block.value = 0.0

            # Set the target shape key value to 1 in the first Config Pair (assuming 1 is the baseline) or set the shape key value after the Transition in the previous Config Pair to 1
            if clothing_avatar_data["name"] == "Template":
                if obj.data.shape_keys:
                    if source_label in obj.data.shape_keys.key_blocks:
                        source_shape_key = obj.data.shape_keys.key_blocks.get(source_label)
                        source_shape_key.value = 1.0
                        print(f"source_label: {source_label} is found in shape keys")
                    else:
                        temp_shape_key_name = f"{source_label}_temp"
                        if temp_shape_key_name in obj.data.shape_keys.key_blocks:
                            obj.data.shape_keys.key_blocks[temp_shape_key_name].value = 1.0
                            print(f"temp_shape_key_name: {temp_shape_key_name} is found in shape keys")
            else:
                # Apply source_blend_shape_settings
                for source_blend_shape_setting in source_blend_shape_settings:
                    source_blend_shape_name = source_blend_shape_setting.get("name", "")
                    source_blend_shape_value = source_blend_shape_setting.get("value", 0.0)
                    if source_blend_shape_name in obj.data.shape_keys.key_blocks:
                        source_blend_shape_key = obj.data.shape_keys.key_blocks.get(source_blend_shape_name)
                        source_blend_shape_key.value = source_blend_shape_value
                        print(f"source_blend_shape_name: {source_blend_shape_name} is found in shape keys")
                    else:
                        temp_blend_shape_key_name = f"{source_blend_shape_name}_temp"
                        if temp_blend_shape_key_name in obj.data.shape_keys.key_blocks:
                            obj.data.shape_keys.key_blocks[temp_blend_shape_key_name].value = source_blend_shape_value
                            print(f"temp_blend_shape_key_name: {temp_blend_shape_key_name} is found in shape keys")

            # Set the blend_shape_key_name (if a shape key with the same name exists, append _generated)
            blend_shape_key_name = label
            if obj.data.shape_keys and label in obj.data.shape_keys.key_blocks:
                blend_shape_key_name = f"{label}_generated"

            if os.path.exists(field_path):
                print(f"Processing config blend shape field with rigid transform: {label} -> {blend_shape_key_name}")
                generated_shape_key = apply_field_delta_with_rigid_transform_single(obj, field_path, blend_shape_labels, clothing_avatar_data, blend_shape_key_name)

                # Add the transition for the corresponding label to the delayed execution list
                if config_data and generated_shape_key:
                    deferred_transitions.append({
                        'target_obj': obj,
                        'config_data': config_data,
                        'target_label': label,
                        'target_shape_key_name': generated_shape_key.name,
                        'base_avatar_data': base_avatar_data,
                        'clothing_avatar_data': clothing_avatar_data,
                        'base_avatar_data': base_avatar_data,
                        'save_original_shape_key': False
                    })

                # Set the generated shape key value to 0
                if generated_shape_key:
                    generated_shape_key.value = 0.0
                    config_generated_shape_keys[generated_shape_key.name] = mask_weights
                    non_relative_shape_keys.add(generated_shape_key.name)

                config_blend_shape_labels.add(label)

                label_to_target_shape_key_name[label] = generated_shape_key.name
            else:
                print(f"Warning: Config blend shape field file not found: {field_path}")

            # Restore original shape key settings
            restore_shape_key_state(obj, original_shape_key_state)

    # Processing for shape keys included in transition_sets but not in config_blend_shape_labels
    if config_data and config_data.get('blend_shape_transition_sets', []):
        print("Processing skipped config blendShapeFields...")

        transition_sets = config_data.get('blend_shape_transition_sets', [])
        for transition_set in transition_sets:
            label = transition_set["label"]
            if label in config_blend_shape_labels or label == 'Basis':
                continue

            source_label = get_source_label(label, config_data)
            if source_label not in label_to_target_shape_key_name:
                print(f"Skipping {label} - source label {source_label} not in label_to_target_shape_key_name")
                continue

            print(f"Processing skipped config blendShapeField: {label}")

            # Get mask weight
            mask_bones = transition_set.get("mask_bones", [])
            print(f"mask_bones: {mask_bones}")
            mask_weights = None
            if mask_bones:
                mask_weights = create_blendshape_mask(obj, mask_bones, clothing_avatar_data, field_name=label, store_debug_mask=True)

            if mask_weights is not None and np.all(mask_weights == 0):
                print(f"Skipping {label} - all mask weights are zero")
                continue

            target_shape_key_name = label_to_target_shape_key_name[source_label]
            target_shape_key = obj.data.shape_keys.key_blocks.get(target_shape_key_name)

            if not target_shape_key:
                print(f"Skipping {label} - target shape key {target_shape_key_name} not found")
                continue

            # Create a copy of the shape key specified by target_shape_key_name
            blend_shape_key_name = label
            if obj.data.shape_keys and label in obj.data.shape_keys.key_blocks:
                blend_shape_key_name = f"{label}_generated"

            skipped_blend_shape_key = obj.shape_key_add(name=blend_shape_key_name)

            for i in range(len(skipped_blend_shape_key.data)):
                skipped_blend_shape_key.data[i].co = target_shape_key.data[i].co.copy()

            print(f"skipped_blend_shape_key: {skipped_blend_shape_key.name}")

            if config_data and skipped_blend_shape_key:
                deferred_transitions.append({
                    'target_obj': obj,
                    'config_data': config_data,
                    'target_label': label,
                    'target_shape_key_name': skipped_blend_shape_key.name,
                    'base_avatar_data': base_avatar_data,
                    'clothing_avatar_data': clothing_avatar_data,
                    'save_original_shape_key': False
                })

                print(f"Added deferred transition: {label} -> {skipped_blend_shape_key.name}")

                config_generated_shape_keys[skipped_blend_shape_key.name] = mask_weights
                non_relative_shape_keys.add(skipped_blend_shape_key.name)
                config_blend_shape_labels.add(label)
                label_to_target_shape_key_name[label] = skipped_blend_shape_key.name


    # 2. Processing for shape keys not included in clothing_avatar_data's blend shapes (currently only copies are made)
    if obj.data.shape_keys:
        # Create a list of blend shapes from clothing_avatar_data
        clothing_blendshapes = set()
        if clothing_avatar_data and "blendshapes" in clothing_avatar_data:
            for blendshape in clothing_avatar_data["blendshapes"]:
                clothing_blendshapes.add(blendshape["name"])

        # Processing for each shape key
        for key_block in obj.data.shape_keys.key_blocks:
            if (key_block.name == "Basis" or
                key_block.name in clothing_blendshapes or
                key_block == shape_key or
                key_block.name.endswith("_BaseShape") or
                key_block.name in config_generated_shape_keys.keys() or
                key_block.name in config_blend_shape_labels or
                key_block.name.endswith("_original") or
                key_block.name.endswith("_generated") or
                key_block.name.endswith("_temp")):
                continue  # Skip those contained in the blendshapes of Basis or clothing_avatar_data, those ending with _BaseShape, or those generated by config

            print(f"Processing additional shape key: {key_block.name}")

            temp_blend_shape_key_name = f"{key_block.name}_generated"
            if temp_blend_shape_key_name in obj.data.shape_keys.key_blocks:
                temp_shape_key = obj.data.shape_keys.key_blocks[temp_blend_shape_key_name]
            else:
                temp_shape_key = obj.shape_key_add(name=temp_blend_shape_key_name)
            for i, vertex in enumerate(temp_shape_key.data):
                vertex.co = key_block.data[i].co.copy()

    # Execute delayed Transition with the cache system
    created_shape_key_mask_weights = {}
    shape_keys_to_remove = []
    if deferred_transitions:
        transition_operations, created_shape_key_mask_weights, used_shape_key_names = execute_transitions_with_cache(deferred_transitions, transition_cache, obj, rigid_transformation=True)
        if used_shape_key_names:
            for config_shape_key_name in config_generated_shape_keys:
                if config_shape_key_name not in used_shape_key_names and config_shape_key_name in obj.data.shape_keys.key_blocks:
                    shape_keys_to_remove.append(config_shape_key_name)

    for created_shape_key_name, mask_weights in created_shape_key_mask_weights.items():
        if created_shape_key_name in obj.data.shape_keys.key_blocks:
            config_generated_shape_keys[created_shape_key_name] = mask_weights
            non_relative_shape_keys.add(created_shape_key_name)
            config_blend_shape_labels.add(created_shape_key_name)
            label_to_target_shape_key_name[created_shape_key_name] = created_shape_key_name
            print(f"Added created shape key: {created_shape_key_name}")

    if overwrite_base_shape_key:
        # Preparation before processing the blendShapeFields of base_avatar_data
        basis_name = 'Basis'
        basis_index = obj.data.shape_keys.key_blocks.find(basis_name)

        bpy.ops.object.mode_set(mode='OBJECT')
        bpy.ops.object.select_all(action='DESELECT')
        bpy.context.view_layer.objects.active = obj
        obj.select_set(True)

        bpy.ops.object.mode_set(mode='EDIT')
        bpy.ops.mesh.select_all(action='SELECT')

        print(f"Shape keys in {obj.name}:")
        for key_block in obj.data.shape_keys.key_blocks:
            print(f"- {key_block.name} (value: {key_block.value})")

        original_shape_key_name = f"{shape_key_name}_original"
        for sk in obj.data.shape_keys.key_blocks:
            if sk.name in non_relative_shape_keys and sk.name != basis_name:
                if shape_key_name in obj.data.shape_keys.key_blocks:
                    obj.active_shape_key_index = obj.data.shape_keys.key_blocks.find(sk.name)
                    bpy.ops.mesh.blend_from_shape(shape=shape_key_name, blend=-1, add=True)
                else:
                    print(f"Warning: {shape_key_name} or {shape_key_name}_original is not found in shape keys")

        bpy.context.object.active_shape_key_index = basis_index
        bpy.ops.mesh.blend_from_shape(shape=shape_key_name, blend=1, add=True)

        bpy.ops.object.mode_set(mode='OBJECT')

        if original_shape_key_name in obj.data.shape_keys.key_blocks:
            original_shape_key = obj.data.shape_keys.key_blocks.get(original_shape_key_name)
            obj.shape_key_remove(original_shape_key)
            print(f"Removed shape key: {original_shape_key_name} from {obj.name}")

        # Delete unnecessary shape keys
        if shape_key:
            obj.shape_key_remove(shape_key)

        # Apply mask_weights to the displacement of shape keys generated by the blendShapeFields in the config file
        if config_generated_shape_keys:
            print(f"Applying mask weights to generated shape keys: {list(config_generated_shape_keys.keys())}")

            # Get the vertex positions of the base shape
            basis_shape_key = obj.data.shape_keys.key_blocks.get(basis_name)
            if basis_shape_key:
                basis_positions = np.array([v.co for v in basis_shape_key.data])

                # Apply a mask to each generated shape key
                for shape_key_name_to_mask, mask_weights in config_generated_shape_keys.items():
                    if shape_key_name_to_mask == basis_name:
                        continue

                    shape_key_to_mask = obj.data.shape_keys.key_blocks.get(shape_key_name_to_mask)
                    if shape_key_to_mask:
                        # Get the vertex positions of the current shape key
                        shape_positions = np.array([v.co for v in shape_key_to_mask.data])

                        # Calculate displacement
                        displacement = shape_positions - basis_positions

                        # Apply mask (multiply displacement by mask_weights)
                        if mask_weights is not None:
                            masked_displacement = displacement * mask_weights[:, np.newaxis]
                        else:
                            masked_displacement = displacement

                        # Calculate the position after applying the mask
                        new_positions = basis_positions + masked_displacement

                        # Update shape key vertex positions
                        for i, vertex in enumerate(shape_key_to_mask.data):
                            vertex.co = new_positions[i]

                        print(f"Applied mask weights to shape key: {shape_key_name_to_mask}")

    for unused_shape_key_name in shape_keys_to_remove:
        if unused_shape_key_name in obj.data.shape_keys.key_blocks:
            unused_shape_key = obj.data.shape_keys.key_blocks.get(unused_shape_key_name)
            if unused_shape_key:
                obj.shape_key_remove(unused_shape_key)
                print(f"Removed shape key: {unused_shape_key_name} from {obj.name}")
            else:
                print(f"Warning: {unused_shape_key_name} is not found in shape keys")
        else:
            print(f"Warning: {unused_shape_key_name} is not found in shape keys")

    return shape_key, config_blend_shape_labels

def process_blendshape_fields_with_rigid_transform(obj, field_data_path, base_avatar_data, clothing_avatar_data, config_blend_shape_labels, influence_range=1.0, config_data=None):
    """
    Process the blendShapeFields of base_avatar_data using rigid body transformation

    Parameters:
        obj: Target mesh object
        field_data_path: Deformation Field path
        base_avatar_data: Base Avatar Data
        clothing_avatar_data: Clothing avatar data
        influence_range: DistanceWeight influence range by vertex group (0.0-1.0, default 0.5)
    """

    # Process the blendShapeFields of base_avatar_data
    if base_avatar_data and "blendShapeFields" in base_avatar_data:
        # Acquisition of the Armature
        armature_obj = get_armature_from_modifier(obj)
        if not armature_obj:
            raise ValueError("Armature modifier not found")

        # Save the original shape key settings for the target mesh object
        original_shape_key_state = save_shape_key_state(obj)

        # Set all shape key values to 0
        if obj.data.shape_keys:
            for key_block in obj.data.shape_keys.key_blocks:
                key_block.value = 0.0

        # Get the vertex positions of the evaluated mesh (after applying Shape Key A)
        depsgraph = bpy.context.evaluated_depsgraph_get()
        depsgraph.update()
        eval_obj = obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data
        vertices = np.array([v.co for v in obj.data.vertices])  # Original vertex array
        deformed_vertices = np.array([v.co for v in eval_mesh.vertices])

        # Process each blendShapeField using rigid body transformation
        for blend_field in base_avatar_data["blendShapeFields"]:
            label = blend_field["label"]

            # If it matches the label in the blendShapeFields section of the config file, skip it
            if label in config_blend_shape_labels:
                print(f"Skipping base avatar blend shape field '{label}' (already processed from config)")
                continue

            field_path = os.path.join(os.path.dirname(field_data_path), blend_field["filePath"])

            if os.path.exists(field_path):
                print(f"Applying blend shape field for {label} with rigid transform")
                # Reading Field Data
                field_info_blend = get_deformation_field_multi_step(field_path)
                blend_points = field_info_blend['all_field_points']
                blend_deltas = field_info_blend['all_delta_positions']
                blend_field_weights = field_info_blend['field_weights']
                blend_matrix = field_info_blend['world_matrix']
                blend_matrix_inv = field_info_blend['world_matrix_inv']
                blend_k_neighbors = field_info_blend['kdtree_query_k']

                # Get mask weight
                mask_weights = None
                if "maskBones" in blend_field:
                    mask_weights = create_blendshape_mask(obj, blend_field["maskBones"], clothing_avatar_data, field_name=label, store_debug_mask=True)

                # Calculate the position after deformation
                deformed_positions = batch_process_vertices_multi_step(
                    deformed_vertices,
                    blend_points,
                    blend_deltas,
                    blend_field_weights,
                    blend_matrix,
                    blend_matrix_inv,
                    obj.matrix_world,
                    obj.matrix_world.inverted(),
                    mask_weights,
                    batch_size=1000,
                    k=blend_k_neighbors
                )

                # Check whether the displacement is zero in world coordinates
                has_displacement = False
                for i in range(len(deformed_vertices)):
                    displacement = deformed_positions[i] - (obj.matrix_world @ Vector(deformed_vertices[i]))
                    if np.any(np.abs(displacement) > 1e-5):  # Neglect minor displacements
                        print(f"blendShapeFields {label} world_displacement: {displacement}")
                        has_displacement = True
                        break

                # Create shape keys only when displacement exists
                if has_displacement:
                    # Calculate similarity transformations from the source and transformed point clouds
                    source_points = np.array([obj.matrix_world @ Vector(v) for v in deformed_vertices])
                    target_points = np.array(deformed_positions)

                    # # DistanceWeight Get influence from vertex group
                    # influence_factors = get_distance_weight_influence_factors(obj, influence_range)

                    # # Calculate optimal similarity transformation (weighted or standard)
                    # if influence_factors is not None:
                    #     print(f"Using weighted similarity transform with DistanceWeight vertex group for blend shape {label}")
                    #     s, R, t = calculate_optimal_similarity_transform_weighted(source_points, target_points, influence_factors)
                    # else:
                    #     s, R, t = calculate_optimal_similarity_transform(source_points, target_points)

                    s, R, t = calculate_optimal_similarity_transform(source_points, target_points)

                    # Calculate the result after applying the similarity transformation
                    similarity_transformed = apply_similarity_transform_to_points(source_points, s, R, t)

                    blend_shape_key_name = label
                    if obj.data.shape_keys and label in obj.data.shape_keys.key_blocks:
                        blend_shape_key_name = f"{label}_generated"

                    # Create a shape key
                    shape_key_b = obj.shape_key_add(name=blend_shape_key_name)
                    shape_key_b.value = 0.0  # The initial value is 0

                    # Save vertex positions to shape keys
                    matrix_armature_inv_fallback = Matrix.Identity(4)
                    for i in range(len(vertices)):
                        matrix_armature_inv = calculate_inverse_pose_matrix(obj, armature_obj, i)
                        if matrix_armature_inv is None:
                            matrix_armature_inv = matrix_armature_inv_fallback
                        # Convert the transformed position to local coordinates
                        deformed_world_pos = matrix_armature_inv @ Vector(similarity_transformed[i])
                        deformed_local_pos = obj.matrix_world.inverted() @ deformed_world_pos
                        shape_key_b.data[i].co = deformed_local_pos
                        matrix_armature_inv_fallback = matrix_armature_inv
                else:
                    print(f"Skipping creation of shape key '{label}' as it has no displacement")

            else:
                print(f"Warning: Field file not found for blend shape {label}")
        # Restore original shape key settings
        restore_shape_key_state(obj, original_shape_key_state)

def calculate_obb_from_object(obj):
    """
    Calculate the Oriented Bounding Box (OBB) of an object

    Parameters:
        obj: Target mesh object

    Returns:
        dict: OBB information (center, axis, radius)
    """
    # Retrieve evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data

    # Obtain vertex coordinates in world space
    vertices = np.array([obj.matrix_world @ v.co for v in eval_mesh.vertices])

    if len(vertices) == 0:
        return None

    # Calculate the average position (center) of the vertices
    center = np.mean(vertices, axis=0)

    # Move the center to the origin
    centered_vertices = vertices - center

    # Calculate the covariance matrix
    covariance_matrix = np.cov(centered_vertices.T)

    # Calculate eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)

    # Normalize the eigenvector
    for i in range(3):
        eigenvectors[:, i] = eigenvectors[:, i] / np.linalg.norm(eigenvectors[:, i])

    # Calculate the maximum value of the projection along each axis
    min_proj = np.full(3, float('inf'))
    max_proj = np.full(3, float('-inf'))

    for vertex in centered_vertices:
        for i in range(3):
            proj = np.dot(vertex, eigenvectors[:, i])
            min_proj[i] = min(min_proj[i], proj)
            max_proj[i] = max(max_proj[i], proj)

    # Calculate the radius (half the length of each axis)
    radii = (max_proj - min_proj) / 2

    # Adjust the center position
    adjusted_center = center + np.sum([(min_proj[i] + max_proj[i]) / 2 * eigenvectors[:, i] for i in range(3)], axis=0)

    return {
        'center': adjusted_center,
        'axes': eigenvectors,
        'radii': radii
    }

def check_mesh_obb_intersection(mesh_obj, obb):
    """
    Check for intersections between the mesh and OBB

    Parameters:
        mesh_obj: Mesh object to be checked
        obb: OBB information (center, axes, radius)

    Returns:
        bool: True if intersecting
    """
    if obb is None:
        return False

    # Retrieve evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = mesh_obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data

    # Convert mesh vertices to OBB space for intersection checking
    for v in eval_mesh.vertices:
        # World coordinates of the vertex
        vertex_world = mesh_obj.matrix_world @ v.co

        # Relative position from the center of the OBB
        relative_pos = vertex_world - Vector(obb['center'])

        # Projection along each axis of the OBB
        projections = [abs(relative_pos.dot(Vector(obb['axes'][:, i]))) for i in range(3)]

        # If the projection lies within the radius on all axes, they intersect
        if all(proj <= radius for proj, radius in zip(projections, obb['radii'])):
            return True

    return False

def calculate_distance_based_weights(source_obj_name, target_obj_name, vertex_group_name="DistanceWeight", min_distance=0.0, max_distance=0.03):
    """
    Measure the closest surface distance from each vertex of the specified object to another object,
    Function to set vertex weights based on distance

    Args:
        source_obj_name (str): Object name to set the weight for
        target_obj_name (str): Object name for distance measurement
        vertex_group_name (str): Name of the vertex group to be created
        min_distance (float): Minimum distance (distance where weight becomes 1.0)
        max_distance (float): Maximum distance (distance at which weight becomes 0.0)
    """

    # Get the object
    source_obj = bpy.data.objects.get(source_obj_name)
    target_obj = bpy.data.objects.get(target_obj_name)

    if not source_obj:
        print(f"Error: Object '{source_obj_name}' not found")
        return False

    if not target_obj:
        print(f"Error: Object '{target_obj_name}' not found")
        return False

    # Acquire mesh data
    source_mesh = source_obj.data
    target_mesh = target_obj.data

    # Create or retrieve a vertex group
    if vertex_group_name not in source_obj.vertex_groups:
        vertex_group = source_obj.vertex_groups.new(name=vertex_group_name)
    else:
        vertex_group = source_obj.vertex_groups[vertex_group_name]

    # Create the BVHTree for the target object
    print("Building BVHTree...")

    # Get the vertices and polygons of the target mesh in world coordinates
    target_verts = []
    target_polys = []

    # Retrieve the evaluated mesh (with modifiers applied)
    depsgraph = bpy.context.evaluated_depsgraph_get()
    target_eval = target_obj.evaluated_get(depsgraph)
    target_mesh_eval = target_eval.data

    # Convert to world coordinates
    target_matrix = target_obj.matrix_world

    for vert in target_mesh_eval.vertices:
        world_co = target_matrix @ vert.co
        target_verts.append(world_co)

    for poly in target_mesh_eval.polygons:
        target_polys.append(poly.vertices)

    # Constructing a BVHTree
    bvh = BVHTree.FromPolygons(target_verts, target_polys)

    print("Calculating distance and setting weights...")

    # Process each vertex of the source object
    source_matrix = source_obj.matrix_world
    source_eval = source_obj.evaluated_get(depsgraph)
    source_mesh_eval = source_eval.data

    weights = []

    for i, vert in enumerate(source_mesh_eval.vertices):
        # Get the world coordinates of the vertex
        world_co = source_matrix @ vert.co

        # Calculate the distance to the nearest surface
        location, normal, index, distance = bvh.find_nearest(world_co)

        if location is None:
            print(f"Warning: The nearest face for vertex {i} cannot be found")
            distance = max_distance

        # Calculate weights based on distance
        if distance <= min_distance:
            weight = 1.0
        elif distance >= max_distance:
            weight = 0.0
        else:
            # Calculate weights using linear interpolation (approaching 0 as they get closer to max_distance)
            weight = 1.0 - ((distance - min_distance) / (max_distance - min_distance))

        weights.append(weight)

        # Set weights for vertex groups
        vertex_group.add([i], weight, 'REPLACE')

    print(f"Completed: Weights have been assigned to {len(weights)} vertices")
    print(f"Minimum weight: {min(weights):.4f}")
    print(f"Maximum weight: {max(weights):.4f}")
    print(f"Average weight: {np.mean(weights):.4f}")

    return True

def process_mesh_with_connected_components_inline(obj, field_data_path, blend_shape_labels, clothing_avatar_data, base_avatar_data, clothing_armature, cloth_metadata=None, subdivision=True, skip_blend_shape_generation=False, config_data=None):
    """
    After processing the mesh by connected component and applying appropriate deformations,
    Merge the results while keeping the original objects intact

    Parameters:
        obj: Mesh object to be processed
        field_data_path: Deformation Field path
        blend_shape_labels: List of blend shape labels to apply
        clothing_avatar_data: Clothing avatar data
        base_avatar_data: Base Avatar Data
        clothing_armature: Clothing Armature Object
        cloth_metadata: Cloth metadata (optional)
    """

    # Save object name
    original_name = obj.name

    # Acquire the base mesh
    base_obj = bpy.data.objects.get("Body.BaseAvatar")
    if not base_obj:
        raise Exception("Base avatar mesh (Body.BaseAvatar) not found")

    calculate_distance_based_weights(
        source_obj_name=original_name,
        target_obj_name=base_obj.name,
        vertex_group_name="DistanceWeight",
        min_distance=0.0,
        max_distance=0.1
    )

    # Separate the connected components (while retaining settings such as the armature)
    separated_objects, non_separated_objects = separate_and_combine_components(obj, clothing_armature, clustering=True)

    # If there are no components to separate, perform the normal processing
    if not separated_objects or (cloth_metadata and obj.name in cloth_metadata):
        if cloth_metadata and obj.name in cloth_metadata:
            subdivision = False
        apply_symmetric_field_delta(obj, field_data_path, blend_shape_labels, clothing_avatar_data, base_avatar_data, subdivision, skip_blend_shape_generation=skip_blend_shape_generation, config_data=config_data)
        for sep_obj in non_separated_objects:
            if sep_obj == obj:
                continue  # The original object itself is skipped
            bpy.data.objects.remove(sep_obj, do_unlink=True)
        for sep_obj in separated_objects:
            if sep_obj == obj:
                continue  # The original object itself is skipped
            bpy.data.objects.remove(sep_obj, do_unlink=True)
        return

    # Report progress
    print(f"Processing {original_name}: {len(separated_objects)} separated, {len(non_separated_objects)} non-separated")

    bpy.context.view_layer.objects.active = obj

    # List of non-separable components
    do_not_separate = []

    # List of objects to be processed
    processed_objects = []

    # Apply rigid body transformation processing to separated objects
    for sep_obj in separated_objects:
        bpy.context.view_layer.objects.active = sep_obj
        _, config_blend_shape_labels = apply_field_delta_with_rigid_transform(sep_obj, field_data_path, blend_shape_labels, base_avatar_data, clothing_avatar_data, "RigidTransformed", config_data=None)

        # Preparation for processing the blendShapeFields of base_avatar_data
        if not skip_blend_shape_generation:
            process_blendshape_fields_with_rigid_transform(sep_obj, field_data_path, base_avatar_data, clothing_avatar_data, config_blend_shape_labels, influence_range=1.0, config_data=config_data)

        # Calculate OBB
        obb = calculate_obb_from_object(sep_obj)

        print(f"Component {sep_obj.name} OBB: \n {obb}")

        # Check for intersections between the base mesh and OBB
        if check_mesh_obb_intersection(base_obj, obb):
            print(f"Component {sep_obj.name} intersects with base mesh, will not be separated")
            do_not_separate.append(sep_obj.name)

        processed_objects.append(sep_obj)

    bpy.context.view_layer.objects.active = obj

    # Delete separated objects
    for sep_obj in separated_objects:
        print(f"Removing {sep_obj.name}")
        bpy.data.objects.remove(sep_obj, do_unlink=True)
    # Delete unseparated objects
    for sep_obj in non_separated_objects:
        print(f"Removing {sep_obj.name}")
        bpy.data.objects.remove(sep_obj, do_unlink=True)

    # Perform separation again using the list of non-separated components
    separated_objects, non_separated_objects = separate_and_combine_components(obj, clothing_armature, do_not_separate, clustering=True)

    # Reset the list of objects to be processed
    processed_objects = []

    # Apply rigid body transformation processing to separated objects
    for sep_obj in separated_objects:
        bpy.context.view_layer.objects.active = sep_obj
        _, config_blend_shape_labels = apply_field_delta_with_rigid_transform(sep_obj, field_data_path, blend_shape_labels, base_avatar_data, clothing_avatar_data, "RigidTransformed", config_data=config_data)

        # Preparation for processing the blendShapeFields of base_avatar_data
        if not skip_blend_shape_generation:
            process_blendshape_fields_with_rigid_transform(sep_obj, field_data_path, base_avatar_data, clothing_avatar_data, config_blend_shape_labels, influence_range=1.0, config_data=config_data)
        processed_objects.append(sep_obj)

    # Apply normal transformation processing to objects that were not separated
    for non_sep_obj in non_separated_objects:
        if non_sep_obj is None:
            continue

        if cloth_metadata and non_sep_obj.name in cloth_metadata:
            subdivision = False

        bpy.context.view_layer.objects.active = non_sep_obj

        apply_symmetric_field_delta(non_sep_obj, field_data_path, blend_shape_labels, clothing_avatar_data, base_avatar_data, subdivision, skip_blend_shape_generation=skip_blend_shape_generation, config_data=config_data)
        processed_objects.append(non_sep_obj)

    # Save the shape key information of the original object
    original_shapekeys = {}
    if obj.data.shape_keys:
        for key_block in obj.data.shape_keys.key_blocks:
            original_shapekeys[key_block.name] = key_block.value

    # Sort the faces of each processed object by material order
    for proc_obj in processed_objects:
        if proc_obj is None:
            continue

        bpy.ops.object.mode_set(mode='OBJECT')

        bpy.ops.object.select_all(action='DESELECT')
        proc_obj.select_set(True)
        bpy.context.view_layer.objects.active = proc_obj

        # Enter edit mode
        bpy.ops.object.mode_set(mode='EDIT')

        # Sort faces by material
        bpy.ops.mesh.sort_elements(type='MATERIAL', elements={'FACE'})

        # Return to Object Mode
        bpy.ops.object.mode_set(mode='OBJECT')

    # Enter Edit Mode to delete the vertices of the original object
    bpy.ops.object.select_all(action='DESELECT')
    obj.select_set(True)
    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.mode_set(mode='EDIT')

    # Select all vertices and delete them
    bpy.ops.mesh.select_all(action='SELECT')
    bpy.ops.mesh.delete(type='VERT')

    # Return to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Concatenate each processed object in sequence
    for proc_obj in processed_objects:
        if proc_obj == obj:
            continue  # The original object itself is skipped

        # Set selection
        bpy.ops.object.select_all(action='DESELECT')
        proc_obj.select_set(True)
        obj.select_set(True)
        bpy.context.view_layer.objects.active = obj  # Activate the original object

        # Combination operation
        bpy.ops.object.join()

    # Restore the original object name (as it may change during merge operations)
    obj.name = original_name

    # Restore Shape Key Values
    if obj.data.shape_keys:
        for key_name, value in original_shapekeys.items():
            if key_name in obj.data.shape_keys.key_blocks:
                obj.data.shape_keys.key_blocks[key_name].value = value

    # Restore the original active object
    bpy.context.view_layer.objects.active = obj



def get_deformation_bones(armature_obj: bpy.types.Object, avatar_data: dict) -> list:
    """
    Reference the avatar data and retrieve all bones except the Humanoid bones and Auxiliary bones

    Parameters:
        armature_obj: Armature Object
        avatar_data: Avatar data

    Returns:
        List of bone names to be deformed
    """
    # Create a set of Humanoid bones and Auxiliary bones
    excluded_bones = set()

    # Add Humanoid Bone
    for bone_map in avatar_data.get("humanoidBones", []):
        if "boneName" in bone_map:
            excluded_bones.add(bone_map["boneName"])

    # Add auxiliary bones
    for aux_set in avatar_data.get("auxiliaryBones", []):
        for aux_bone in aux_set.get("auxiliaryBones", []):
            excluded_bones.add(aux_bone)

    # Get all bones except excluded bones
    deform_bones = []
    for bone in armature_obj.data.bones:
        if bone.name not in excluded_bones:
            deform_bones.append(bone.name)

    return deform_bones

def apply_bone_field_delta(armature_obj: bpy.types.Object, field_data_path: str, avatar_data: dict) -> None:
    """
    Apply Deformation Field to the bone

    Parameters:
        armature_obj: Armature Object
        field_data_path: Path to Deformation Field data
        avatar_data: Avatar data
    """
    # Data loading
    field_info = get_deformation_field_multi_step(field_data_path)
    all_field_points = field_info['all_field_points']
    all_delta_positions = field_info['all_delta_positions']
    all_field_weights = field_info['field_weights']
    field_matrix = field_info['world_matrix']
    field_matrix_inv = field_info['world_matrix_inv']
    k_neighbors = field_info['kdtree_query_k']

    # Get the bone to be deformed
    deform_bones = get_deformation_bones(armature_obj, avatar_data)

    bpy.ops.object.mode_set(mode='OBJECT')

    # Clear all selections
    bpy.ops.object.select_all(action='DESELECT')

    # Set active objects
    armature_obj.select_set(True)
    bpy.context.view_layer.objects.active = armature_obj

    # ------------------------------------------------------------------
    # [Additional Processing: Recording Parent-Child Head Positions Before Processing]
    # For bones with "only one child" within deform_bones, the parent bone and its child bone
    # Record the Head position in world space.
    # ------------------------------------------------------------------
    original_heads = {}
    for bone in armature_obj.pose.bones:
        if bone.name in deform_bones and len(bone.children) == 1:
            child = bone.children[0]
            parent_head_world = armature_obj.matrix_world @ (bone.matrix @ Vector((0, 0, 0)))
            child_head_world = armature_obj.matrix_world @ (child.matrix @ Vector((0, 0, 0)))
            # Copy and save (for future reference)
            original_heads[bone.name] = (parent_head_world.copy(), child_head_world.copy())

    def process_bone_hierarchy(bone_name, parent_world_displacement, kdtree, delta_positions):
        """Recursively process the bone hierarchy"""

        bone = armature_obj.pose.bones[bone_name]
        ret_displacement = parent_world_displacement

        if bone_name in deform_bones:
            base_matrix = armature_obj.data.bones[bone.name].matrix_local
            current_world_matrix = armature_obj.matrix_world @ (bone.matrix @ base_matrix.inverted())

            # Obtain the head position
            head_world = (armature_obj.matrix_world @ bone.matrix @ Vector((0, 0, 0))) - parent_world_displacement

            # Calculate the coordinates of the head in the field space
            head_field = field_matrix_inv @ head_world

            # Search for the Head's Most Recent Contact Point
            head_distances, head_indices = kdtree.query(head_field, k=k_neighbors)

            # Calculate the head displacement
            weights = 1.0 / (head_distances + 0.0001)
            weights /= weights.sum()
            deltas = delta_positions[head_indices]
            head_displacement = (deltas * weights[:, np.newaxis]).sum(axis=0)

            # Calculate displacement in world space
            world_displacement = (field_matrix.to_3x3() @ Vector(head_displacement)) - parent_world_displacement

            new_matrix = Matrix.Translation(world_displacement)
            combined_matrix = new_matrix @ current_world_matrix
            bone.matrix = armature_obj.matrix_world.inverted() @ combined_matrix @ base_matrix

            ret_displacement = world_displacement + parent_world_displacement

        # Process child bones
        for child in bone.children:
            process_bone_hierarchy(child.name, ret_displacement, kdtree, delta_positions)

    # Apply the displacement cumulatively at each step
    num_steps = len(all_field_points)
    for step in range(num_steps):
        field_points = all_field_points[step]
        delta_positions = all_delta_positions[step]
        # Search for nearest points using KDTree (construct a new KDTree at each step)
        kdtree = cKDTree(field_points, balanced_tree=False, compact_nodes=False)

        # Begin processing from Root Bone
        root_displacement = Vector((0, 0, 0))
        root_bones = [bone.name for bone in armature_obj.pose.bones if not bone.parent]
        for root_bone in root_bones:
            process_bone_hierarchy(root_bone, root_displacement, kdtree, delta_positions)

        bpy.context.view_layer.update()


    # ------------------------------------------------------------------
    # [Additional Processing: Applying Rotation Correction]
    # For the target deform_bone (with only one child), the values before and after processing
    # Calculate the rotational difference from the change in direction vectors between parent and child Head, and apply that rotation
    # Apply it to the parent bone while applying a correction to the child bone that cancels out its effect.
    # ------------------------------------------------------------------
    # for parent_name, (old_parent_head, old_child_head) in original_heads.items():
    #     parent_bone = armature_obj.pose.bones.get(parent_name)
    #     if not parent_bone or len(parent_bone.children) != 1:
    #         continue
    #     child_bone = parent_bone.children[0]

    #     # [After processing] Calculate the parent and child Head positions (world coordinates)
    #     new_parent_head = armature_obj.matrix_world @ (parent_bone.matrix @ Vector((0, 0, 0)))
    #     new_child_head = armature_obj.matrix_world @ (child_bone.matrix @ Vector((0, 0, 0)))

    #     # Calculate the direction vector before and after processing (Child Head - Parent Head)
    #     old_dir = old_child_head - old_parent_head
    #     new_dir = new_child_head - new_parent_head
    #     # If either vector is zero-length, skip
    #     if old_dir.length == 0.001 or new_dir.length == 0.001:
    #         continue
    #     old_dir.normalize()
    #     new_dir.normalize()

    #     # Calculate the rotational difference between "old_dir" and "new_dir"
    #     rot_diff = old_dir.rotation_difference(new_dir)

    #     # Apply rot_diff to the parent bone centered on the parent's Head
    #     parent_world_matrix = armature_obj.matrix_world @ parent_bone.matrix
    #     T = Matrix.Translation(new_parent_head)
    #     T_inv = Matrix.Translation(-new_parent_head)
    #     rot_matrix = rot_diff.to_matrix().to_4x4()
    #     R = T @ rot_matrix @ T_inv
    #     new_parent_world_matrix = R @ parent_world_matrix
    #     parent_bone.matrix = armature_obj.matrix_world.inverted() @ new_parent_world_matrix

    #     # Apply inverse compensation to child bones so that they are not affected by the parent's rotation changes
    #     child_world_matrix = armature_obj.matrix_world @ child_bone.matrix
    #     compensation = T @ rot_matrix.inverted() @ T_inv
    #     new_child_world_matrix = compensation @ child_world_matrix
    #     child_bone.matrix = armature_obj.matrix_world.inverted() @ new_child_world_matrix

    #     bpy.context.view_layer.update()

    bpy.context.view_layer.update()

    # Return to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')



def find_connected_clusters(bm, vertex_indices):
    """
    Find clusters of vertices connected by edges

    Args:
        bm: bmesh object
        vertex_indices: Set of vertex indices to analyze

    Returns:
        list: A list of vertex index lists for each cluster
    """
    # Create an adjacency list
    adjacency = defaultdict(set)
    for edge in bm.edges:
        v1, v2 = edge.verts[0].index, edge.verts[1].index
        if v1 in vertex_indices and v2 in vertex_indices:
            adjacency[v1].add(v2)
            adjacency[v2].add(v1)

    visited = set()
    clusters = []

    # Search for clusters from each unvisited vertex using BFS
    for vertex_idx in vertex_indices:
        if vertex_idx not in visited:
            cluster = []
            queue = deque([vertex_idx])
            visited.add(vertex_idx)

            while queue:
                current = queue.popleft()
                cluster.append(current)

                # Add adjacent unvisited vertices to the queue
                for neighbor in adjacency[current]:
                    if neighbor not in visited:
                        visited.add(neighbor)
                        queue.append(neighbor)

            clusters.append(cluster)

    return clusters

def filter_clusters_by_x_coordinate(bm, clusters):
    """
    Filter only clusters containing both positive and negative X-coordinates or zero

    Args:
        bm: bmesh object
        clusters: List of clusters

    Returns:
        list: List of filtered clusters
    """
    filtered_clusters = []

    for cluster in clusters:
        has_positive_x = False
        has_negative_x = False
        has_zero_x = False

        # Check the X-coordinate of the vertices within the cluster
        for vertex_idx in cluster:
            x_coord = bm.verts[vertex_idx].co.x

            if x_coord > 0.001:  # Positive values (accounting for minor errors)
                has_positive_x = True
            elif x_coord < -0.001:  # Negative values (accounting for small errors)
                has_negative_x = True
            else:  # near zero
                has_zero_x = True

        # Only retain when the X coordinate includes both positive and negative values or zero
        if (has_positive_x and has_negative_x) or has_zero_x:
            filtered_clusters.append(cluster)
            print(f"Cluster retention: {len(cluster)} vertices (Positive: {has_positive_x}, Negative: {has_negative_x}, Zero: {has_zero_x})")
        else:
            print(f"Cluster Removal: {len(cluster)} vertices (Positive: {has_positive_x}, Negative: {has_negative_x}, Zero: {has_zero_x})")

    return filtered_clusters

def select_vertices_by_conditions(target_object, vertex_group_name, avatar_data, radius=0.075, max_angle_degrees=45.0):
    """
    Create a new vertex group for vertices that meet specific conditions within the specified mesh object and assign weights to them

    Conditions:
    1. Within the specified radius, there are vertices that have vertex weights for both the LeftUpperLeg or its auxiliary bone and the RightUpperLeg or its auxiliary bone
    2. The angle between the vertex normal and the -Z direction vector is less than or equal to the specified angle

    Args:
        target_object: Mesh object to be processed
        vertex_group_name (str): Name of the vertex group to be created
        avatar_data (dict): Avatar data (including humanoidBones and auxiliaryBones)
        radius (float): Search radius
        max_angle_degrees (float): Maximum angle (degrees)
    """

    # Object Validation
    if not target_object or target_object.type != 'MESH':
        print("Error: The specified object is not a mesh")
        return

    # Save the current active object and selection state
    original_active = bpy.context.active_object
    original_selected = bpy.context.selected_objects
    original_mode = bpy.context.mode

    # Activate the target object and switch to Edit mode
    bpy.ops.object.select_all(action='DESELECT')
    target_object.select_set(True)
    bpy.context.view_layer.objects.active = target_object

    # Maintain Object mode to obtain the meshed data after evaluation
    if bpy.context.mode != 'OBJECT':
        bpy.ops.object.mode_set(mode='OBJECT')

    # Acquire the mesh data after evaluation
    depsgraph = bpy.context.evaluated_depsgraph_get()
    evaluated_object = target_object.evaluated_get(depsgraph)
    mesh_data = evaluated_object.data

    # Acquire the World Matrix
    world_matrix = evaluated_object.matrix_world

    # Ensure mesh data updates
    mesh_data.calc_loop_triangles()
    mesh_data.update()

    # Retrieve bone names from avatar data
    left_upper_leg_bone_name = None
    right_upper_leg_bone_name = None
    hips_bone_name = None

    # Get the name of the auxiliary bone
    left_upper_leg_auxiliary_bones = []
    right_upper_leg_auxiliary_bones = []
    hips_auxiliary_bones = []

    for bone_info in avatar_data.get('humanoidBones', []):
        if bone_info['humanoidBoneName'] == 'LeftUpperLeg':
            left_upper_leg_bone_name = bone_info['boneName']
        elif bone_info['humanoidBoneName'] == 'RightUpperLeg':
            right_upper_leg_bone_name = bone_info['boneName']
        elif bone_info['humanoidBoneName'] == 'Hips':
            hips_bone_name = bone_info['boneName']

    # Acquire auxiliary bone
    for aux_set in avatar_data.get('auxiliaryBones', []):
        if aux_set['humanoidBoneName'] == 'LeftUpperLeg':
            left_upper_leg_auxiliary_bones = aux_set['auxiliaryBones']
        elif aux_set['humanoidBoneName'] == 'RightUpperLeg':
            right_upper_leg_auxiliary_bones = aux_set['auxiliaryBones']
        elif aux_set['humanoidBoneName'] == 'Hips':
            hips_auxiliary_bones = aux_set['auxiliaryBones']

    # Acquiring the vertex group
    upper_leg_l_groups = []
    upper_leg_r_groups = []
    hips_groups = []

    # Get the vertex group of the main bone
    for vg in target_object.vertex_groups:
        if left_upper_leg_bone_name and vg.name == left_upper_leg_bone_name:
            upper_leg_l_groups.append(vg)
        elif right_upper_leg_bone_name and vg.name == right_upper_leg_bone_name:
            upper_leg_r_groups.append(vg)
        elif hips_bone_name and vg.name == hips_bone_name:
            hips_groups.append(vg)

    # Get the vertex group of the auxiliary bone
    for vg in target_object.vertex_groups:
        if vg.name in left_upper_leg_auxiliary_bones:
            upper_leg_l_groups.append(vg)
        elif vg.name in right_upper_leg_auxiliary_bones:
            upper_leg_r_groups.append(vg)
        elif vg.name in hips_auxiliary_bones:
            hips_groups.append(vg)

    if not upper_leg_l_groups or not upper_leg_r_groups:
        available_l_bones = [left_upper_leg_bone_name] + left_upper_leg_auxiliary_bones
        available_r_bones = [right_upper_leg_bone_name] + right_upper_leg_auxiliary_bones
        print(f"Error: The vertex group for LeftUpperLeg({available_l_bones}) or RightUpperLeg({available_r_bones}) was not found")
        # Restore to original state
        bpy.ops.object.mode_set(mode='OBJECT')
        bpy.ops.object.select_all(action='DESELECT')
        for obj in original_selected:
            obj.select_set(True)
        if original_active:
            bpy.context.view_layer.objects.active = original_active
        return

    # Index set for the target vertex group
    target_group_indices = set()
    for group in upper_leg_l_groups + upper_leg_r_groups + hips_groups:
        target_group_indices.add(group.index)

    # Collect the indices of vertices belonging to the target vertex group (using post-evaluation mesh data)
    target_vertex_indices = set()
    for vertex_idx, vertex in enumerate(mesh_data.vertices):
        for group_elem in vertex.groups:
            if group_elem.group in target_group_indices and group_elem.weight > 0.001:
                target_vertex_indices.add(vertex_idx)
                break

    # -Z direction vector
    neg_z_vector = Vector((0, 0, -1))
    max_angle_rad = math.radians(max_angle_degrees)

    # Create a new vertex group (if one already exists, delete it first before creating)
    if vertex_group_name in target_object.vertex_groups:
        target_object.vertex_groups.remove(target_object.vertex_groups[vertex_group_name])
    new_vertex_group = target_object.vertex_groups.new(name=vertex_group_name)

    selected_count = 0
    condition_met_vertices = []

    start_time = time.time()

    # Convert all vertex coordinates to world coordinates and convert to a list (using post-evaluation mesh data)
    vertex_coords = [(world_matrix @ vert.co)[:] for vert in mesh_data.vertices]

    print(f"Number of vertices: {len(vertex_coords)}")

    # Construct a KDTree (in world coordinates)
    kdtree = cKDTree(vertex_coords)

    # Check conditions only for vertices belonging to the target vertex group (using post-evaluation mesh data)
    for vert_idx, vert in enumerate(mesh_data.vertices):
        if vert_idx not in target_vertex_indices:
            continue
        # Check whether this vertex satisfies the condition
        should_select = False

        # Convert the current vertex to world coordinates
        world_vert_co = world_matrix @ vert.co

        if world_vert_co.y < 0.0:
            should_select = True
        else:
            # Retrieve vertex indices within a specified radius using KDTree (in world coordinates)
            neighbor_indices = kdtree.query_ball_point(world_vert_co[:], radius)

            # Exclude oneself
            neighbor_indices = [idx for idx in neighbor_indices if idx != vert_idx]

            # Check neighboring vertices
            for neighbor_idx in neighbor_indices:
                has_upper_leg_l_or_aux = False
                has_upper_leg_r_or_aux = False

                # Weight Check (Using post-evaluation mesh data)
                for group_elem in mesh_data.vertices[neighbor_idx].groups:
                    # Check the weight of LeftUpperLeg or its auxiliary bone
                    for left_group in upper_leg_l_groups:
                        if group_elem.group == left_group.index and group_elem.weight > 0.05:
                            has_upper_leg_l_or_aux = True
                            break

                    # Check the weight of RightUpperLeg or its auxiliary bone
                    for right_group in upper_leg_r_groups:
                        if group_elem.group == right_group.index and group_elem.weight > 0.05:
                            has_upper_leg_r_or_aux = True
                            break

                    # If both are found, end early
                    if has_upper_leg_l_or_aux and has_upper_leg_r_or_aux:
                        break

                # When both weights (main or auxiliary) are present
                if has_upper_leg_l_or_aux and has_upper_leg_r_or_aux:
                    should_select = True
                    break

        # Normal checks when conditions are met will be performed later
        if should_select:
            # Retrieve vertex normals (using post-evaluation mesh data)
            local_normal = vert.normal

            # Transform normals into world space (apply only rotation and scaling, ignore translations)
            world_normal = (world_matrix.to_3x3().inverted().transposed() @ local_normal).normalized()

            # Calculate the angle between the normal vector and the -Z direction vector (in world coordinate system)
            dot_product = world_normal.dot(neg_z_vector)
            # Clamp the dot product to the range [-1, 1]
            dot_product = max(-1.0, min(1.0, dot_product))
            angle = math.acos(abs(dot_product))

            # Add only if the angle is less than or equal to the specified value
            if angle <= max_angle_rad:
                condition_met_vertices.append(vert_idx)
                selected_count += 1

    end_time = time.time()
    print(f"KDTree search completed: {end_time - start_time:.3f} seconds")

    # Cluster Analysis and Filtering
    if condition_met_vertices:
        print(f"Cluster analysis start: {len(condition_met_vertices)} vertices")

        # Create bmesh from evaluated mesh (for cluster analysis)
        temp_bm = bmesh.new()
        temp_bm.from_mesh(mesh_data)
        temp_bm.verts.ensure_lookup_table()
        temp_bm.edges.ensure_lookup_table()
        temp_bm.faces.ensure_lookup_table()

        # Split into clusters
        clusters = find_connected_clusters(temp_bm, set(condition_met_vertices))
        print(f"Number of clusters discovered: {len(clusters)}")

        # Filtering by X-coordinate
        filtered_clusters = filter_clusters_by_x_coordinate(temp_bm, clusters)
        print(f"Number of clusters after filtering: {len(filtered_clusters)}")

        # Release temporary bmesh
        temp_bm.free()

        # Reconstruct the vertex list from the filtered cluster
        final_vertices = []
        for cluster in filtered_clusters:
            final_vertices.extend(cluster)

        condition_met_vertices = final_vertices
        selected_count = len(condition_met_vertices)

        print(f"Final vertex count: {selected_count}")

    end_time = time.time()
    print(f"KDTree search completed: {end_time - start_time:.3f} seconds")

    # Assign a weight of 1 to vertices that satisfy the condition, and assign a weight of 0 to all others
    for vertex_idx in range(len(target_object.data.vertices)):
        if vertex_idx in condition_met_vertices:
            new_vertex_group.add([vertex_idx], 1.0, 'REPLACE')
        else:
            new_vertex_group.add([vertex_idx], 0.0, 'REPLACE')

    # Restore to original state
    bpy.ops.object.select_all(action='DESELECT')
    for obj in original_selected:
        obj.select_set(True)
    if original_active:
        bpy.context.view_layer.objects.active = original_active
        if original_mode.startswith('EDIT'):
            bpy.ops.object.mode_set(mode='EDIT')

    print(f"Created vertex group: {vertex_group_name}")
    print(f"Number of vertices meeting the conditions: {selected_count}")
    print(f"Number of target vertices: {len(target_vertex_indices)}")
    print(f"Search radius: {radius}")
    print(f"Maximum angle: {max_angle_degrees} degrees")
    print(f"Total number of vertices: {len(target_object.data.vertices)}")

def transfer_weights_from_nearest_vertex(base_mesh, target_obj, vertex_group_name, angle_min=-1.0, angle_max=-1.0, normal_radius=0.0):
    """
    Transfer the weights of the specified vertex group from base_mesh to target_obj

    For each vertex of target_obj, obtain the closest vertex of base_mesh and set its weight value
    Adjust weights based on the angle formed by the normal vector

    Args:
        base_mesh: Base mesh object (source of weights)
        target_obj: Target mesh object (where weights are transferred to)
        vertex_group_name (str): Name of the vertex group to copy
        angle_min (float): Minimum angle value, below this value, weight factor 0.0 (in degrees)
        angle_max (float): Maximum angle value, beyond this value, the weight factor is 1.0 (in degrees)
        normal_radius (float): The radius of the sphere considered when calculating the weighted average of normals
    """

    # Object Validation
    if not base_mesh or base_mesh.type != 'MESH':
        print("Error: Base mesh not specified or is not a mesh")
        return

    if not target_obj or target_obj.type != 'MESH':
        print("Error: Target mesh not specified or is not a mesh")
        return

    # Get the vertex groups of the base mesh
    base_vertex_group = None
    for vg in base_mesh.vertex_groups:
        if vg.name == vertex_group_name:
            base_vertex_group = vg
            break

    if not base_vertex_group:
        print(f"Error: Vertex group '{vertex_group_name}' not found in base mesh")
        return

    print(f"Transferring vertex group '{vertex_group_name}' weights from base mesh '{base_mesh.name}' to target mesh '{target_obj.name}'...")

    # Check the mode and switch to Object Mode
    original_mode = bpy.context.mode
    if original_mode != 'OBJECT':
        bpy.ops.object.mode_set(mode='OBJECT')

    angle_min_rad = math.radians(angle_min)
    angle_max_rad = math.radians(angle_max)

    # Create a BVH tree (for fast nearest point search)
    # Get the target mesh after applying modifiers
    body_bm = get_evaluated_mesh(base_mesh)
    body_bm.faces.ensure_lookup_table()

    # Create the BVH tree for the target mesh
    bvh_time_start = time.time()
    bvh_tree = BVHTree.FromBMesh(body_bm)
    bvh_time = time.time() - bvh_time_start
    print(f"  BVH Tree Creation: {bvh_time:.2f} seconds")

    # If the vertex group does not yet exist, create it
    if vertex_group_name not in target_obj.vertex_groups:
        target_obj.vertex_groups.new(name=vertex_group_name)
    target_vertex_group = target_obj.vertex_groups[vertex_group_name]

    # Get the source mesh after applying modifiers
    cloth_bm = get_evaluated_mesh(target_obj)
    cloth_bm.verts.ensure_lookup_table()
    cloth_bm.faces.ensure_lookup_table()

    # Cache the transform matrix (to avoid repeated calculations)
    body_normal_matrix = base_mesh.matrix_world.inverted().transposed()
    cloth_normal_matrix = target_obj.matrix_world.inverted().transposed()

    # Dictionary storing corrected normals
    adjusted_normals_time_start = time.time()
    adjusted_normals = {}

    # Processing normals for each vertex of the clothing mesh (checking if reversal is necessary)
    for i, vertex in enumerate(cloth_bm.verts):
        # Vertex positions and normals in the world coordinate system
        cloth_vert_world = vertex.co
        original_normal_world = (cloth_normal_matrix @ Vector((vertex.normal[0], vertex.normal[1], vertex.normal[2], 0))).xyz.normalized()

        # Search for the nearest face on the base mesh
        nearest_result = bvh_tree.find_nearest(cloth_vert_world)
        if nearest_result:
            # BVHTree.find_nearest() returns (co, normal, index, distance)
            nearest_point, nearest_normal, nearest_face_index, _ = nearest_result

            # Obtain the nearest neighbor face
            face = body_bm.faces[nearest_face_index]
            face_normal = face.normal

            # Convert the surface normal to world coordinates
            face_normal_world = (body_normal_matrix @ Vector((face_normal[0], face_normal[1], face_normal[2], 0))).xyz.normalized()

            # If the dot product is negative, reverse the normal vector
            dot_product = original_normal_world.dot(face_normal_world)
            if dot_product < 0:
                adjusted_normal = -original_normal_world
            else:
                adjusted_normal = original_normal_world

            # Save adjusted normals to dictionary
            adjusted_normals[i] = adjusted_normal
        else:
            # If the nearest point cannot be found, use the original normal
            adjusted_normals[i] = original_normal_world
    adjusted_normals_time = time.time() - adjusted_normals_time_start
    print(f"  Normal adjustment: {adjusted_normals_time:.2f} seconds")

    # Pre-calculate and cache the center point and area of each face
    face_cache_time_start = time.time()
    face_centers = []
    face_areas = {}
    face_adjusted_normals = {}
    face_indices = []

    for face in cloth_bm.faces:
        # Calculate the center point of the surface
        center = Vector((0, 0, 0))
        for v in face.verts:
            center += v.co
        center /= len(face.verts)
        face_centers.append(center)
        face_indices.append(face.index)

        # Calculate the area
        face_areas[face.index] = face.calc_area()

        # Calculate adjusted surface normals
        face_normal = Vector((0, 0, 0))
        for v in face.verts:
            face_normal += adjusted_normals[v.index]
        face_adjusted_normals[face.index] = face_normal.normalized()
    face_cache_time = time.time() - face_cache_time_start
    print(f"  Face cache creation: {face_cache_time:.2f} seconds")

    # Construct a KDTree for the faces of the clothing mesh
    kdtree_time_start = time.time()

    # kd.balance()
    kd = cKDTree(face_centers)
    kdtree_time = time.time() - kdtree_time_start
    print(f"  KDTree construction: {kdtree_time:.2f} seconds")

    # Update each vertex's normal with the weighted average of the surrounding faces' normals
    normal_avg_time_start = time.time()
    for i, vertex in enumerate(cloth_bm.verts):
        # Search for surfaces within a specified radius
        co = vertex.co
        weighted_normal = Vector((0, 0, 0))
        total_weight = 0

        # Efficiently search for nearby faces using KDTree
        for index in kd.query_ball_point(co, normal_radius):
            # Calculate weights based on distance (closer distances have greater influence)
            face_index = face_indices[index]
            area = face_areas[face_index]
            dist = (co - face_centers[index]).length
            # Distance-based attenuation coefficient
            distance_factor = 1.0 - (dist / normal_radius) if dist < normal_radius else 0.0
            weight = area * distance_factor

            weighted_normal += face_adjusted_normals[face_index] * weight
            total_weight += weight

        # If the sum of the weights is not zero, normalize
        if total_weight > 0:
            weighted_normal /= total_weight
            weighted_normal.normalize()
            # Update adjusted normals
            adjusted_normals[i] = weighted_normal
    normal_avg_time = time.time() - normal_avg_time_start
    print(f"  Normal-weighted average calculation: {normal_avg_time:.2f} seconds")

    # Processing for each vertex of the clothing mesh
    weight_calc_time_start = time.time()
    for i, vertex in enumerate(cloth_bm.verts):
        # Vertex position in the world coordinate system
        cloth_vert_world = vertex.co

        # Use adjusted normals
        cloth_normal_world = adjusted_normals[i]

        # Search for the nearest face on the base mesh
        nearest_result = bvh_tree.find_nearest(cloth_vert_world)
        distance = float('inf')  # Set the initial value to infinity

        # Initial vertex weight
        weight = 0.0

        if nearest_result:
            # BVHTree.find_nearest() returns (co, normal, index, distance)
            nearest_point, nearest_normal, nearest_face_index, _ = nearest_result

            # Obtain the nearest neighbor face
            face = body_bm.faces[nearest_face_index]
            face_normal = face.normal

            # Calculate the nearest point on the surface
            closest_point_on_face = mathutils.geometry.closest_point_on_tri(
                cloth_vert_world,
                face.verts[0].co,
                face.verts[1].co,
                face.verts[2].co
            )

            # Calculate the weights of the {vertex_group_name} vertex group on the base_mesh face using linear interpolation
            # Get the three vertices of the face
            v0, v1, v2 = face.verts[0], face.verts[1], face.verts[2]

            # Get the weight of each vertex
            vg_index = base_vertex_group.index
            w0 = 0.0
            w1 = 0.0
            w2 = 0.0

            # Retrieve vertex weights from the original mesh data of base_mesh
            base_mesh_data = base_mesh.data
            try:
                for group in base_mesh_data.vertices[v0.index].groups:
                    if group.group == vg_index:
                        w0 = group.weight
                        break
            except (IndexError, KeyError):
                pass

            try:
                for group in base_mesh_data.vertices[v1.index].groups:
                    if group.group == vg_index:
                        w1 = group.weight
                        break
            except (IndexError, KeyError):
                pass

            try:
                for group in base_mesh_data.vertices[v2.index].groups:
                    if group.group == vg_index:
                        w2 = group.weight
                        break
            except (IndexError, KeyError):
                pass

            # Calculate the center of gravity coordinates
            # Calculate the coordinates of the centroid from the three vertices of a triangle and a point on the plane
            p0 = v0.co
            p1 = v1.co
            p2 = v2.co
            p = closest_point_on_face

            v0v1 = p1 - p0
            v0v2 = p2 - p0
            v0p = p - p0

            d00 = v0v1.dot(v0v1)
            d01 = v0v1.dot(v0v2)
            d11 = v0v2.dot(v0v2)
            d20 = v0p.dot(v0v1)
            d21 = v0p.dot(v0v2)

            denom = d00 * d11 - d01 * d01
            if abs(denom) > 1e-8:
                # Calculate the center of gravity coordinates (u, v, w)
                v = (d11 * d20 - d01 * d21) / denom
                w = (d00 * d21 - d01 * d20) / denom
                u = 1.0 - v - w

                # Linear interpolation of weights using center-of-gravity coordinates
                weight = u * w0 + v * w1 + w * w2
                # Clamp the weight within the range of 0 to 1
                weight = max(0.0, min(1.0, weight))
            else:
                # For degenerate triangles, use the weight of the nearest vertex
                dist0 = (p - p0).length
                dist1 = (p - p1).length
                dist2 = (p - p2).length

                if dist0 <= dist1 and dist0 <= dist2:
                    weight = w0
                elif dist1 <= dist2:
                    weight = w1
                else:
                    weight = w2

            # Convert the surface normal to world coordinates
            face_normal_world = (body_normal_matrix @ Vector((face_normal[0], face_normal[1], face_normal[2], 0))).xyz.normalized()

            # Calculate distance
            distance = (cloth_vert_world - closest_point_on_face).length

            # Set the nearest point and normal
            nearest_point = closest_point_on_face
            nearest_normal = face_normal_world
        else:
            # If the nearest point cannot be found, set the initial value to None
            nearest_point = None
            nearest_normal = None

        if nearest_point:
            # Weight based on normal angle (linear interpolation)
            angle_weight = 0.0
            if nearest_normal:
                # Calculate the angle of the normal
                angle = math.acos(min(1.0, max(-1.0, cloth_normal_world.dot(nearest_normal))))

                # If the angle exceeds 90 degrees, invert the normal and recalculate
                if angle > math.pi / 2:
                    inverted_normal = -nearest_normal
                    angle = math.acos(min(1.0, max(-1.0, cloth_normal_world.dot(inverted_normal))))

                # Linear interpolation of angles
                if angle <= angle_min_rad:
                    angle_weight = 0.0
                elif angle >= angle_max_rad:
                    angle_weight = 1.0
                else:
                    # Linear interpolation
                    angle_weight = (angle - angle_min_rad) / (angle_max_rad - angle_min_rad)

            weight = weight * angle_weight

        # Set weights for vertex groups
        target_vertex_group.add([i], weight, 'REPLACE')
    weight_calc_time = time.time() - weight_calc_time_start
    print(f"  Weight calculation: {weight_calc_time:.2f} seconds")

    # Restore to original mode
    if original_mode != 'OBJECT':
        if original_mode.startswith('EDIT'):
            bpy.ops.object.mode_set(mode='EDIT')

def barycentric_coords_from_point(p, a, b, c):
    """
    Calculate the coordinates of the centroid of point p on the triangle

    Args:
        p: Point coordinates (Vector)
        a, b, c: Triangle vertex coordinates (Vector)

    Returns:
        (u, v, w): Tuple of centroid coordinates (u + v + w = 1)
    """
    v0 = b - a
    v1 = c - a
    v2 = p - a

    d00 = v0.dot(v0)
    d01 = v0.dot(v1)
    d11 = v1.dot(v1)
    d20 = v2.dot(v0)
    d21 = v2.dot(v1)

    denom = d00 * d11 - d01 * d01

    if abs(denom) < 1e-10:
        # For degenerate triangles, set the weight of the nearest vertex to 1
        dist_a = (p - a).length
        dist_b = (p - b).length
        dist_c = (p - c).length
        min_dist = min(dist_a, dist_b, dist_c)
        if min_dist == dist_a:
            return (1.0, 0.0, 0.0)
        elif min_dist == dist_b:
            return (0.0, 1.0, 0.0)
        else:
            return (0.0, 0.0, 1.0)

    v = (d11 * d20 - d01 * d21) / denom
    w = (d00 * d21 - d01 * d20) / denom
    u = 1.0 - v - w

    return (u, v, w)

def find_vertices_near_faces(base_mesh, target_mesh, vertex_group_name, max_distance=1.0, max_angle_degrees=None, use_all_faces=False,  smooth_repeat=3):
    """
    Find vertices in the target mesh that lie within a specified distance from faces belonging to a specific vertex group in the base mesh, considering the direction of the normal

    Args:
        base_mesh: Base mesh object (possesses vertex groups to which the vertices forming the faces belong)
        target_mesh: Target mesh object (containing the vertices to be searched)
        vertex_group_name (str): Name of the vertex group to search for (common to both meshes)
        max_distance (float): Maximum distance
        max_angle_degrees (float): Maximum angle (degrees), if None, the normal direction is not considered
        use_all_faces (bool): Whether to use all faces
        smooth_repeat (int): Number of smoothing iterations
    """

    # Object Validation
    if not base_mesh or base_mesh.type != 'MESH':
        print("Error: Base mesh not specified or is not a mesh")
        return

    if not target_mesh or target_mesh.type != 'MESH':
        print("Error: Target mesh not specified or is not a mesh")
        return

    # Get the vertex groups of the base mesh
    base_vertex_group = None
    for vg in base_mesh.vertex_groups:
        if vg.name == vertex_group_name:
            base_vertex_group = vg
            break

    if not base_vertex_group:
        print(f"Error: Vertex group '{vertex_group_name}' not found in base mesh")
        return

    # Save the current active object and selection state
    original_active = bpy.context.active_object
    original_selected = bpy.context.selected_objects
    original_mode = bpy.context.mode

    print(f"Analyzing faces belonging to vertex group '{vertex_group_name}' of base mesh '{base_mesh.name}'...")

    # Extract the target faces from the base mesh
    bpy.ops.object.select_all(action='DESELECT')
    base_mesh.select_set(True)
    bpy.context.view_layer.objects.active = base_mesh
    bpy.ops.object.mode_set(mode='OBJECT')

    # Duplicate the base mesh and convert it to triangles
    print("Duplicating the base mesh and converting to triangles...")
    bpy.ops.object.duplicate()
    temp_base_mesh = bpy.context.active_object
    temp_base_mesh.name = f"{base_mesh.name}_temp_triangulated"

    # Triangulate the duplicated mesh
    bpy.ops.object.mode_set(mode='EDIT')
    bpy.ops.mesh.select_all(action='SELECT')
    bpy.ops.mesh.quads_convert_to_tris(quad_method='BEAUTY', ngon_method='BEAUTY')
    bpy.ops.object.mode_set(mode='OBJECT')

    # Acquire post-evaluation mesh data (triangulated base mesh)
    depsgraph = bpy.context.evaluated_depsgraph_get()
    evaluated_base_mesh = temp_base_mesh.evaluated_get(depsgraph)
    base_mesh_data = evaluated_base_mesh.data
    base_world_matrix = evaluated_base_mesh.matrix_world

    # Get the vertex group index of the original base mesh
    base_vertex_group_idx = base_vertex_group.index

    # Verify that the same vertex group exists in the duplicate mesh
    temp_base_vertex_group = None
    for vg in temp_base_mesh.vertex_groups:
        if vg.name == vertex_group_name:
            temp_base_vertex_group = vg
            break

    if not temp_base_vertex_group:
        print(f"Error: Vertex group '{vertex_group_name}' not found in duplicate mesh")
        # Temporarily delete the mesh
        bpy.data.objects.remove(temp_base_mesh, do_unlink=True)
        return

    # Get vertices belonging to vertex groups in the base mesh (using post-evaluation mesh data)
    base_vertices_in_group = set()
    for vertex_idx, vertex in enumerate(base_mesh_data.vertices):
        for group_elem in vertex.groups:
            if group_elem.group == temp_base_vertex_group.index and group_elem.weight > 0.001:
                base_vertices_in_group.add(vertex_idx)
                break

    print(f"Number of vertices belonging to the vertex group: {len(base_vertices_in_group)}")

    # Find faces where all constituent vertices belong to vertex groups (using post-evaluation mesh data)
    target_face_indices = []
    if use_all_faces:
        target_face_indices = [face.index for face in base_mesh_data.polygons]
    else:
        for face in base_mesh_data.polygons:
            if all(vertex_idx in base_vertices_in_group for vertex_idx in face.vertices):
                target_face_indices.append(face.index)

    print(f"Number of faces meeting the conditions: {len(target_face_indices)} (all triangles)")

    if not target_face_indices:
        print("Warning: No surfaces meeting the conditions were found")
        # Temporarily delete the mesh
        bpy.data.objects.remove(temp_base_mesh, do_unlink=True)
        # Restore to original state
        bpy.ops.object.select_all(action='DESELECT')
        for obj in original_selected:
            obj.select_set(True)
        if original_active:
            bpy.context.view_layer.objects.active = original_active
        return

    # Create or retrieve a vertex group for the target mesh
    target_vertex_group = None
    if vertex_group_name in target_mesh.vertex_groups:
        target_mesh.vertex_groups.remove(target_mesh.vertex_groups[vertex_group_name])
    target_vertex_group = target_mesh.vertex_groups.new(name=vertex_group_name)

    # Check the distance for each vertex of the target mesh
    found_vertices = []

    # Acquire post-evaluation data for the target mesh
    evaluated_target_mesh = target_mesh.evaluated_get(depsgraph)
    target_mesh_data = evaluated_target_mesh.data
    target_world_matrix = evaluated_target_mesh.matrix_world
    target_normal_matrix = evaluated_target_mesh.matrix_world.inverted().transposed()


    # Acceleration Using BVHTree
    print("Performing high-speed search using BVHTree...")
    import time
    start_time = time.time()

    # Construct a BVHTree from a triangulated base mesh
    temp_bm = bmesh.new()
    temp_bm.from_mesh(base_mesh_data)
    temp_bm.faces.ensure_lookup_table()
    temp_bm.verts.ensure_lookup_table()

    # Prepare the vertex coordinates and face indices for the target face
    vertices = []
    faces = []

    # Add all vertices (world coordinates)
    for vert in temp_bm.verts:
        world_vert = base_world_matrix @ vert.co
        vertices.append(world_vert)

    # Add only the target faces (all triangles)
    for face_idx in target_face_indices:
        face = temp_bm.faces[face_idx]
        face_indices = [v.index for v in face.verts]
        faces.append(face_indices)

    # Constructing a BVHTree
    if faces:  # Only when a face exists
        bvh = BVHTree.FromPolygons(vertices, faces)

        # Dictionary storing interpolation weights for each vertex
        vertex_interpolated_weights = {}

        for vertex_idx, vertex in enumerate(target_mesh_data.vertices):
            # Vertex world coordinates (using post-evaluation target mesh data)
            world_vertex_pos = target_world_matrix @ vertex.co

            nearest_point, normal, face_idx, distance = bvh.find_nearest(world_vertex_pos)

            if max_angle_degrees is not None:
                v = (world_vertex_pos - nearest_point).normalized()
                angle = math.degrees(math.acos(min(1.0, max(-1.0, v.dot(normal)))))
                if angle > max_angle_degrees:
                    vertex_interpolated_weights[vertex_idx] = 0.0
                    continue

            # Get the distance to the nearest face
            if nearest_point is not None and distance <= max_distance and face_idx is not None:
                found_vertices.append(vertex_idx)

                # Get the vertex indices that make up the face (all triangles)
                face_vertex_indices = faces[face_idx]

                # Retrieve the world coordinates of the vertices that make up the face
                face_vertices = [vertices[vi] for vi in face_vertex_indices]

                # Calculate the center of gravity coordinates of a triangle
                bary_coords = barycentric_coords_from_point(nearest_point, face_vertices[0], face_vertices[1], face_vertices[2])

                # Get the weights for each vertex in the base mesh vertex group
                weights = []
                for vi in face_vertex_indices:
                    base_vert = base_mesh_data.vertices[vi]
                    vert_weight = 0.0
                    for group_elem in base_vert.groups:
                        if group_elem.group == temp_base_vertex_group.index:
                            vert_weight = group_elem.weight
                            break
                    weights.append(vert_weight)

                # Interpolation using center-of-gravity coordinates
                interpolated_weight = (bary_coords[0] * weights[0] +
                                     bary_coords[1] * weights[1] +
                                     bary_coords[2] * weights[2])
                vertex_interpolated_weights[vertex_idx] = max(0.0, min(1.0, interpolated_weight))
            else:
                vertex_interpolated_weights[vertex_idx] = 0.0
    else:
        print("Warning: Target surface not found")
        # Temporarily delete the mesh
        bpy.data.objects.remove(temp_base_mesh, do_unlink=True)
        return

    # Release temporary bmesh
    temp_bm.free()

    end_time = time.time()
    print(f"BVHTree search completed: {end_time - start_time:.3f} seconds")

    # Set weights for the vertex groups of the target mesh
    for vertex_idx in range(len(target_mesh_data.vertices)):
        weight = vertex_interpolated_weights.get(vertex_idx, 0.0)
        target_vertex_group.add([vertex_idx], weight, 'REPLACE')

    bpy.ops.object.select_all(action='DESELECT')
    target_mesh.select_set(True)
    bpy.context.view_layer.objects.active = target_mesh

    # Switch to Edit mode and select all vertices
    bpy.ops.object.mode_set(mode='EDIT')
    bpy.ops.mesh.select_all(action='SELECT')

    # Select a group
    for i, group in enumerate(target_mesh.vertex_groups):
        target_mesh.vertex_groups.active_index = i
        if group.name == vertex_group_name:
            break

    bpy.ops.object.mode_set(mode='WEIGHT_PAINT')

    # Apply smoothing
    if smooth_repeat > 0:
        bpy.ops.object.vertex_group_smooth(factor=0.5, repeat=smooth_repeat, expand=0.5)
    bpy.ops.object.mode_set(mode='OBJECT')

    # Delete the temporarily created triangular mesh
    print(f"Deleting temporary mesh '{temp_base_mesh.name}'...")
    bpy.data.objects.remove(temp_base_mesh, do_unlink=True)

    # Restore to original state
    bpy.ops.object.select_all(action='DESELECT')
    for obj in original_selected:
        obj.select_set(True)
    if original_active:
        bpy.context.view_layer.objects.active = original_active
        if original_mode.startswith('EDIT'):
            bpy.ops.object.mode_set(mode='EDIT')

    print(f"Created vertex group: {vertex_group_name}")
    print(f"Number of vertices satisfying the conditions: {len(found_vertices)}")
    print(f"Maximum distance: {max_distance}")

def strip_numeric_suffix(bone_name: str) -> str:
    """
    Remove the '.number' pattern from the end of the bone name

    Parameters:
        bone_name: Bone Name

    Returns:
        str: '.number' has been removed from the bone name
    """
    return re.sub(r'\.[\d]+$', '', bone_name)

def is_left_side_bone(bone_name: str, humanoid_name: str = None) -> bool:
    """
    Determine whether the bone is on the left side

    Parameters:
        bone_name: Bone Name
        humanoid_name: Humanoid bone name (optional)

    Returns:
        bool: True for the left bone
    """
    # Humanoid Bone Name Check
    if humanoid_name and any(k in humanoid_name for k in ["Left", "left"]):
        return True

    # Remove the trailing digits
    cleaned_name = strip_numeric_suffix(bone_name)

    # Determination by bone name
    if any(k in cleaned_name for k in ["Left", "left"]):
        return True

    # End-of-line determination (including cases where spaces are present)
    suffixes = ["_L", ".L", " L"]
    return any(cleaned_name.endswith(suffix) for suffix in suffixes)

def is_right_side_bone(bone_name: str, humanoid_name: str = None) -> bool:
    """
    Determine whether the bone is on the right side

    Parameters:
        bone_name: Bone Name
        humanoid_name: Humanoid bone name (optional)

    Returns:
        bool: True for the right bone
    """
    # Humanoid Bone Name Check
    if humanoid_name and any(k in humanoid_name for k in ["Right", "right"]):
        return True

    # Remove the trailing digits
    cleaned_name = strip_numeric_suffix(bone_name)

    # Determination by bone name
    if any(k in cleaned_name for k in ["Right", "right"]):
        return True

    # End-of-line determination (including cases where spaces are present)
    suffixes = ["_R", ".R", " R"]
    return any(cleaned_name.endswith(suffix) for suffix in suffixes)

def duplicate_mesh_with_partial_weights(base_mesh: bpy.types.Object, base_avatar_data: dict) -> tuple:
    """
    Create a duplicate of the base mesh and separate the left and right half-body weights
    Returns: (Right-side mesh only, Left-side mesh)
    """
    # Classify left and right bones
    left_bones, right_bones = set(), set()

    # Legs, feet, toes, and chest bones grouped separately on the left and right sides
    leg_foot_chest_bones = {
        "LeftUpperLeg", "RightUpperLeg", "LeftLowerLeg", "RightLowerLeg",
        "LeftFoot", "RightFoot", "LeftToes", "RightToes", "LeftBreast", "RightBreast",
        "LeftFootThumbProximal", "LeftFootThumbIntermediate", "LeftFootThumbDistal",
        "LeftFootIndexProximal", "LeftFootIndexIntermediate", "LeftFootIndexDistal",
        "LeftFootMiddleProximal", "LeftFootMiddleIntermediate", "LeftFootMiddleDistal",
        "LeftFootRingProximal", "LeftFootRingIntermediate", "LeftFootRingDistal",
        "LeftFootLittleProximal", "LeftFootLittleIntermediate", "LeftFootLittleDistal",
        "RightFootThumbProximal", "RightFootThumbIntermediate", "RightFootThumbDistal",
        "RightFootIndexProximal", "RightFootIndexIntermediate", "RightFootIndexDistal",
        "RightFootMiddleProximal", "RightFootMiddleIntermediate", "RightFootMiddleDistal",
        "RightFootRingProximal", "RightFootRingIntermediate", "RightFootRingDistal",
        "RightFootLittleProximal", "RightFootLittleIntermediate", "RightFootLittleDistal"
    }

    # Finger bone to be placed in the right group
    right_group_fingers = {
        "LeftThumbProximal", "LeftThumbIntermediate", "LeftThumbDistal",
        "LeftMiddleProximal", "LeftMiddleIntermediate", "LeftMiddleDistal",
        "LeftLittleProximal", "LeftLittleIntermediate", "LeftLittleDistal",
        "RightThumbProximal", "RightThumbIntermediate", "RightThumbDistal",
        "RightMiddleProximal", "RightMiddleIntermediate", "RightMiddleDistal",
        "RightLittleProximal", "RightLittleIntermediate", "RightLittleDistal"
    }

    # Finger bone to be placed in the left group
    left_group_fingers = {
        "LeftIndexProximal", "LeftIndexIntermediate", "LeftIndexDistal",
        "LeftRingProximal", "LeftRingIntermediate", "LeftRingDistal",
        "RightIndexProximal", "RightIndexIntermediate", "RightIndexDistal",
        "RightRingProximal", "RightRingIntermediate", "RightRingDistal"
    }

    # Non-separating shoulder, arm, and hand bones
    excluded_bones = {
        "LeftShoulder", "RightShoulder", "LeftUpperArm", "RightUpperArm",
        "LeftLowerArm", "RightLowerArm", "LeftHand", "RightHand"
    }

    for bone_map in base_avatar_data.get("humanoidBones", []):
        bone_name = bone_map["boneName"]
        humanoid_name = bone_map["humanoidBoneName"]

        if humanoid_name in excluded_bones:
            # Do not separate
            continue
        elif humanoid_name in leg_foot_chest_bones:
            # Legs, feet, toes, and chest will continue to be separated by left and right sides
            if any(k in humanoid_name for k in ["Left", "left"]):
                left_bones.add(bone_name)
            elif any(k in humanoid_name for k in ["Right", "right"]):
                right_bones.add(bone_name)
        elif humanoid_name in right_group_fingers:
            # Finger bone to be placed in the right group
            right_bones.add(bone_name)
        elif humanoid_name in left_group_fingers:
            # Finger bone to be placed in the left group
            left_bones.add(bone_name)

    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        humanoid_name = aux_set["humanoidBoneName"]
        for aux_bone in aux_set["auxiliaryBones"]:
            if humanoid_name in excluded_bones:
                # Do not separate
                continue
            elif humanoid_name in leg_foot_chest_bones:
                # Legs, feet, toes, and chest will continue to be separated by left and right sides
                if is_left_side_bone(aux_bone, humanoid_name):
                    left_bones.add(aux_bone)
                elif is_right_side_bone(aux_bone, humanoid_name):
                    right_bones.add(aux_bone)
            elif humanoid_name in right_group_fingers:
                # Finger bone to be placed in the right group
                right_bones.add(aux_bone)
            elif humanoid_name in left_group_fingers:
                # Finger bone to be placed in the left group
                left_bones.add(aux_bone)

    # Duplicate Mesh (Standard Version)
    right_mesh = base_mesh.copy()
    right_mesh.data = base_mesh.data.copy()
    right_mesh.name = base_mesh.name + ".RightOnly"
    bpy.context.scene.collection.objects.link(right_mesh)

    left_mesh = base_mesh.copy()
    left_mesh.data = base_mesh.data.copy()
    left_mesh.name = base_mesh.name + ".LeftOnly"
    bpy.context.scene.collection.objects.link(left_mesh)

    left_base_mesh_armature_settings = store_armature_modifier_settings(left_mesh)
    right_base_mesh_armature_settings = store_armature_modifier_settings(right_mesh)
    apply_modifiers_keep_shapekeys_with_temp(left_mesh)
    apply_modifiers_keep_shapekeys_with_temp(right_mesh)
    restore_armature_modifier(left_mesh, left_base_mesh_armature_settings)
    restore_armature_modifier(right_mesh, right_base_mesh_armature_settings)
    set_armature_modifier_visibility(left_mesh, False, False)
    set_armature_modifier_visibility(right_mesh, False, False)

    print(f"left_bones: {left_bones}")
    print(f"right_bones: {right_bones}")

    # Standard Edition Processing
    # Delete the left and right vertex groups
    for bone_name in left_bones:
        if bone_name in right_mesh.vertex_groups:
            right_mesh.vertex_groups.remove(right_mesh.vertex_groups[bone_name])

    for bone_name in right_bones:
        if bone_name in left_mesh.vertex_groups:
            left_mesh.vertex_groups.remove(left_mesh.vertex_groups[bone_name])

    return right_mesh, left_mesh

def find_containing_objects(clothing_meshes, threshold=0.02):
    """
    Find pairs where one object contains all other objects
    When contained within multiple objects, it is contained only within the one with the smallest average distance

    Parameters:
        clothing_meshes: List of mesh objects to check
        threshold: Distance threshold

    Returns:
        dict: A dictionary where the keys are the containing objects and the values are lists of contained objects
    """
    # A dictionary that tracks the average distance between vertices
    average_distances = {}  # {(container, contained): average_distance}

    # Check each object pair
    for i, obj1 in enumerate(clothing_meshes):
        for j, obj2 in enumerate(clothing_meshes):
            if i == j:  # Skip duplicate objects
                continue

            # Acquire evaluated mesh for distance calculation
            depsgraph = bpy.context.evaluated_depsgraph_get()

            eval_obj1 = obj1.evaluated_get(depsgraph)
            eval_mesh1 = eval_obj1.data

            eval_obj2 = obj2.evaluated_get(depsgraph)
            eval_mesh2 = eval_obj2.data

            # Construct a BVH tree
            bm1 = bmesh.new()
            bm1.from_mesh(eval_mesh1)
            bm1.transform(obj1.matrix_world)
            bvh_tree1 = BVHTree.FromBMesh(bm1)

            # Flag indicating whether all vertices are within the threshold and the sum of distances
            all_within_threshold = True
            total_distance = 0.0
            vertex_count = 0

            # For each vertex of the second object, find the distance to the nearest face
            for vert in eval_mesh2.vertices:
                # Calculate the world coordinates of the vertex
                vert_world = obj2.matrix_world @ vert.co

                # Exploring the Closest Point and Distance
                nearest = bvh_tree1.find_nearest(vert_world)

                if nearest is None:
                    all_within_threshold = False
                    break

                # Distance is the fourth element (index 3)
                distance = nearest[3]
                total_distance += distance
                vertex_count += 1

                if distance > threshold:
                    all_within_threshold = False
                    break

            # If all vertices are within the threshold, record the average distance
            if all_within_threshold and vertex_count > 0:
                average_distance = total_distance / vertex_count
                average_distances[(obj1, obj2)] = average_distance

            bm1.free()

    # Select the container with the smallest average distance
    best_containers = {}  # {contained: (container, avg_distance)}

    for (container, contained), avg_distance in average_distances.items():
        if contained not in best_containers or avg_distance < best_containers[contained][1]:
            best_containers[contained] = (container, avg_distance)

    # Build the dictionary of results
    containing_objects = {}

    for contained, (container, _) in best_containers.items():
        if container not in containing_objects:
            containing_objects[container] = []
        containing_objects[container].append(contained)

    if not containing_objects:
        return {}

    # Integrate nested relationships so that each object appears only once
    parent_map = {}
    for container, contained_list in containing_objects.items():
        for child in contained_list:
            parent_map[child] = container

    def get_bounding_box_volume(obj):
        try:
            dims = getattr(obj, "dimensions", None)
            if dims is None:
                return 0.0
            return float(dims[0]) * float(dims[1]) * float(dims[2])
        except Exception:
            return 0.0

    def find_root(obj):
        visited_list = []
        visited_set = set()
        current = obj

        while current in parent_map and current not in visited_set:
            visited_list.append(current)
            visited_set.add(current)
            current = parent_map[current]

        if current in visited_set:
            cycle_start = visited_list.index(current)
            cycle_nodes = visited_list[cycle_start:]
            root = max(
                cycle_nodes,
                key=lambda o: (
                    get_bounding_box_volume(o),
                    getattr(o, "name", str(id(o)))
                )
            )
        else:
            root = current

        for node in visited_list:
            parent_map[node] = root

        return root

    def collect_descendants(obj, visited):
        result = []
        for child in containing_objects.get(obj, []):
            if child in visited:
                continue
            visited.add(child)
            result.append(child)
            result.extend(collect_descendants(child, visited))
        return result

    merged_containing_objects = {}
    roots_in_order = []

    for container in containing_objects.keys():
        root = find_root(container)
        if root not in merged_containing_objects:
            merged_containing_objects[root] = []
            roots_in_order.append(root)

    assigned_objects = set()
    for root in roots_in_order:
        visited = {root}
        descendants = collect_descendants(root, visited)
        for child in descendants:
            if child in assigned_objects:
                continue
            merged_containing_objects[root].append(child)
            assigned_objects.add(child)

    for contained, (container, _) in best_containers.items():
        if contained in assigned_objects:
            continue
        root = find_root(container)
        if root not in merged_containing_objects:
            merged_containing_objects[root] = []
            roots_in_order.append(root)
        if contained == root:
            continue
        merged_containing_objects[root].append(contained)
        assigned_objects.add(contained)

    final_result = {root: merged_containing_objects[root] for root in roots_in_order if merged_containing_objects[root]}

    if final_result:
        seen_objects = set()
        duplicate_objects = set()

        for container, contained_list in final_result.items():
            if container in seen_objects:
                duplicate_objects.add(container)
            else:
                seen_objects.add(container)

            for obj in contained_list:
                if obj in seen_objects:
                    duplicate_objects.add(obj)
                else:
                    seen_objects.add(obj)

        if duplicate_objects:
            duplicate_names = sorted(
                {getattr(obj, "name", str(id(obj))) for obj in duplicate_objects}
            )
            print(
                "find_containing_objects: The same object was detected multiple times -> "
                + ", ".join(duplicate_names)
            )

    return final_result

def temporarily_merge_for_weight_transfer(container_obj, contained_objs, base_armature, base_avatar_data, clothing_avatar_data, field_path, clothing_armature, blend_shape_settings, cloth_metadata):
    """
    Temporarily merge objects, apply only weight transfer, then restore the results to the original objects

    Parameters:
        container_obj: Contained object
        contained_objs: List of contained objects
        base_armature: Base Armature
        base_avatar_data: Base Avatar Data
        clothing_avatar_data: Clothing avatar data
        field_path: Field path
        clothing_armature: Clothing Armature
        cloth_metadata: Cloth metadata
    """
    # Save the original data
    original_active = bpy.context.active_object
    original_mode = bpy.context.mode

    # Save the selection state of all objects
    original_selection = {obj: obj.select_get() for obj in bpy.data.objects}

    # Add all objects to the temporary list
    to_merge = [container_obj] + contained_objs

    # Save vertex group information
    vertex_groups_data = {}
    for obj in to_merge:
        vertex_groups_data[obj.name] = {}
        for vg in obj.vertex_groups:
            vg_data = []
            for v in obj.data.vertices:
                weight = 0.0
                for g in v.groups:
                    if g.group == vg.index:
                        weight = g.weight
                        break
                if weight > 0:
                    vg_data.append((v.index, weight))
            vertex_groups_data[obj.name][vg.name] = vg_data

    # Create duplicates of all objects
    duplicated_objs = []
    bpy.ops.object.select_all(action='DESELECT')

    for obj in to_merge:
        obj.select_set(True)
        bpy.context.view_layer.objects.active = obj
        bpy.ops.object.duplicate()
        dup_obj = bpy.context.active_object
        duplicated_objs.append(dup_obj)
        bpy.ops.object.select_all(action='DESELECT')

    # Merge duplicated objects
    bpy.ops.object.select_all(action='DESELECT')
    for obj in duplicated_objs:
        obj.select_set(True)

    bpy.context.view_layer.objects.active = duplicated_objs[0]
    bpy.ops.object.join()

    # Combined objects
    merged_obj = bpy.context.active_object
    merged_obj.name = f"TempMerged_{container_obj.name}"

    # Apply only weight transfer to the combined objects
    # process_weight_transfer(merged_obj, base_armature, base_avatar_data, field_path, clothing_armature, cloth_metadata)
    process_weight_transfer_with_component_normalization(merged_obj, base_armature, base_avatar_data, clothing_avatar_data, field_path, clothing_armature, blend_shape_settings, cloth_metadata)

    depsgraph = bpy.context.evaluated_depsgraph_get()

    # Get the source mesh after applying modifiers
    eval_merged_obj = merged_obj.evaluated_get(depsgraph)
    eval_merged_mesh = eval_merged_obj.data
    merged_world_coords = [merged_obj.matrix_world @ v.co for v in eval_merged_mesh.vertices]

    # Fastest search for the nearest vertex using KDTree
    kdtree = KDTree(len(merged_world_coords))
    for i, v_co in enumerate(merged_world_coords):
        kdtree.insert(v_co, i)
    kdtree.balance()

    # Restore vertex group information to the original object
    for obj in to_merge:
        # Clear existing vertex groups
        for vg in obj.vertex_groups[:]:
            obj.vertex_groups.remove(vg)

        # Create a new vertex group from the combined object
        for vg in merged_obj.vertex_groups:
            obj.vertex_groups.new(name=vg.name)

        # Retrieve evaluated vertex coordinates (current state)
        eval_obj = obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data
        obj_world_coords = [obj.matrix_world @ v.co for v in eval_mesh.vertices]

        # Find the closest vertex for each vertex of the original object and copy the weight
        for i, vert_co in enumerate(obj_world_coords):
            co, merged_vert_idx, dist = kdtree.find(vert_co)

            # Copy weight information from corresponding vertices within merged objects
            if merged_vert_idx >= 0:
                for g in merged_obj.data.vertices[merged_vert_idx].groups:
                    vg_name = merged_obj.vertex_groups[g.group].name
                    if vg_name in obj.vertex_groups:
                        obj.vertex_groups[vg_name].add([i], g.weight, 'REPLACE')

    # Delete temporary objects
    bpy.ops.object.select_all(action='DESELECT')
    merged_obj.select_set(True)
    bpy.ops.object.delete()

    # Restore original selection state
    for obj, was_selected in original_selection.items():
        if obj.name in bpy.data.objects:  # Verify that the object exists
            obj.select_set(was_selected)

    # Restore the original active object and original mode
    if original_active and original_active.name in bpy.data.objects:
        bpy.context.view_layer.objects.active = original_active

    if original_mode != 'OBJECT':
        bpy.ops.object.mode_set(mode=original_mode)

def group_components_by_weight_pattern(obj, base_avatar_data, clothing_armature):
    """
    Group connected components with the same weight pattern

    Parameters:
        obj: Mesh object to be processed
        base_avatar_data: Base Avatar Data

    Returns:
        dict: A dictionary with weight patterns as keys and lists of connected components as values
    """
    # Create a BMesh
    bm = bmesh.new()
    bm.from_mesh(obj.data)
    bm.verts.ensure_lookup_table()
    bm.edges.ensure_lookup_table()
    bm.faces.ensure_lookup_table()

    base_obj = bpy.data.objects.get("Body.BaseAvatar")
    if not base_obj:
        raise Exception("Base avatar mesh (Body.BaseAvatar) not found")

    # Get all concatenated components
    components = find_connected_components(obj)

    # Display the number of vertices for each component
    # for j, comp in enumerate(components):
    #     print(f"Component {j}: {len(comp)} vertices")

    # Get the vertex group to be checked
    target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)
    if clothing_armature:
        target_groups.update(bone.name for bone in clothing_armature.data.bones)

    # Extract only the target groups existing within the mesh
    existing_target_groups = {vg.name for vg in obj.vertex_groups if vg.name in target_groups}

    # Calculate the weight pattern for each component
    component_patterns = {}
    uniform_components = []

    if "Rigid2" not in obj.vertex_groups:
        obj.vertex_groups.new(name="Rigid2")
    rigid_group = obj.vertex_groups["Rigid2"]

    for component in components:
        # Collect the weight patterns for each vertex within the component
        vertex_weights = []
        for vert_idx in component:
            vert = obj.data.vertices[vert_idx]
            weights = {group: 0.0 for group in existing_target_groups}

            for g in vert.groups:
                group_name = obj.vertex_groups[g.group].name
                if group_name in existing_target_groups:
                    weights[group_name] = g.weight

            vertex_weights.append(weights)

        # If the vertex weight is empty, skip to the next component
        if not vertex_weights:
            continue

        # Check if the same weight pattern exists in all groups under review
        is_uniform = True
        first_weights = vertex_weights[0]

        for weights in vertex_weights[1:]:
            for group_name in existing_target_groups:
                if abs(weights[group_name] - first_weights[group_name]) >= 0.0001:
                    is_uniform = False
                    break
            if not is_uniform:
                break

        # Record only connected components with uniform weight patterns
        if is_uniform:
            # Retrieve vertex coordinates from the evaluated mesh
            component_points = []
            for idx in component:
                if idx < len(bm.verts):
                    component_points.append(obj.matrix_world @ bm.verts[idx].co)

            # Check for intersections with the base mesh
            if len(component_points) >= 3:
                # Calculate OBB
                obb = calculate_obb_from_points(component_points)
                # If OBB cannot be calculated, skip
                if obb is not None:
                    # Check for intersections with the base mesh
                    if check_mesh_obb_intersection(base_obj, obb):
                        print(f"Component with {len(component)} vertices intersects with base mesh, excluding from rigid transfer")
                        continue

            uniform_components.append(component)

            # Convert weight patterns to a hashable format
            pattern_tuple = tuple(sorted((k, round(v, 4)) for k, v in first_weights.items() if v > 0))

            # Execute processing only if pattern_tuple is not empty
            if pattern_tuple:
                # Set the weight of the Rigid vertex group to 1 for all vertices of connected components with uniform weight patterns
                for vert_idx in component:
                    rigid_group.add([vert_idx], 1.0, 'REPLACE')

                if pattern_tuple not in component_patterns:
                    component_patterns[pattern_tuple] = []
                component_patterns[pattern_tuple].append(component)

    # Release BMesh
    bm.free()

    print(f"Found {len(components)} connected components in {obj.name}")
    print(f"Found {len(component_patterns)} uniform weight patterns in {obj.name}")

    # Display detailed information for each pattern for debugging purposes
    for i, (pattern, components_list) in enumerate(component_patterns.items()):
        total_vertices = sum(len(comp) for comp in components_list)
        print(f"Pattern {i}: {pattern}")
        print(f"  Components: {len(components_list)}, Total vertices: {total_vertices}")

        # Display the number of vertices for each component
        for j, comp in enumerate(components_list):
            print(f"    Component {j}: {len(comp)} vertices")

    return component_patterns

def calculate_obb_from_points(points):
    """
    Calculate Oriented Bounding Box (OBB) from point cloud

    Parameters:
        points: A list of point clouds (Vector type or tuple)

    Returns:
        dict: Dictionary containing OBB information
            'center': Center coordinates
            'axes': Primary axes (3x3 matrix, each column representing an axis)
            'radii': Radius in each axis direction
        or None: When calculation is not possible
    """

    # Return None if the point cloud is too sparse
    if len(points) < 3:
        print(f"Warning: Too few points ({len(points)} points). OBB calculation skipped.")
        return None

    try:
        # Convert point cloud to numpy array
        points_np = np.array([[p.x, p.y, p.z] for p in points])

        # Calculate the center of the point cloud
        center = np.mean(points_np, axis=0)

        # Move the center to the origin
        centered_points = points_np - center

        # Calculate the covariance matrix
        cov_matrix = np.cov(centered_points, rowvar=False)

        # Check the rank of a matrix
        if np.linalg.matrix_rank(cov_matrix) < 3:
            print("Warning: Insufficient rank of the covariance matrix. OBB calculation will be skipped.")
            return None

        # Calculate eigenvalues and eigenvectors
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

        # If the eigenvalue is very small, skip
        if np.any(np.abs(eigenvalues) < 1e-10):
            print("Warning: Eigenvalues are extremely small. OBB calculation will be skipped.")
            return None

        # Sort by Eigenvalue (Descending)
        idx = eigenvalues.argsort()[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]

        # Acquire the principal axis (as a column vector)
        axes = eigenvectors

        # Calculate the projection of points in each axis direction
        projections = np.abs(np.dot(centered_points, axes))

        # Use the maximum value in each axis direction as the radius
        radii = np.max(projections, axis=0)

        # Return the results as a dictionary
        return {
            'center': center,
            'axes': axes,
            'radii': radii
        }
    except Exception as e:
        print(f"An error occurred during OBB calculation: {e}")
        return None

def reset_bone_weights(target_obj, bone_groups):
    """Set the weight of the specified vertex group to 0"""
    for vert in target_obj.data.vertices:
        for group in target_obj.vertex_groups:
            if group.name in bone_groups:
                try:
                    group.add([vert.index], 0, 'REPLACE')
                except RuntimeError:
                    continue

def store_weights(target_obj, bone_groups_to_store):
    """Save vertex group weights"""
    weights = {}
    for vert in target_obj.data.vertices:
        weights[vert.index] = {}
        for group in target_obj.vertex_groups:
            if group.name in bone_groups_to_store:
                try:
                    for g in vert.groups:
                        if g.group == group.index:
                            weights[vert.index][group.name] = g.weight
                            break
                except RuntimeError:
                    continue
    return weights

def restore_weights(target_obj, stored_weights):
    """Restore saved weights"""
    for vert_idx, groups in stored_weights.items():
        for group_name, weight in groups.items():
            if group_name in target_obj.vertex_groups:
                target_obj.vertex_groups[group_name].add([vert_idx], weight, 'REPLACE')

def pre_process_a_pose_setup(target_obj, armature, base_avatar_data, clothing_avatar_data, clothing_armature, humanoid_to_bone):
    """
    Perform preprocessing for A-pose processing

    Parameters:
        target_obj: Mesh object to be processed
        armature: Armature object
        base_avatar_data: Base Avatar Data
        clothing_avatar_data: Clothing avatar data
        clothing_armature: Clothing Armature
        humanoid_to_bone: Humanoid Bone Mapping

    Returns:
        tuple: (non_humanoid_weights, temp_A_pose_shape_key_name)
    """
    global _is_A_pose
    non_humanoid_weights = {}
    temp_A_pose_shape_key_name = "temp_A_pose_shape_key"

    if _is_A_pose and armature and armature.type == 'ARMATURE':
        print("  Execute processing for A pose")

        all_bone_groups = set()
        for vertex_group in target_obj.vertex_groups:
            all_bone_groups.add(vertex_group.name)
        all_original_weights = store_weights(target_obj, all_bone_groups)

        # Processing vertex group weights for bones not included in the Humanoid bones or auxiliary bones
        print("  Begin weight processing for non-Humanoid/auxiliary bones")

        # Create a set of Humanoid bones and auxiliary bones
        humanoid_and_aux_bones = set()

        # Add Humanoid Bone
        for bone_map in base_avatar_data.get("humanoidBones", []):
            if "boneName" in bone_map:
                humanoid_and_aux_bones.add(bone_map["boneName"])

        # Add an auxiliary bone
        for aux_set in base_avatar_data.get("auxiliaryBones", []):
            for aux_bone in aux_set.get("auxiliaryBones", []):
                humanoid_and_aux_bones.add(aux_bone)

        # Create Humanoid bone mapping for clothing_armature
        clothing_bones_to_humanoid = {}
        for bone_map in clothing_avatar_data.get("humanoidBones", []):
            if "boneName" in bone_map and "humanoidBoneName" in bone_map:
                clothing_bones_to_humanoid[bone_map["boneName"]] = bone_map["humanoidBoneName"]

        # Set to record added vertex groups
        added_vertex_groups = set()

        # Check all vertex groups of target_obj
        for vertex_group in target_obj.vertex_groups:
            group_name = vertex_group.name

            # Skip if contained within a Humanoid bone or auxiliary bone
            if group_name in humanoid_and_aux_bones:
                continue

            # Trace the parent through the clothing_armature to find the Humanoid bone
            if clothing_armature and clothing_armature.type == 'ARMATURE':
                current_bone = clothing_armature.data.bones.get(group_name)
                target_humanoid_bone_name = None

                while current_bone and current_bone.parent:
                    parent_bone = current_bone.parent
                    if parent_bone.name in clothing_bones_to_humanoid and clothing_bones_to_humanoid[parent_bone.name] in humanoid_to_bone:
                        target_humanoid_bone_name = humanoid_to_bone[clothing_bones_to_humanoid[parent_bone.name]]
                        break
                    current_bone = parent_bone

                if target_humanoid_bone_name:
                    # Check if target_obj contains a vertex group for the Humanoid bone
                    target_group = target_obj.vertex_groups.get(target_humanoid_bone_name)
                    if not target_group:
                        # Create a vertex group
                        target_group = target_obj.vertex_groups.new(name=target_humanoid_bone_name)
                        added_vertex_groups.add(target_humanoid_bone_name)
                        print(f"    Add vertex group '{target_humanoid_bone_name}'")

                    # Transfer weight
                    source_group = vertex_group
                    if source_group:
                        # Transfer the weight of each vertex
                        for vertex in target_obj.data.vertices:
                            try:
                                source_weight = 0.0
                                for g in target_obj.data.vertices[vertex.index].groups:
                                    if g.group == source_group.index:
                                        source_weight = g.weight
                                        break
                                if source_weight > 0:
                                    # If an existing weight exists, add it
                                    try:
                                        existing_weight = 0.0
                                        for g in target_obj.data.vertices[vertex.index].groups:
                                            if g.group == target_group.index:
                                                existing_weight = g.weight
                                                break
                                        combined_weight = min(1.0, existing_weight + source_weight)
                                        target_group.add([vertex.index], combined_weight, 'REPLACE')
                                        if (target_humanoid_bone_name, vertex.index) not in non_humanoid_weights:
                                            non_humanoid_weights[(target_humanoid_bone_name, vertex.index)] = 0.0
                                        non_humanoid_weights[(target_humanoid_bone_name, vertex.index)] = non_humanoid_weights[(target_humanoid_bone_name, vertex.index)] + source_weight
                                    except RuntimeError:
                                        # If no existing weight exists, add a new one
                                        target_group.add([vertex.index], source_weight, 'ADD')
                            except RuntimeError:
                                # If the vertex does not belong to the source group
                                pass

                        print(f"    Weight Transfer: '{group_name}' -> '{target_humanoid_bone_name}'")

        set_armature_modifier_visibility(target_obj, True, True)

        # Apply Y-axis rotation to the LeftUpperArm and RightUpperArm bones
        print("  Apply Y-axis rotation to the LeftUpperArm and RightUpperArm bones")
        bpy.context.view_layer.objects.active = armature
        bpy.ops.object.mode_set(mode='POSE')

        # Get the boneName for LeftUpperArm and RightUpperArm from humanoidBones
        left_upper_arm_bone = None
        right_upper_arm_bone = None

        for bone_map in base_avatar_data.get("humanoidBones", []):
            if bone_map.get("humanoidBoneName") == "LeftUpperArm":
                left_upper_arm_bone = bone_map.get("boneName")
            elif bone_map.get("humanoidBoneName") == "RightUpperArm":
                right_upper_arm_bone = bone_map.get("boneName")

        # Apply a 45-degree rotation around the Y-axis to the LeftUpperArm bone
        if left_upper_arm_bone and left_upper_arm_bone in armature.pose.bones:
            bone = armature.pose.bones[left_upper_arm_bone]
            current_world_matrix = armature.matrix_world @ bone.matrix
            # Apply a 45-degree rotation around the Y-axis in the global coordinate system
            head_world_transformed = armature.matrix_world @ bone.head
            offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
            rotation_matrix = mathutils.Matrix.Rotation(math.radians(45), 4, 'Y')
            bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        # Apply a -45-degree Y-axis rotation to the RightUpperArm bone
        if right_upper_arm_bone and right_upper_arm_bone in armature.pose.bones:
            bone = armature.pose.bones[right_upper_arm_bone]
            current_world_matrix = armature.matrix_world @ bone.matrix
            # Apply a -45-degree rotation around the Y-axis in the global coordinate system
            head_world_transformed = armature.matrix_world @ bone.head
            offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
            rotation_matrix = mathutils.Matrix.Rotation(math.radians(-45), 4, 'Y')
            bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        bpy.ops.object.mode_set(mode='OBJECT')
        bpy.context.view_layer.objects.active = target_obj
        bpy.context.view_layer.update()

        shape_key_state = save_shape_key_state(target_obj)
        for key_block in target_obj.data.shape_keys.key_blocks:
            key_block.value = 0.0

        # Create a temporary shape key
        if target_obj.data.shape_keys is None:
            target_obj.shape_key_add(name='Basis')
        if target_obj.data.shape_keys and temp_A_pose_shape_key_name in target_obj.data.shape_keys.key_blocks:
            temp_A_pose_shape_key = target_obj.data.shape_keys.key_blocks[temp_A_pose_shape_key_name]
        else:
            temp_A_pose_shape_key = target_obj.shape_key_add(name=temp_A_pose_shape_key_name)

        #Retrieve the current evaluated mesh and save the state after armature deformation
        depsgraph = bpy.context.evaluated_depsgraph_get()
        eval_obj = target_obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data
        for i, vert in enumerate(eval_mesh.vertices):
            temp_A_pose_shape_key.data[i].co = vert.co.copy()
        set_armature_modifier_visibility(target_obj, False, False)
        restore_shape_key_state(target_obj, shape_key_state)
        temp_A_pose_shape_key.value = 1.0

        # Delete the added vertex group
        if added_vertex_groups:
            print("  Deleting the added vertex group...")
            for group_name in added_vertex_groups:
                if group_name in target_obj.vertex_groups:
                    target_obj.vertex_groups.remove(target_obj.vertex_groups[group_name])
                    print(f"    Delete vertex group '{group_name}'")

        reset_bone_weights(target_obj, all_bone_groups)
        restore_weights(target_obj, all_original_weights)

    return non_humanoid_weights, temp_A_pose_shape_key_name

def post_process_a_pose_cleanup(target_obj, armature, base_avatar_data, non_humanoid_weights, temp_A_pose_shape_key_name):
    """
    Perform post-processing for A-pose processing

    Parameters:
        target_obj: Mesh object to be processed
        armature: Armature object
        base_avatar_data: Base Avatar Data
        non_humanoid_weights: Non-Humanoid Weight Dictionary
        temp_A_pose_shape_key_name: Temporary shape key name
    """
    global _is_A_pose

    if _is_A_pose and armature and armature.type == 'ARMATURE':
        print("  Execute processing for A pose")
        set_armature_modifier_visibility(target_obj, True, True)

        all_bone_groups = set()
        for vertex_group in target_obj.vertex_groups:
            all_bone_groups.add(vertex_group.name)
        all_original_weights = store_weights(target_obj, all_bone_groups)

        for (bone_name, vertex_index), weight in non_humanoid_weights.items():
            if bone_name not in target_obj.vertex_groups:
                target_obj.vertex_groups.new(name=bone_name)
            group_index = target_obj.vertex_groups[bone_name].index
            source_weight = 0.0
            for g in target_obj.data.vertices[vertex_index].groups:
                if g.group == group_index:
                    source_weight = g.weight
                    break
            combined_weight = min(1.0, source_weight + weight)
            target_obj.vertex_groups[bone_name].add([vertex_index], combined_weight, 'REPLACE')

        # Apply Y-axis reverse rotation to the LeftUpperArm and RightUpperArm bones
        print("  Apply Y-axis reverse rotation to the LeftUpperArm and RightUpperArm bones")
        bpy.context.view_layer.objects.active = armature
        bpy.ops.object.mode_set(mode='POSE')

        # Get the boneName for LeftUpperArm and RightUpperArm from humanoidBones
        left_upper_arm_bone = None
        right_upper_arm_bone = None

        for bone_map in base_avatar_data.get("humanoidBones", []):
            if bone_map.get("humanoidBoneName") == "LeftUpperArm":
                left_upper_arm_bone = bone_map.get("boneName")
            elif bone_map.get("humanoidBoneName") == "RightUpperArm":
                right_upper_arm_bone = bone_map.get("boneName")

        # Apply a -90-degree rotation to the Y-axis of the LeftUpperArm bone
        if left_upper_arm_bone and left_upper_arm_bone in armature.pose.bones:
            bone = armature.pose.bones[left_upper_arm_bone]
            current_world_matrix = armature.matrix_world @ bone.matrix
            # Apply a -90-degree rotation around the Y-axis in the global coordinate system
            head_world_transformed = armature.matrix_world @ bone.head
            offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
            rotation_matrix = mathutils.Matrix.Rotation(math.radians(-90), 4, 'Y')
            bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        # Apply a 90-degree rotation around the Y-axis to the RightUpperArm bone
        if right_upper_arm_bone and right_upper_arm_bone in armature.pose.bones:
            bone = armature.pose.bones[right_upper_arm_bone]
            current_world_matrix = armature.matrix_world @ bone.matrix
            # Apply a 90-degree rotation around the Y-axis in the global coordinate system
            head_world_transformed = armature.matrix_world @ bone.head
            offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
            rotation_matrix = mathutils.Matrix.Rotation(math.radians(90), 4, 'Y')
            bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        shape_key_state = save_shape_key_state(target_obj)
        for key_block in target_obj.data.shape_keys.key_blocks:
            key_block.value = 0.0
        if temp_A_pose_shape_key_name in target_obj.data.shape_keys.key_blocks:
            temp_A_pose_shape_key = target_obj.data.shape_keys.key_blocks[temp_A_pose_shape_key_name]
            temp_A_pose_shape_key.value = 1.0

        #Retrieve the current evaluated mesh and save the state after armature deformation
        depsgraph = bpy.context.evaluated_depsgraph_get()
        eval_obj = target_obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data
        # Get the shape key of the Basis
        basis_shape_key = target_obj.data.shape_keys.key_blocks["Basis"]
        for i, vert in enumerate(eval_mesh.vertices):
            basis_shape_key.data[i].co = vert.co.copy()

        # Apply a 45-degree rotation around the Y-axis to the LeftUpperArm bone
        if left_upper_arm_bone and left_upper_arm_bone in armature.pose.bones:
            bone = armature.pose.bones[left_upper_arm_bone]
            current_world_matrix = armature.matrix_world @ bone.matrix
            # Apply a 45-degree rotation around the Y-axis in the global coordinate system
            head_world_transformed = armature.matrix_world @ bone.head
            offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
            rotation_matrix = mathutils.Matrix.Rotation(math.radians(45), 4, 'Y')
            bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        # Apply a -45-degree Y-axis rotation to the RightUpperArm bone
        if right_upper_arm_bone and right_upper_arm_bone in armature.pose.bones:
            bone = armature.pose.bones[right_upper_arm_bone]
            current_world_matrix = armature.matrix_world @ bone.matrix
            # Apply a -45-degree rotation around the Y-axis in the global coordinate system
            head_world_transformed = armature.matrix_world @ bone.head
            offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
            rotation_matrix = mathutils.Matrix.Rotation(math.radians(-45), 4, 'Y')
            bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

        restore_shape_key_state(target_obj, shape_key_state)

        # Temporarily delete shape keys
        if temp_A_pose_shape_key_name in target_obj.data.shape_keys.key_blocks:
            temp_A_pose_shape_key = target_obj.data.shape_keys.key_blocks[temp_A_pose_shape_key_name]
            target_obj.shape_key_remove(temp_A_pose_shape_key)

        bpy.ops.object.mode_set(mode='OBJECT')
        bpy.context.view_layer.objects.active = target_obj
        bpy.context.view_layer.update()
        set_armature_modifier_visibility(target_obj, False, False)

        reset_bone_weights(target_obj, all_bone_groups)
        restore_weights(target_obj, all_original_weights)

def process_weight_transfer_with_component_normalization(target_obj, armature, base_avatar_data, clothing_avatar_data, field_path, clothing_armature, blend_shape_settings, cloth_metadata=None):
    """
    Perform weight transfer processing and normalize the weights for each connected component

    Parameters:
        target_obj: Mesh object to be processed
        armature: Armature object
        base_avatar_data: Base Avatar Data
        clothing_avatar_data: Clothing avatar data
        field_path: Field path
        clothing_armature: Clothing Armature
        cloth_metadata: Cloth metadata
    """
    import time
    start_total = time.time()

    print(f"Process_weight_transfer_with_component_normalization Started: {target_obj.name}")

    # Create humanoid_to_bone mapping
    humanoid_to_bone = {}
    for bone_map in base_avatar_data.get("humanoidBones", []):
        if "boneName" in bone_map and "humanoidBoneName" in bone_map:
            humanoid_to_bone[bone_map["humanoidBoneName"]] = bone_map["boneName"]

    # Acquire the base mesh
    start_time = time.time()
    base_obj = bpy.data.objects.get("Body.BaseAvatar")
    if not base_obj:
        raise Exception("Base avatar mesh (Body.BaseAvatar) not found")

    left_base_obj = bpy.data.objects["Body.BaseAvatar.LeftOnly"]
    right_base_obj = bpy.data.objects["Body.BaseAvatar.RightOnly"]

    print(f"Set blend_shape_settings: {blend_shape_settings}")
    if base_obj.data.shape_keys:
        for blend_shape_setting in blend_shape_settings:
            if blend_shape_setting['name'] in base_obj.data.shape_keys.key_blocks:
                base_obj.data.shape_keys.key_blocks[blend_shape_setting['name']].value = blend_shape_setting['value']
                left_base_obj.data.shape_keys.key_blocks[blend_shape_setting['name']].value = blend_shape_setting['value']
                right_base_obj.data.shape_keys.key_blocks[blend_shape_setting['name']].value = blend_shape_setting['value']
                print(f"Set {blend_shape_setting['name']} to {blend_shape_setting['value']}")

    # Retrieve the evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_target_obj = target_obj.evaluated_get(depsgraph)
    eval_mesh = eval_target_obj.data

    # Get the vertex group to be checked
    target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

    # Extract only the target groups existing within the mesh
    existing_target_groups = {vg.name for vg in target_obj.vertex_groups if vg.name in target_groups}
    print(f"Preparation time: {time.time() - start_time:.2f} seconds")

    # Group connected components with the same weight pattern before processing
    start_time = time.time()
    component_patterns = group_components_by_weight_pattern(target_obj, base_avatar_data, clothing_armature)
    print(f"Component pattern extraction time: {time.time() - start_time:.2f} seconds")

    # Save the weight pattern for each vertex before processing
    start_time = time.time()
    original_vertex_weights = {}
    for vert_idx, vert in enumerate(target_obj.data.vertices):
        weights = {}
        for group_name in existing_target_groups:
            weight = 0.0
            for g in vert.groups:
                if target_obj.vertex_groups[g.group].name == group_name:
                    weight = g.weight
                    break
            if weight > 0.0001:
                weights[group_name] = weight
        original_vertex_weights[vert_idx] = weights
    print(f"Original weight save time: {time.time() - start_time:.2f} seconds")

    # Perform the standard weight transfer process
    start_time = time.time()
    process_weight_transfer(target_obj, armature, base_avatar_data, clothing_avatar_data, field_path, clothing_armature, cloth_metadata)
    print(f"Normal weight transfer processing time: {time.time() - start_time:.2f} seconds")

    start_time = time.time()
    new_component_patterns = {}

    # Processing for each pattern group
    for pattern, components in component_patterns.items():

        # If the pattern contains only groups not included in existing_target_groups
        if not any(group in existing_target_groups for group in pattern):
            all_deform_groups = set(existing_target_groups)
            if clothing_armature:
                all_deform_groups.update(bone.name for bone in clothing_armature.data.bones)
            # Check if the NonHumanoidDifference group's weight exists, and retrieve the vertex with the maximum weight
            non_humanoid_difference_group = target_obj.vertex_groups.get("NonHumanoidDifference")
            is_non_humanoid_difference_group = False
            max_weight = 0.0
            if non_humanoid_difference_group:
                for component in components:
                    for vert_idx in component:
                        vert = target_obj.data.vertices[vert_idx]
                        for g in vert.groups:
                            if g.group == non_humanoid_difference_group.index and g.weight > 0.0001:
                                is_non_humanoid_difference_group = True
                                if g.weight > max_weight:
                                    max_weight = g.weight
            # If a weight exists in the NonHumanoidDifference group, apply the average weight of the vertex with the highest weight to all other vertices
            if is_non_humanoid_difference_group:
                max_avg_pattern = {}
                count = 0
                for component in components:
                    for vert_idx in component:
                        vert = target_obj.data.vertices[vert_idx]
                        for g in vert.groups:
                            if g.group == non_humanoid_difference_group.index and g.weight == max_weight:
                                for g2 in vert.groups:
                                    if target_obj.vertex_groups[g2.group].name in all_deform_groups:
                                        if g2.group not in max_avg_pattern:
                                            max_avg_pattern[g2.group] = g2.weight
                                        else:
                                            max_avg_pattern[g2.group] += g2.weight
                                count += 1
                                break
                if count > 0:
                    for group_name, weight in max_avg_pattern.items():
                        max_avg_pattern[group_name] = weight / count
                for component in components:
                    for vert_idx in component:
                        vert = target_obj.data.vertices[vert_idx]
                        for g in vert.groups:
                            if g.group not in max_avg_pattern and target_obj.vertex_groups[g.group].name in all_deform_groups:
                                g.weight = 0.0
                        for max_group_id, max_weight in max_avg_pattern.items():
                            group = target_obj.vertex_groups[max_group_id]
                            group.add([vert_idx], max_weight, 'REPLACE')
            continue

        # Extract only the groups contained in existing_target_groups from pattern
        original_pattern_dict = {}
        for group_name, weight in pattern:
            original_pattern_dict[group_name] = weight
        original_pattern = tuple(sorted((k, v) for k, v in original_pattern_dict.items() if k in existing_target_groups))

        # Collect the weights of all vertices within each group
        all_weights = {group: [] for group in existing_target_groups}
        all_vertices = set()

        for component in components:
            for vert_idx in component:
                all_vertices.add(vert_idx)
                vert = target_obj.data.vertices[vert_idx]

                for group_name in existing_target_groups:
                    weight = 0.0
                    for g in vert.groups:
                        if target_obj.vertex_groups[g.group].name == group_name:
                            weight = g.weight
                            break
                    all_weights[group_name].append(weight)

        # Calculate the average weight for each group
        avg_weights = {}
        for group_name, weights in all_weights.items():
            if weights:
                avg_weights[group_name] = sum(weights) / len(weights)
            else:
                avg_weights[group_name] = 0.0

        # Apply an average weight to all vertices
        for vert_idx in all_vertices:
            for group_name, avg_weight in avg_weights.items():
                group = target_obj.vertex_groups[group_name]
                if avg_weight > 0.0001:
                    group.add([vert_idx], avg_weight, 'REPLACE')
                else:
                    group.add([vert_idx], 0.0, 'REPLACE')

        # Update the pattern in component_patterns
        new_pattern = tuple(sorted((k, round(v, 4)) for k, v in avg_weights.items() if v > 0.0001))
        new_component_patterns[(new_pattern, original_pattern)] = components

    component_patterns = new_component_patterns
    print(f"Component pattern normalization time: {time.time() - start_time:.2f} seconds")

    # Calculate the OBB of vertices included in the component pattern and process affecting surrounding vertices
    if component_patterns:
        # OBB Data Collection
        start_time = time.time()
        # Get the evaluated mesh in Object Mode
        bpy.ops.object.mode_set(mode='OBJECT')
        bpy.ops.object.select_all(action='DESELECT')
        target_obj.select_set(True)
        bpy.context.view_layer.objects.active = target_obj

        depsgraph = bpy.context.evaluated_depsgraph_get()
        eval_obj = target_obj.evaluated_get(depsgraph)
        eval_mesh = eval_obj.data

        # Safety Check: Verify that the evaluated mesh is not empty
        if len(eval_mesh.vertices) == 0:
            print(f"Warning: {target_obj.name} has no vertices in its evaluated mesh. OBB calculation will be skipped.")
            return

        # Collect the necessary data before entering EDIT mode
        obb_data = []

        all_rigid_component_vertices = set()
        for (new_pattern, original_pattern), components in component_patterns.items():
            # Collect all vertices within the component
            for component in components:
                all_rigid_component_vertices.update(component)

        component_count = 0
        # Processing for each pattern's components
        for (new_pattern, original_pattern), components in component_patterns.items():
            # Convert the weight information for the new pattern into a dictionary
            pattern_weights = {}
            for group_name, weight in new_pattern:
                pattern_weights[group_name] = weight

            # Convert weight information for original patterns into a dictionary
            original_pattern_weights = {}
            for group_name, weight in original_pattern:
                original_pattern_weights[group_name] = weight

            # Collect the vertices of all components sharing the same pattern
            all_component_vertices = set()
            for component in components:
                all_component_vertices.update(component)

            # Retrieve the vertex coordinates and size information for each component
            component_coords = {}
            component_sizes = {}

            for component_idx, component in enumerate(components):
                coords = []
                for vert_idx in component:
                    if vert_idx < len(eval_mesh.vertices):
                        coords.append(eval_obj.matrix_world @ eval_mesh.vertices[vert_idx].co)

                if coords:
                    component_coords[component_idx] = coords

                    # Calculate the component size (maximum distance or bounding box size)
                    size = calculate_component_size(coords)
                    component_sizes[component_idx] = size

            # Skip empty components
            if not component_coords:
                continue

            # Clustering based on the distance between components
            # Determining thresholds adaptively based on size
            clusters = cluster_components_by_adaptive_distance(component_coords, component_sizes)

            # Calculate OBB for each cluster
            for cluster_idx, cluster in enumerate(clusters):
                # Collect all vertex coordinates within the cluster
                cluster_vertices = set()
                cluster_coords = []

                for comp_idx in cluster:
                    for vert_idx in components[comp_idx]:
                        cluster_vertices.add(vert_idx)
                        if vert_idx < len(eval_mesh.vertices):
                            cluster_coords.append(eval_obj.matrix_world @ eval_mesh.vertices[vert_idx].co)

                # If there are too few vertices, skip
                if len(cluster_coords) < 3:
                    print(f"Warning: Cluster {cluster_idx} of pattern {pattern} has too few valid vertices ({len(cluster_coords)} points). Skipping.")
                    continue

                # Calculate OBB
                obb = calculate_obb_from_points(cluster_coords)

                # If OBB calculation fails, skip
                if obb is None:
                    print(f"Warning: Failed to compute OBB for cluster {cluster_idx} of pattern {pattern}. Skipping.")
                    continue

                # Expand OBB by 20%
                obb['radii'] = [radius * 1.3 for radius in obb['radii']]

                # Save data for vertex selection
                vertices_in_obb = []
                for vert_idx, vert in enumerate(target_obj.data.vertices):
                    if vert_idx not in all_rigid_component_vertices and vert_idx < len(eval_mesh.vertices):
                        try:
                            # World coordinates of evaluated vertices
                            vert_world = eval_obj.matrix_world @ eval_mesh.vertices[vert_idx].co

                            # Relative position from the center of the OBB
                            relative_pos = vert_world - Vector(obb['center'])

                            # Projection along each axis of the OBB
                            projections = [abs(relative_pos.dot(Vector(obb['axes'][:, i]))) for i in range(3)]

                            # If the projection is within the radius on all axes, it is inside the OBB
                            if all(proj <= radius for proj, radius in zip(projections, obb['radii'])):
                                vertices_in_obb.append(vert_idx)
                        except Exception as e:
                            print(f"Warning: Error occurred during OBB check for vertex {vert_idx}: {e}")
                            continue

                if not vertices_in_obb:
                    print(f"Warning: No vertices found within the OBB of cluster {cluster_idx} for pattern {pattern}. Skipping.")
                    continue

                obb_data.append({
                    'component_vertices': cluster_vertices,
                    'vertices_in_obb': vertices_in_obb,
                    'component_id': component_count,
                    'pattern_weights': pattern_weights,
                    'original_pattern_weights': original_pattern_weights
                })

                component_count += 1
        print(f"OBB data collection time: {time.time() - start_time:.2f} seconds")

        # If OBB data is not present, skip processing
        if not obb_data:
            print("Warning: No valid OBB data found. Skipping processing.")
            return

        start_time = time.time()
        #vert_neighbors = create_vertex_neighbors_list(target_obj, expand_distance=0.04, sigma=0.02)
        neighbors_info, offsets, num_verts = create_vertex_neighbors_array(target_obj, expand_distance=0.02, sigma=0.00659)
        print(f"Vertex neighborhood list creation time: {time.time() - start_time:.2f} seconds")

        # OBB processing start
        start_time = time.time()
        # Enter edit mode
        bpy.ops.object.mode_set(mode='EDIT')

        # Processing for each OBB data
        for obb_idx, data in enumerate(obb_data):
            obb_start = time.time()
            # Create or retrieve a "Connected" vertex group
            connected_group = target_obj.vertex_groups.new(name=f"Connected_{data['component_id']}")
            print(f"    Connected Vertex Group Creation: {connected_group.name}")

            # Clear all selections
            bpy.ops.mesh.select_all(action='DESELECT')

            # Select vertices using BMesh
            bm = bmesh.from_edit_mesh(target_obj.data)
            bm.verts.ensure_lookup_table()

            # Select a vertex within the OBB
            obb_vertex_select_start = time.time()
            for vert_idx in data['vertices_in_obb']:
                if vert_idx < len(bm.verts):
                    bm.verts[vert_idx].select = True

            # Apply BMesh changes to the mesh
            bmesh.update_edit_mesh(target_obj.data)
            print(f"    OBB vertex selection time: {time.time() - obb_vertex_select_start:.2f} seconds")

            # Detect edge loops contained within the selected vertices
            # Save current selection
            edge_loop_start = time.time()
            initial_selection = {v.index for v in bm.verts if v.select}

            if initial_selection:
                # Get the edges composed of the selected vertices
                selected_edges = [e for e in bm.edges if all(v.select for v in e.verts)]

                # Record a closed edge loop that is fully contained
                complete_loops = set()

                # Perform loop selection for each edge
                edge_count = len(selected_edges)
                print(f"    Number of edges processed: {edge_count}")

                for edge_idx, edge in enumerate(selected_edges):
                    if edge_idx % 100 == 0 and edge_idx > 0:
                        print(f"    Edge processing progress: {edge_idx}/{edge_count} ({edge_idx/edge_count*100:.1f}%)")

                    # Clear current selection
                    bpy.ops.mesh.select_all(action='DESELECT')

                    # Select the edge
                    edge.select = True
                    bmesh.update_edit_mesh(target_obj.data)

                    # Select edge loop
                    bpy.ops.mesh.loop_multi_select(ring=False)

                    # Get the vertices and edges of the selected loop
                    bm = bmesh.from_edit_mesh(target_obj.data)
                    loop_verts = {v.index for v in bm.verts if v.select}

                    # Verify that the loop is closed (each vertex is connected to exactly two selected edges)
                    is_closed_loop = True
                    for v in bm.verts:
                        if v.select:
                            # Count the number of selected edges connected to the selected vertex
                            selected_edge_count = sum(1 for e in v.link_edges if e.select)
                            # Count the total number of edges connected to the selected vertex
                            total_edge_count = len(v.link_edges)
                            # The vertices contained within a loop are two vertices within the loop and two vertices outside the loop,
                            # It must be connected by a total of four vertices and edges
                            if selected_edge_count != 2 or total_edge_count != 4:
                                is_closed_loop = False
                                break

                    # Verify that the loop is closed and fully contained within the initial selection
                    # if is_closed_loop and loop_verts.issubset(initial_selection):
                    if is_closed_loop:
                        # Verify that the original weight pattern of the vertices within the loop is similar to the component pattern
                        pattern_check_start = time.time()
                        is_similar_pattern = True
                        pattern_weights = data['original_pattern_weights']

                        for vert_idx in loop_verts:
                            if vert_idx in original_vertex_weights:
                                orig_weights = original_vertex_weights[vert_idx]

                                # Check for similarity in weight patterns
                                similarity_score = 0.0
                                total_weight = 0.0

                                # For each group within the pattern
                                for group_name, pattern_weight in pattern_weights.items():
                                    orig_weight = orig_weights.get(group_name, 0.0)
                                    diff = abs(pattern_weight - orig_weight)
                                    similarity_score += diff
                                    total_weight += pattern_weight

                                # Normalize similarity scores (closer to 0 indicates greater similarity)
                                if total_weight > 0:
                                    normalized_score = similarity_score / total_weight
                                    # If the threshold is exceeded, it is determined that they are not similar
                                    if normalized_score > 0.05:  # The threshold is adjustable
                                        is_similar_pattern = False
                                        break

                        if is_similar_pattern:
                            complete_loops.update(loop_verts)

                # Clear all selections
                bpy.ops.mesh.select_all(action='DESELECT')

                # Select only closed loops
                bm = bmesh.from_edit_mesh(target_obj.data)
                for vert in bm.verts:
                    if vert.index in complete_loops:
                        vert.select = True

                bmesh.update_edit_mesh(target_obj.data)

            print(f"    Edge loop detection time: {time.time() - edge_loop_start:.2f} seconds")

            # Expand the selection
            select_more_start = time.time()
            for _ in range(1):
                bpy.ops.mesh.select_more()

            # Get the index of the selected vertex
            bm = bmesh.from_edit_mesh(target_obj.data)
            selected_verts = [v.index for v in bm.verts if v.select]
            print(f"    Selection expansion time: {time.time() - select_more_start:.2f} seconds")

            if len(selected_verts) == 0:
                print(f"Warning: No vertices found in OBB {obb_idx}. Skipping.")
                continue

            # Return to Object Mode
            mode_switch_start = time.time()
            bpy.ops.object.mode_set(mode='OBJECT')
            print(f"    Mode switch time: {time.time() - mode_switch_start:.2f} seconds")

            # Set the weight of the Connected vertex group for the selected vertex
            weight_assign_start = time.time()
            for vert_idx in selected_verts:
                if vert_idx not in data['component_vertices']:  # Exclude vertices within the component
                    connected_group.add([vert_idx], 1.0, 'REPLACE')
            print(f"    Weight assignment time: {time.time() - weight_assign_start:.2f} seconds")

            # Apply smoothing to the Connected group
            smoothing_start = time.time()
            bpy.ops.object.select_all(action='DESELECT')
            target_obj.select_set(True)
            bpy.context.view_layer.objects.active = target_obj

            # Select the Connected group
            for i, group in enumerate(target_obj.vertex_groups):
                target_obj.vertex_groups.active_index = i
                if group.name == f"Connected_{data['component_id']}":
                    break

            bpy.ops.object.mode_set(mode='WEIGHT_PAINT')

            # Apply smoothing
            smooth_op_start = time.time()
            bpy.ops.object.vertex_group_smooth(factor=0.5, repeat=3, expand=0.5)
            print(f"    Standard smoothing time: {time.time() - smooth_op_start:.2f} seconds")

            custom_smooth_start = time.time()
            #custom_max_vertex_group(target_obj, f"Connected_{data['component_id']}", vert_neighbors, repeat=1, weight_factor=1.0)
            custom_max_vertex_group_numpy(target_obj, f"Connected_{data['component_id']}", neighbors_info, offsets, num_verts, repeat=3, weight_factor=1.0)
            print(f"    Custom smoothing time: {time.time() - custom_smooth_start:.2f} seconds")

            bpy.ops.object.mode_set(mode='OBJECT')
            print(f"    Smoothing processing time: {time.time() - smoothing_start:.2f} seconds")

            # After smoothing, weights are decayed based on the difference between original_pattern and the original_vertex_weights of each vertex
            decay_start = time.time()
            connected_group = target_obj.vertex_groups[f"Connected_{data['component_id']}"]
            original_pattern_weights = data['original_pattern_weights']

            for vert_idx, vert in enumerate(target_obj.data.vertices):
                if vert_idx in data['component_vertices']:
                    connected_group.add([vert_idx], 0.0, 'REPLACE')
                    continue
                if vert_idx not in data['component_vertices'] and vert_idx in original_vertex_weights:  # Exclude vertices within the component
                    # Retrieve the original weight pattern
                    orig_weights = original_vertex_weights[vert_idx]

                    # Calculate the difference from the pattern
                    similarity_score = 0.0
                    total_weight = 0.0

                    orig_weight_dict = {}

                    # For each group within the pattern
                    for group_name, pattern_weight in original_pattern_weights.items():
                        orig_weight = orig_weights.get(group_name, 0.0)
                        diff = abs(pattern_weight - orig_weight)
                        similarity_score += diff
                        total_weight += pattern_weight
                        orig_weight_dict[group_name] = orig_weight

                    # Normalize similarity scores (closer to 0 indicates greater similarity)
                    if total_weight > 0:
                        normalized_score = similarity_score / total_weight
                        # Calculate the damping coefficient based on similarity (stronger damping for lower similarity)
                        decay_factor = 1.0 - min(normalized_score * 3.33333, 1.0)  # Up to 90% attenuation

                        # Get the weight of the Connected group
                        connected_weight = 0.0
                        for g in target_obj.data.vertices[vert_idx].groups:
                            if g.group == connected_group.index:
                                connected_weight = g.weight
                                break

                        # Apply the decayed weight
                        if normalized_score > 0.3:
                            connected_group.add([vert_idx], 0.0, 'REPLACE')
                        else:
                            connected_group.add([vert_idx], connected_weight * decay_factor, 'REPLACE')
                    else:
                        connected_group.add([vert_idx], 0.0, 'REPLACE')
            print(f"    Weight decay time: {time.time() - decay_start:.2f} seconds")

            print(f"  OBB {obb_idx+1}/{len(obb_data)} Processing time: {time.time() - obb_start:.2f} seconds")

            # Return to edit mode (for the next loop)
            if obb_idx < len(obb_data) - 1:
                bpy.ops.object.mode_set(mode='EDIT')

        bpy.ops.object.mode_set(mode='OBJECT')
        print(f"OBB processing time: {time.time() - start_time:.2f} seconds")

        # Weight synthesis start
        start_time = time.time()
        # Weights for each pattern are synthesized based on the weights of the Connected group
        # For vertices belonging to multiple Connected groups, calculate the weighted average
        connected_groups = [vg for vg in target_obj.vertex_groups if vg.name.startswith("Connected_")]

        if connected_groups:
            # Processing for each vertex
            for vert in target_obj.data.vertices:
                # Vertices within the component are skipped (already processed)
                skip = False
                for (new_pattern, original_pattern), components in component_patterns.items():
                    for component in components:
                        if vert.index in component:
                            skip = True
                            break
                    if skip:
                        break

                if skip:
                    continue

                # Collect the weights and patterns for each Connected group
                connected_weights = {}
                total_weight = 0.0

                for connected_group in connected_groups:
                    weight = 0.0
                    for g in vert.groups:
                        if g.group == connected_group.index:
                            weight = g.weight
                            break

                    if weight > 0:
                        # Extract component ID from group names
                        component_id = int(connected_group.name.split('_')[1])
                        # Find the corresponding pattern
                        for i, data in enumerate(obb_data):
                            if data['component_id'] == component_id:
                                # Convert from pattern weight to tuple format
                                pattern_tuple = tuple(sorted((k, v) for k, v in data['pattern_weights'].items() if v > 0.0001))
                                connected_weights[pattern_tuple] = weight
                                total_weight += weight
                                break

                # If the weight is 0, skip
                if total_weight <= 0:
                    continue

                # Combine the weights of each group
                combined_weights = {}

                for pattern, weight in connected_weights.items():
                    # Pattern Normalization
                    normalized_weight = weight / total_weight

                    # Extract group names and weight values from the pattern
                    for group_name, value in pattern:
                        if group_name not in combined_weights:
                            combined_weights[group_name] = 0.0
                        combined_weights[group_name] += value * normalized_weight

                factor = total_weight
                if total_weight > 1.0:
                    factor = 1.0

               # Save existing weight values
                existing_weights = {}
                for group_name in existing_target_groups:
                    if group_name in target_obj.vertex_groups:
                        group = target_obj.vertex_groups[group_name]
                        weight = 0.0
                        for g in vert.groups:
                            if g.group == group.index:
                                weight = g.weight
                                break
                        existing_weights[group_name] = weight

                new_weights = {}

                # Update existing weight values (based on factor decay)
                for group_name, weight in existing_weights.items():
                    if group_name in target_obj.vertex_groups and group_name in existing_target_groups:
                        group = target_obj.vertex_groups[group_name]
                        # Decay existing weights by a factor of (1-factor)
                        new_weights[group_name] = weight * (1.0 - factor)

                # Add the weight for each pattern
                for pattern, weight in connected_weights.items():
                    # Pattern Normalization
                    normalized_weight = weight / total_weight
                    if total_weight < 1.0:
                        normalized_weight = weight
                    # Extract group names and weight values from the pattern
                    for group_name, value in pattern:
                        if group_name in target_obj.vertex_groups and group_name in existing_target_groups:
                            group = target_obj.vertex_groups[group_name]
                            # Calculate the new weight value
                            compornent_weight = value * normalized_weight
                            # Update weight
                            new_weights[group_name] = new_weights[group_name] + compornent_weight

                for group_name, weight in new_weights.items():
                    if weight > 1.0:
                        weight = 1.0
                    group = target_obj.vertex_groups[group_name]
                    group.add([vert.index], weight, 'REPLACE')
        print(f"Weighting synthesis time: {time.time() - start_time:.2f} seconds")

    print(f"Total processing time: {time.time() - start_total:.2f} seconds")

def create_vertex_neighbors_array(obj, expand_distance=0.05, sigma=0.02):
    """
    Create a NumPy array containing the adjacent vertex information for each vertex

    Parameters:
        obj: Target mesh object
        expand_distance: Search range (in meters)
        sigma: standard deviation of the Gaussian function

    Returns:
        neighbors_info (np.ndarray): Flat array with shape = (M, 2)
                                    Each row is [neighbor_idx, weight_factor]
        offsets (np.ndarray): shape = (num_verts+1,)
                              The proximity data for vertex i is stored in neighbors_info[offsets[i]:offsets[i+1]]
        num_verts (int): Number of vertices
    """
    # Retrieve evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data

    num_verts = len(eval_mesh.vertices)

    # Get the world coordinates of the vertex
    world_coords = [eval_obj.matrix_world @ v.co for v in eval_mesh.vertices]

    # Build a KDTree
    kdtree = cKDTree(world_coords)

    # Gaussian function
    def gaussian(distance, sigma):
        return math.exp(-(distance**2) / (2 * sigma**2))

    # Create a list of neighboring vertices
    neighbors_list = [[] for _ in range(num_verts)]
    for vert_idx, vert_world in enumerate(world_coords):
        # Search for vertices within the range
        for idx in kdtree.query_ball_point(vert_world, expand_distance):
            if idx != vert_idx:
                dist = (world_coords[idx] - vert_world).length
                weight_factor = gaussian(dist, sigma)
                neighbors_list[vert_idx].append((idx, weight_factor))

    # Create flat arrays and offset arrays
    # offsets[i] represents the index where the adjacency array for the i-th vertex begins
    offsets = np.zeros(num_verts+1, dtype=np.int64)
    for i in range(num_verts):
        offsets[i+1] = offsets[i] + len(neighbors_list[i])

    flat_data = []
    for i in range(num_verts):
        flat_data.extend(neighbors_list[i])

    # (neighbor_idx, weight_factor) -> NumPy array
    neighbors_info = np.array(flat_data, dtype=np.float64)  # shape = (M, 2)
    # However, since neighbor_idx is an integer, cast it to int before using it later

    return neighbors_info, offsets, num_verts


def custom_max_vertex_group_numpy(obj, group_name, neighbors_info, offsets, num_verts,
                                  repeat=3, weight_factor=1.0):
    """
    High-Speed Implementation of Custom Smoothing (MAX-Based) Using NumPy

    Parameters:
        obj: Target mesh object
        group_name: Name of the vertex group to be smoothed
        neighbors_info: Flat array of vertex neighbors created by create_vertex_neighbors_array
        offsets: Offsets for each vertex created by create_vertex_neighbors_array
        num_verts: Number of vertices
        repeat: Number of smoothing iterations
        weight_factor: Coefficient multiplied by the maximum value from surrounding vertices
    """
    if group_name not in obj.vertex_groups:
        print(f"The vertex group '{group_name}' was not found")
        return

    group_index = obj.vertex_groups[group_name].index

    # Retrieve vertex weights as a NumPy array
    current_weights = np.zeros(num_verts, dtype=np.float64)
    for v in obj.data.vertices:
        w = 0.0
        for g in v.groups:
            if g.group == group_index:
                w = g.weight
                break
        current_weights[v.index] = w

    # Repeated smoothing
    for _ in range(repeat):
        new_weights = np.copy(current_weights)

        # For each vertex, take the maximum value of (weight * factor) for its neighboring vertices
        for vert_idx in range(num_verts):
            start = offsets[vert_idx]
            end = offsets[vert_idx+1]
            if start == end:
                # When there are no adjacent vertices
                continue

            # neighbors_info[start:end, 0] -> neighbor_idx (cast to int since it's float)
            neighbor_idx = neighbors_info[start:end, 0].astype(np.int64)
            dist_factors = neighbors_info[start:end, 1]  # weight_dist_factor

            # Multiply the weights of the surrounding vertices by a distance coefficient and find the maximum value
            local_max = np.max(current_weights[neighbor_idx] * dist_factors)

            # Apply the larger value compared to the current weight
            new_weights[vert_idx] = max(new_weights[vert_idx], local_max * weight_factor)

        current_weights = new_weights

    # Apply calculation results to vertex groups (batch write-back)
    vg = obj.vertex_groups[group_name]
    for vert_idx in range(num_verts):
        w = current_weights[vert_idx]
        if w > 1.0:
            w = 1.0
        vg.add([vert_idx], float(w), 'REPLACE')

def create_vertex_neighbors_list(obj, expand_distance=0.05, sigma=0.02):
    """
    Create a list of adjacent vertices for each vertex

    Parameters:
        obj: Target mesh object
        expand_distance: Search range (in meters)
        sigma: standard deviation of the Gaussian function

    Returns:
        list: List of neighboring vertices for each vertex
            vert_neighbors[vert_idx] = [(neighbor_idx, weight_factor), ... ]
            weight_factor is a weighting coefficient based on distances calculated using a Gaussian function
    """
    # Retrieve evaluated mesh
    depsgraph = bpy.context.evaluated_depsgraph_get()
    eval_obj = obj.evaluated_get(depsgraph)
    eval_mesh = eval_obj.data

    # Get the world coordinates of the vertex
    world_coords = [eval_obj.matrix_world @ v.co for v in eval_mesh.vertices]

    # Build a KDTree
    kdtree = cKDTree(world_coords)

    # Gaussian function
    def gaussian(distance, sigma):
        return math.exp(-(distance**2) / (2 * sigma**2))

    # Create a list of neighboring vertices
    vert_neighbors = [[] for _ in range(len(world_coords))]
    for vert_idx, vert_world in enumerate(world_coords):
        # Search for vertices within the range
        for idx in kdtree.query_ball_point(vert_world, expand_distance):
            if idx != vert_idx:
                dist = (world_coords[idx] - vert_world).length
                weight_factor = gaussian(dist, sigma)
                vert_neighbors[vert_idx].append((idx, weight_factor))

    return vert_neighbors

def custom_max_vertex_group(obj, group_name, vert_neighbors, repeat=3, weight_factor=1.0):
    """
    Custom Smoothing Using the Gaussian Function

    Parameters:
        obj: Target mesh object
        group_name: Name of the vertex group to smooth
        repeat: Number of repetitions
        weight_factor: Weight expansion factor
    """
    if group_name not in obj.vertex_groups:
        print(f"The vertex group {group_name} was not found")
        return

    # Get the vertex group index
    group_index = obj.vertex_groups[group_name].index

    # Get the current weight value
    current_weights = {}
    for vert_idx, vert in enumerate(obj.data.vertices):
        weight = 0.0
        for g in vert.groups:
            if g.group == group_index:
                weight = g.weight
                break
        current_weights[vert_idx] = weight

    # Repeat a specified number of times
    for _ in range(repeat):
        new_weights = current_weights.copy()

        # Processing for each vertex
        for vert_idx, vert in enumerate(obj.data.vertices):
            # Retrieve vertices within the specified distance
            nearby_verts = vert_neighbors[vert_idx]

            if not nearby_verts:
                continue

            weight_max = 0.0
            for idx, weight_dist_factor in nearby_verts:
                weight_max = max(weight_max, current_weights[idx] * weight_dist_factor)

            # We also consider the current vertex's weight
            current_vert_weight = current_weights[vert_idx]

            # Calculate new weight value (linear interpolation of current value and surrounding weighted averages)
            new_weights[vert_idx] = max(current_vert_weight, weight_max * weight_factor)

        # Set the new weight value as the current weight value
        current_weights = new_weights

    # Apply the final weight values to the vertex group
    group = obj.vertex_groups[group_name]
    for vert_idx, weight in current_weights.items():
        if weight > 1.0:
            weight = 1.0
        group.add([vert_idx], weight, 'REPLACE')

# Function to calculate weight pattern similarity
def calculate_weight_pattern_similarity(weights1, weights2):
    """
    Calculate the similarity between two weight patterns

    Parameters:
        weights1: First weight pattern {group_name: weight}
        weights2: Second weight pattern {group_name: weight}

    Returns:
        float: Similarity (0.0 to 1.0, 1.0 being an exact match)
    """
    # Retrieve the group present in both patterns
    all_groups = set(weights1.keys()) | set(weights2.keys())

    if not all_groups:
        return 0.0

    # Calculate the sum of the weight differences for each group
    total_diff = 0.0
    for group in all_groups:
        w1 = weights1.get(group, 0.0)
        w2 = weights2.get(group, 0.0)
        total_diff += abs(w1 - w2)

    # Normalization (dividing by the number of groups)
    normalized_diff = total_diff / len(all_groups)

    # Convert to similarity (the smaller the difference, the higher the similarity)
    similarity = 1.0 - min(normalized_diff, 1.0)

    return similarity

def calculate_component_size(coords):
    """
    Calculate the size of the component

    Parameters:
        coords: List of vertex coordinates

    Returns:
        float: Component size (diameter or length of the largest side)
    """
    if len(coords) < 2:
        return 0.0

    # Calculate the bounding box
    min_x = min(co.x for co in coords)
    max_x = max(co.x for co in coords)
    min_y = min(co.y for co in coords)
    max_y = max(co.y for co in coords)
    min_z = min(co.z for co in coords)
    max_z = max(co.z for co in coords)

    # Calculate the length of the diagonal of the bounding box
    diagonal = ((max_x - min_x)**2 + (max_y - min_y)**2 + (max_z - min_z)**2)**0.5

    return diagonal

def cluster_components_by_adaptive_distance(component_coords, component_sizes):
    """
    Cluster based on the distance between components (using an adaptive threshold based on size)

    Parameters:
        component_coords: A dictionary with component indices as keys and lists of vertex coordinates as values
        component_sizes: A dictionary with component indices as keys and sizes as values

    Returns:
        list: List of clusters (each cluster is a list of component indices)
    """
    if not component_coords:
        return []

    # Calculate the center point of each component
    centers = {}
    for comp_idx, coords in component_coords.items():
        if coords:
            center = Vector((0, 0, 0))
            for co in coords:
                center += co
            center /= len(coords)
            centers[comp_idx] = center

    # List of clusters (initially, each component is an independent cluster)
    clusters = [[comp_idx] for comp_idx in centers.keys()]

    # Calculate the average size of components
    if component_sizes:
        average_size = sum(component_sizes.values()) / len(component_sizes)
    else:
        average_size = 0.1  # Default value

    # Set the minimum threshold and maximum threshold
    min_threshold = 0.1
    max_threshold = 1.0

    # Merge the clusters
    merged = True
    while merged:
        merged = False

        # Check each cluster pair
        for i in range(len(clusters)):
            if i >= len(clusters):  # Safety Check When the Number of Clusters Changes
                break

            for j in range(i + 1, len(clusters)):
                if j >= len(clusters):  # Safety Check When the Number of Clusters Changes
                    break

                # Calculate the minimum distance between components within each cluster and the associated size
                min_distance = float('inf')
                comp_i_size = 0.0
                comp_j_size = 0.0

                for comp_i in clusters[i]:
                    for comp_j in clusters[j]:
                        if comp_i in centers and comp_j in centers:
                            dist = (centers[comp_i] - centers[comp_j]).length
                            if dist < min_distance:
                                min_distance = dist
                                comp_i_size = component_sizes.get(comp_i, average_size)
                                comp_j_size = component_sizes.get(comp_j, average_size)

                # Calculate adaptive thresholds based on the sizes of two components
                # Use a fixed percentage of the size of larger components
                adaptive_threshold = max(comp_i_size, comp_j_size) * 0.5

                # Limit the threshold range
                adaptive_threshold = max(min_threshold, min(max_threshold, adaptive_threshold))

                # Merge clusters if the distance is below the threshold
                if min_distance <= adaptive_threshold:
                    clusters[i].extend(clusters[j])
                    clusters.pop(j)
                    merged = True
                    break

            if merged:
                break

    return clusters

# def normalize_overlapping_vertices_weights(clothing_meshes, base_avatar_data, distance_threshold=0.0001):
#     """
#     Align the weights of vertices that are nearly overlapping in world coordinates
#     Process using a mesh subdivided by one level, then apply the results to the original mesh

#     Parameters:
#         clothing_meshes: List of clothing meshes to be processed
#         base_avatar_data: Base Avatar Data
#         distance_threshold: Threshold distance for determining overlap
#     """
#     print("Normalizing weights for overlapping vertices using subdivision approach...")

#     # Get the vertex group to be checked
#     target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

#     # Filter the meshes to be processed (only meshes with an InpaintMask)
#     valid_meshes = []
#     for mesh in clothing_meshes:
#         if "InpaintMask" in mesh.vertex_groups:
#             valid_meshes.append(mesh)

#     if not valid_meshes:
#         print("No meshes with InpaintMask found, skipping normalization")
#         return

#     # Processing for each mesh
#     for mesh_obj in valid_meshes:
#         # Duplicate the original mesh and subdivide it
#         bpy.ops.object.select_all(action='DESELECT')
#         mesh_obj.select_set(True)
#         bpy.context.view_layer.objects.active = mesh_obj
#         bpy.ops.object.duplicate(linked=False)
#         subdiv_obj = bpy.context.active_object
#         subdiv_obj.name = f"{mesh_obj.name}_TempSubdiv"

#         # Add Subdivision Modifier
#         subdiv_mod = subdiv_obj.modifiers.new(name="TempSubdivision", type='SUBSURF')
#         subdiv_mod.levels = 1
#         subdiv_mod.render_levels = 1
#         subdiv_mod.subdivision_type = 'SIMPLE'  # Use Simple mode

#         # Apply modifiers
#         apply_modifiers_keep_shapekeys_with_temp(subdiv_obj)
#         bpy.context.view_layer.objects.active = mesh_obj

#         # Copy vertex group
#         for group_name in target_groups:
#             if group_name in mesh_obj.vertex_groups and group_name not in subdiv_obj.vertex_groups:
#                 subdiv_obj.vertex_groups.new(name=group_name)

#         if "InpaintMask" in mesh_obj.vertex_groups and "InpaintMask" not in subdiv_obj.vertex_groups:
#             subdiv_obj.vertex_groups.new(name="InpaintMask")

#         if "Rigid" in mesh_obj.vertex_groups and "Rigid" not in subdiv_obj.vertex_groups:
#             subdiv_obj.vertex_groups.new(name="Rigid")

#         # Retrieve evaluated data
#         depsgraph = bpy.context.evaluated_depsgraph_get()
#         eval_subdiv_obj = subdiv_obj.evaluated_get(depsgraph)
#         eval_subdiv_mesh = eval_subdiv_obj.data

#         # Create a BMesh and retrieve the vertex edge information
#         bm = bmesh.new()
#         bm.from_mesh(eval_subdiv_mesh)
#         bm.verts.ensure_lookup_table()
#         bm.edges.ensure_lookup_table()

#         # Collect vertex data
#         all_vertices = []
#         for vert_idx, vert in enumerate(eval_subdiv_mesh.vertices):
#             # Check the weights of the Rigid vertex group
#             rigid_weight = 0.0
#             if "Rigid" in subdiv_obj.vertex_groups:
#                 rigid_group = subdiv_obj.vertex_groups["Rigid"]
#                 for g in vert.groups:
#                     if g.group == rigid_group.index:
#                         rigid_weight = g.weight
#                         break

#             # Vertices with weights greater than 0 in the Rigid vertex group are ignored
#             if rigid_weight > 0:
#                 continue

#             # Retrieve the weight of the InpaintMask
#             inpaint_weight = 0.0
#             if "InpaintMask" in subdiv_obj.vertex_groups:
#                 inpaint_group = subdiv_obj.vertex_groups["InpaintMask"]
#                 for g in vert.groups:
#                     if g.group == inpaint_group.index:
#                         inpaint_weight = g.weight
#                         break

#             # Calculate the world coordinates of the vertex
#             world_pos = subdiv_obj.matrix_world @ vert.co

#             # Collect the weights for the target group
#             weights = {}
#             for group_name in target_groups:
#                 if group_name in subdiv_obj.vertex_groups:
#                     group = subdiv_obj.vertex_groups[group_name]
#                     for g in vert.groups:
#                         if g.group == group.index:
#                             weights[group_name] = g.weight
#                             break

#             # Collect the direction vectors of edges connected to the vertex
#             edge_directions = []
#             bm_vert = bm.verts[vert_idx]
#             for edge in bm_vert.link_edges:
#                 other_vert = edge.other_vert(bm_vert)
#                 direction = (other_vert.co - bm_vert.co).normalized()
#                 edge_directions.append(direction)

#             # Save vertex data
#             all_vertices.append({
#                 'vert_idx': vert_idx,
#                 'world_pos': world_pos,
#                 'weights': weights,
#                 'inpaint_weight': inpaint_weight,
#                 'edge_directions': edge_directions
#             })

#         # Build a KDTree to efficiently search for nearby vertices
#         positions = [v['world_pos'] for v in all_vertices]
#         kdtree = KDTree(len(positions))
#         for i, pos in enumerate(positions):
#             kdtree.insert(pos, i)
#         kdtree.balance()

#         # Detect overlapping vertices and align their weights
#         processed = set()  # Record processed vertex indices
#         normalized_weights = {}  # Normalized weight {vert_idx: {group_name: weight}}

#         for i, vert_data in enumerate(all_vertices):
#             if i in processed:
#                 continue

#             # Search for Nearby Vertices
#             overlapping_indices = []
#             for (co, idx, dist) in kdtree.find_range(vert_data['world_pos'], distance_threshold):
#                 if idx != i and idx not in processed:  # Exclude the vertex itself and processed vertices
#                     # Check similarity in the edge direction
#                     if check_edge_direction_similarity(vert_data['edge_directions'], all_vertices[idx]['edge_directions']):
#                         overlapping_indices.append(idx)

#             if not overlapping_indices:
#                 continue

#             # Include overlapping vertex groups
#             overlapping_indices.append(i)
#             processed.add(i)

#             # Sort overlapping vertices by InpaintMask weight
#             overlapping_verts = [all_vertices[idx] for idx in overlapping_indices]
#             overlapping_verts.sort(key=lambda x: x['inpaint_weight'])

#             # Use the weight of the vertex with the smallest weight in the InpaintMask
#             reference_vert = overlapping_verts[0]
#             reference_weights = reference_vert['weights']
#             min_inpaint_weight = reference_vert['inpaint_weight']
#             same_weight_verts = [v for v in overlapping_verts if abs(v['inpaint_weight'] - min_inpaint_weight) < 0.0001]

#             if len(same_weight_verts) > 1:
#                 # Calculate the average weight
#                 avg_weights = {}
#                 for group_name in target_groups:
#                     weights_sum = 0.0
#                     count = 0
#                     for v in same_weight_verts:
#                         if group_name in v['weights']:
#                             weights_sum += v['weights'][group_name]
#                             count += 1
#                     if count > 0:
#                         avg_weights[group_name] = weights_sum / count
#                 reference_weights = avg_weights

#             # Apply reference weights to all overlapping vertices
#             for vert in overlapping_verts:
#                 vert_idx = vert['vert_idx']
#                 normalized_weights[vert_idx] = reference_weights.copy()
#                 processed.add(overlapping_indices[overlapping_verts.index(vert)])

#         # Release BMesh
#         bm.free()

#         # Update subdivision mesh vertex weights
#         for vert_idx, weights in normalized_weights.items():
#             # Clear existing weights
#             for group_name in target_groups:
#                 if group_name in subdiv_obj.vertex_groups:
#                     try:
#                         subdiv_obj.vertex_groups[group_name].remove([vert_idx])
#                     except RuntimeError:
#                         pass

#             # Apply new weight
#             for group_name, weight in weights.items():
#                 if weight > 0:
#                     if group_name not in subdiv_obj.vertex_groups:
#                         subdiv_obj.vertex_groups.new(name=group_name)
#                     subdiv_obj.vertex_groups[group_name].add([vert_idx], weight, 'REPLACE')

#         # Transfer results from the subdivided mesh to the original mesh
#         # Use KDTree to find the closest subdivision mesh vertex for each vertex of the original mesh
#         original_verts_world = [mesh_obj.matrix_world @ v.co for v in mesh_obj.data.vertices]
#         subdiv_verts_world = [subdiv_obj.matrix_world @ v.co for v in subdiv_obj.data.vertices]

#         subdiv_kdtree = KDTree(len(subdiv_verts_world))
#         for i, pos in enumerate(subdiv_verts_world):
#             subdiv_kdtree.insert(pos, i)
#         subdiv_kdtree.balance()

#         # Find the closest subdivision mesh vertex for each vertex of the original mesh and transfer the weights
#         for i, orig_pos in enumerate(original_verts_world):
#             # Check the weights of the Rigid vertex group
#             rigid_weight = 0.0
#             if "Rigid" in mesh_obj.vertex_groups:
#                 rigid_group = mesh_obj.vertex_groups["Rigid"]
#                 rigid_group_index = rigid_group.index

#                 for g in mesh_obj.data.vertices[i].groups:
#                     if g.group == rigid_group_index:
#                         rigid_weight = g.weight
#                         break

#             # Vertices with weights greater than 0 in the Rigid vertex group are ignored
#             if rigid_weight > 0:
#                 continue

#             # Find the nearest subdivision mesh vertex
#             co, subdiv_idx, dist = subdiv_kdtree.find(orig_pos)

#             # Transfer weights only when the distance is within the threshold
#             if dist <= distance_threshold * 2:  # Use a slightly larger threshold
#                 # Verify that the vertices of the subdivided mesh are included in normalized_weights
#                 if subdiv_idx in normalized_weights:
#                     # Clear existing weights
#                     for group_name in target_groups:
#                         if group_name in mesh_obj.vertex_groups:
#                             group = mesh_obj.vertex_groups[group_name]
#                             group_index = group.index

#                             # Verify whether the vertex exists in this group
#                             vertex_in_group = False
#                             for g in mesh_obj.data.vertices[i].groups:
#                                 if g.group == group_index:
#                                     vertex_in_group = True
#                                     break

#                             # Attempt deletion only if the vertex exists in a group
#                             if vertex_in_group:
#                                 try:
#                                     group.remove([i])
#                                 except RuntimeError:
#                                     pass

#                     # Apply new weight
#                     for group_name, weight in normalized_weights[subdiv_idx].items():
#                         if weight > 0:
#                             if group_name not in mesh_obj.vertex_groups:
#                                 mesh_obj.vertex_groups.new(name=group_name)
#                             mesh_obj.vertex_groups[group_name].add([i], weight, 'REPLACE')

#         # Delete temporary subdivision mesh
#         bpy.data.objects.remove(subdiv_obj, do_unlink=True)

#     print(f"Normalized weights using subdivision approach")

def subdivide_selected_vertices(obj_name, vertex_indices, level=2):
    """
    Subdivide selected vertices of a specific mesh

    Arguments:
        obj_name (str): Name of the target object
        vertex_indices (list): Index list of vertices to select
        cuts (int): Number of subdivision segments
    """
    # Active Object Settings
    bpy.ops.object.mode_set(mode='OBJECT')
    obj = bpy.data.objects.get(obj_name)

    if obj is None:
        print(f"Object '{obj_name}' not found")
        return

    # Select an object to activate it
    bpy.ops.object.select_all(action='DESELECT')
    obj.select_set(True)
    bpy.context.view_layer.objects.active = obj

    # Switch to edit mode
    bpy.ops.object.mode_set(mode='EDIT')

    # Acquire bmesh
    me = obj.data
    bm = bmesh.from_edit_mesh(me)
    bm.verts.ensure_lookup_table()
    bm.edges.ensure_lookup_table()
    bm.faces.ensure_lookup_table()

    # Clear all selections
    for v in bm.verts:
        v.select = False
    for e in bm.edges:
        e.select = False
    for f in bm.faces:
        f.select = False

    # Select the specified vertex
    for idx in vertex_indices:
        if idx < len(bm.verts):
            bm.verts[idx].select = True

    # Select only edges consisting of selected vertices "between" each other
    # In other words, the edge is selected only if both endpoints are included in the selected vertex list
    selected_verts = set(bm.verts[idx] for idx in vertex_indices if idx < len(bm.verts))
    connected_edges = []

    for e in bm.edges:
        # Only select when both endpoints of an edge are included in the selected vertex set
        if e.verts[0] in selected_verts and e.verts[1] in selected_verts:
            e.select = True
            connected_edges.append(e)

    # Apply changes
    bmesh.update_edit_mesh(me)

    # Subdivision operation
    if connected_edges:
        for _ in range(level):
            bpy.ops.mesh.subdivide(number_cuts=1)
        print(f"{len(connected_edges)} edges have been subdivided")
    else:
        print("No edge was found between the selected vertices")

    # Return to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')
    obj.data.update()

def normalize_overlapping_vertices_weights(clothing_meshes, base_avatar_data, overlap_attr_name="Overlapped", world_pos_attr_name="OriginalWorldPosition"):
    """
    Only faces and edges composed of vertices with an Overlapped attribute of 1
    Normalize the weights of overlapping vertices

    Parameters:
        clothing_meshes: List of clothing meshes to be processed
        base_avatar_data: Base Avatar Data
        overlap_attr_name: Overlap detection flag attribute name
        world_pos_attr_name: Attribute name where world coordinates are stored
    """
    print("Normalizing weights for overlapping vertices using custom attributes...")

    original_active = bpy.context.view_layer.objects.active

    # Get the vertex group to be checked
    target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

    # Filter the target mesh (only meshes with the required attributes)
    valid_meshes = []
    for mesh in clothing_meshes:
        if (overlap_attr_name in mesh.data.attributes and
            world_pos_attr_name in mesh.data.attributes and
            "InpaintMask" in mesh.vertex_groups):
            valid_meshes.append(mesh)

    if not valid_meshes:
        print(f"Warning: No mesh with {overlap_attr_name} and {world_pos_attr_name} attributes was found. Skipping processing.")
        return

    # Processing for each mesh
    for mesh_obj in valid_meshes:
        # Select an object to enter edit mode
        bpy.ops.object.select_all(action='DESELECT')
        mesh_obj.select_set(True)
        bpy.context.view_layer.objects.active = mesh_obj

        # Duplicate the original mesh to create a processing object
        bpy.ops.object.duplicate(linked=False)
        work_obj = bpy.context.active_object
        work_obj.name = f"{mesh_obj.name}_OverlapWork"

        # Retrieve custom attributes
        overlap_attr = mesh_obj.data.attributes[overlap_attr_name]
        world_pos_attr = mesh_obj.data.attributes[world_pos_attr_name]

        # Identify overlapping vertices (with attribute values of 1.0)
        overlapping_verts_ids = [i for i, data in enumerate(overlap_attr.data) if data.value > 0.9999]

        if not overlapping_verts_ids:
            print(f"Warning: No overlapping vertices found in {mesh_obj.name}. Skipping processing.")
            bpy.data.objects.remove(work_obj, do_unlink=True)
            continue

        subdivide_selected_vertices(work_obj.name, overlapping_verts_ids, level=2)
        subdiv_overlap_attr = work_obj.data.attributes[overlap_attr_name]
        subdiv_overlapping_verts_ids = [i for i, data in enumerate(subdiv_overlap_attr.data) if data.value > 0.9999]
        subdiv_world_pos_attr = work_obj.data.attributes[world_pos_attr_name]

        # Build a KDTree to efficiently search for nearby vertices
        subdiv_original_world_positions = []

        for vert_idx in subdiv_overlapping_verts_ids:
            world_pos = Vector(subdiv_world_pos_attr.data[vert_idx].vector)
            subdiv_original_world_positions.append(world_pos)

        # Group overlapping vertices (combine vertices at the same position)
        distance_threshold = 0.0001  # Overlap threshold
        overlapping_groups = {}

        for orig_idx, world_pos in zip(subdiv_overlapping_verts_ids, subdiv_original_world_positions):
            found_group = False

            for group_id, (group_pos, members) in overlapping_groups.items():
                if (world_pos - group_pos).length <= distance_threshold:
                    members.append(orig_idx)
                    found_group = True
                    break

            if not found_group:
                group_id = len(overlapping_groups)
                overlapping_groups[group_id] = (world_pos, [orig_idx])

        # Calculate the reference weight of overlapping vertices in each group
        reference_weights = {}
        vert_weights = {}
        for group_id, (group_pos, member_indices) in overlapping_groups.items():
            # Retrieve the weight of the InpaintMask
            member_inpaint_weights = []
            for idx in member_indices:
                inpaint_weight = 0.0
                if "InpaintMask" in work_obj.vertex_groups:
                    inpaint_group = work_obj.vertex_groups["InpaintMask"]
                    for g in work_obj.data.vertices[idx].groups:
                        if g.group == inpaint_group.index:
                            inpaint_weight = g.weight
                            break
                member_inpaint_weights.append((idx, inpaint_weight))

            # Sort by InpaintMask weight
            member_inpaint_weights.sort(key=lambda x: x[1])

            # InpaintMask uses the vertex with the smallest weight as the reference point
            if member_inpaint_weights:
                reference_idx = member_inpaint_weights[0][0]
                ref_weights = {}

                # Get the weight of the vertex group
                for group_name in target_groups:
                    if group_name in work_obj.vertex_groups:
                        group = work_obj.vertex_groups[group_name]
                        weight = 0.0
                        for g in work_obj.data.vertices[reference_idx].groups:
                            if g.group == group.index:
                                weight = g.weight
                                break
                        ref_weights[group_name] = weight

                reference_weights[group_id] = ref_weights

                # If there are vertices with the same InpaintMask weight, calculate the average value
                min_inpaint_weight = member_inpaint_weights[0][1]
                same_weight_vert_ids = [v[0] for v in member_inpaint_weights if abs(v[1] - min_inpaint_weight) < 0.0001]

                same_weight_verts = [work_obj.data.vertices[idx] for idx in same_weight_vert_ids]

                if len(same_weight_verts) > 1:
                    # Calculate the average weight
                    avg_weights = {}
                    for group_name in target_groups:
                        if group_name in work_obj.vertex_groups:
                            weights_sum = 0.0
                            count = 0
                            for v in same_weight_verts:
                                weight = 0.0
                                for g in v.groups:
                                    if g.group == mesh_obj.vertex_groups[group_name].index:
                                        weight = g.weight
                                        break
                                if weight > 0:
                                    weights_sum += weight
                                    count += 1
                            if count > 0:
                                avg_weights[group_name] = weights_sum / count
                    reference_weights[group_id] = avg_weights

                # Apply reference weights to all overlapping vertices
                for vert_idx in member_indices:
                    vert_weights[vert_idx] = reference_weights[group_id].copy()

        # Return to the original mesh
        bpy.ops.object.select_all(action='DESELECT')
        mesh_obj.select_set(True)
        bpy.context.view_layer.objects.active = mesh_obj

        # Update vertex group
        updated_count = 0

        # Mapping subdivided mesh vertices to original mesh vertices
        for orig_idx in overlapping_verts_ids:
            # Get the world coordinates of the original vertex
            orig_world_pos = Vector(world_pos_attr.data[orig_idx].vector)

            # Find the nearest subdivision mesh vertex
            closest_idx = None
            min_dist = float('inf')

            for subdiv_idx, subdiv_pos in zip(subdiv_overlapping_verts_ids, subdiv_original_world_positions):
                dist = (orig_world_pos - subdiv_pos).length
                if dist < min_dist:
                    min_dist = dist
                    closest_idx = subdiv_idx

            # If within a certain distance, apply the weight
            if closest_idx is not None and closest_idx in vert_weights and min_dist < distance_threshold:
                # Update vertex group weights
                for group_name, weight in vert_weights[closest_idx].items():
                    if group_name in mesh_obj.vertex_groups:
                        mesh_obj.vertex_groups[group_name].add([orig_idx], weight, 'REPLACE')
                updated_count += 1

        # Delete the work object
        bpy.data.objects.remove(work_obj, do_unlink=True)

        print(f"{mesh_obj.name}'s {updated_count} vertices have had their weights normalized.")

    bpy.context.view_layer.objects.active = original_active
    print("Weight normalization for overlapping vertices is complete.")

def normalize_weights_from_overlapping_uvmap(clothing_meshes, base_avatar_data, uvmap_name="OverlappingVertices"):
    """
    Extract overlapping vertices from the UV map saved by create_overlapping_vertices_uvmap,
    Normalize vertex weights for overlapping vertices

    Parameters:
        clothing_meshes: List of clothing meshes to be processed
        base_avatar_data: Base Avatar Data
        uvmap_name: The name of the UV map storing overlapping vertex information
    """
    print(f"Normalizing weights from overlapping vertices UV map: {uvmap_name}")

    # Get the vertex group to be checked
    target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

    # Filter the target mesh (only meshes with specified UV maps)
    valid_meshes = []
    for mesh in clothing_meshes:
        if uvmap_name in mesh.data.uv_layers and "InpaintMask" in mesh.vertex_groups:
            valid_meshes.append(mesh)

    if not valid_meshes:
        print(f"Warning: No mesh with {uvmap_name} and InpaintMask was found. Skipping processing.")
        return

    # Processing for each mesh
    for mesh_obj in valid_meshes:

        all_vertices = {}
        for vert_idx, vert in enumerate(mesh_obj.data.vertices):
            # Check the weights of the Rigid vertex group
            rigid_weight = 0.0
            if "Rigid" in mesh_obj.vertex_groups:
                rigid_group = mesh_obj.vertex_groups["Rigid"]
                for g in vert.groups:
                    if g.group == rigid_group.index:
                        rigid_weight = g.weight
                        break

            # Retrieve the weight of the InpaintMask
            inpaint_weight = 0.0
            if "InpaintMask" in mesh_obj.vertex_groups:
                inpaint_group = mesh_obj.vertex_groups["InpaintMask"]
                for g in vert.groups:
                    if g.group == inpaint_group.index:
                        inpaint_weight = g.weight
                        break

            # vertex coordinates
            pos = vert.co

            # Collect the weights for the target group
            weights = {}
            for group_name in target_groups:
                if group_name in mesh_obj.vertex_groups:
                    group = mesh_obj.vertex_groups[group_name]
                    for g in vert.groups:
                        if g.group == group.index:
                            weights[group_name] = g.weight
                            break

            # Save vertex data
            all_vertices[vert_idx] = {
                'pos': pos,
                'weights': weights,
                'rigid_weight': rigid_weight,
                'inpaint_weight': inpaint_weight
            }

        # Retrieve overlap information from the UV map
        uv_layer = mesh_obj.data.uv_layers[uvmap_name]

        # Group UV map values to identify overlapping vertices
        overlapping_groups = {}

        # Collect UV map values
        for poly in mesh_obj.data.polygons:
            for loop_idx in poly.loop_indices:
                vert_idx = mesh_obj.data.loops[loop_idx].vertex_index
                uv = uv_layer.data[loop_idx].uv

                # Skip if the UV coordinates are at the origin
                if abs(uv.x) < 0.0001 and abs(uv.y) < 0.0001:
                    continue

                # Round the UV value to create a hashable key
                uv_key = (round(uv.x, 5), round(uv.y, 5))

                if uv_key not in overlapping_groups:
                    overlapping_groups[uv_key] = []

                if vert_idx not in overlapping_groups[uv_key]:
                    overlapping_groups[uv_key].append(vert_idx)

        # Extract only overlapping vertex groups (groups containing two or more vertices)
        valid_groups = {k: v for k, v in overlapping_groups.items() if len(v) >= 2}

        if not valid_groups:
            print(f"Warning: No overlapping vertices found in {mesh_obj.name}. Skipping processing.")
            continue

        print(f"Process {len(valid_groups)} overlapping vertex groups in {mesh_obj.name}.")

        # Processing for each overlapping group
        for uv_key, vert_indices in valid_groups.items():

            # Sort overlapping vertices by InpaintMask weight
            overlapping_verts = [all_vertices[idx] for idx in vert_indices]
            overlapping_verts.sort(key=lambda x: x['inpaint_weight'])

            # Use the weight of the vertex with the smallest weight in the InpaintMask
            reference_vert = overlapping_verts[0]
            reference_weights = reference_vert['weights']

            # If there are vertices with the same InpaintMask weight, calculate the average value
            min_inpaint_weight = reference_vert['inpaint_weight']
            same_weight_verts = [v for v in overlapping_verts if abs(v['inpaint_weight'] - min_inpaint_weight) < 0.0001]

            if len(same_weight_verts) > 1:
                # Calculate the average weight
                avg_weights = {}
                for group_name in target_groups:
                    weights_sum = 0.0
                    count = 0
                    for v in same_weight_verts:
                        if group_name in v['weights']:
                            weights_sum += v['weights'][group_name]
                            count += 1
                    if count > 0:
                        avg_weights[group_name] = weights_sum / count
                reference_weights = avg_weights

            # Apply weights to each vertex
            for vert_idx in vert_indices:
                for group_name, new_weight in reference_weights.items():
                    if new_weight > 0:
                        mesh_obj.vertex_groups[group_name].add([vert_idx], new_weight, 'REPLACE')
                    else:
                        mesh_obj.vertex_groups[group_name].add([vert_idx], 0.0, 'REPLACE')

    print("Weight normalization for overlapping vertices is complete.")

def check_edge_direction_similarity(directions1, directions2, angle_threshold=3.0):
    """
    Check whether the edge direction sets of two vertices are similar

    Parameters:
        directions1: List of edge direction vectors for the first vertex
        directions2: List of edge direction vectors for the second vertex
        angle_threshold: Threshold angle (degrees) for determining similarity

    Returns:
        bool: True if at least one edge direction is similar
    """
    # Returns False if the vertex is isolated (has no edges)
    if not directions1 or not directions2:
        return False

    # Convert the angle threshold to radians
    angle_threshold_rad = math.radians(angle_threshold)

    # Check combinations in each direction
    for dir1 in directions1:
        for dir2 in directions2:
            # Calculate the angle between two direction vectors
            dot_product = dir1.dot(dir2)
            # Since the inner product may exceed 1, clamp it
            dot_product = max(min(dot_product, 1.0), -1.0)
            angle = math.acos(dot_product)

            # The angle is below the threshold, or greater than the value obtained by subtracting the threshold from 180 degrees (considering the reverse direction as well)
            if angle <= angle_threshold_rad or angle >= (math.pi - angle_threshold_rad):
                return True

    return False


def process_humanoid_vertex_groups(mesh_obj: bpy.types.Object, clothing_armature: bpy.types.Object, base_avatar_data: dict, clothing_avatar_data: dict) -> None:
    """
    Processing the Humanoid bone vertex groups for clothing meshes
    - Convert Humanoid bone names to those of the base avatar data
    - Add an auxiliary bone vertex group
    - If the conditions are met, add the Optional Humanoid bone's vertex group
    """

    # Get bone names from clothing armature
    clothing_bone_names = set(bone.name for bone in clothing_armature.data.bones)

    # Create a mapping for Humanoid bone names
    base_humanoid_to_bone = {bone_map["humanoidBoneName"]: bone_map["boneName"]
                        for bone_map in base_avatar_data["humanoidBones"]}
    clothing_humanoid_to_bone = {bone_map["humanoidBoneName"]: bone_map["boneName"]
                           for bone_map in clothing_avatar_data["humanoidBones"]}
    clothing_bone_to_humanoid = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                           for bone_map in clothing_avatar_data["humanoidBones"]}

    # Create auxiliary bone mapping
    auxiliary_bones = {}
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        if humanoid_bone in base_humanoid_to_bone:
            auxiliary_bones[base_humanoid_to_bone[humanoid_bone]] = aux_set["auxiliaryBones"]

    # Get the name of the existing vertex group
    existing_groups = set(vg.name for vg in mesh_obj.vertex_groups)

    # Identify groups requiring name changes
    groups_to_rename = {}
    for group in mesh_obj.vertex_groups:
        if group.name in clothing_bone_to_humanoid:
            humanoid_name = clothing_bone_to_humanoid[group.name]
            if humanoid_name in base_humanoid_to_bone:
                base_bone_name = base_humanoid_to_bone[humanoid_name]
                groups_to_rename[group.name] = base_bone_name

    # Change Group Name
    for old_name, new_name in groups_to_rename.items():
        if old_name in mesh_obj.vertex_groups:
            group = mesh_obj.vertex_groups[old_name]
            group_index = group.index
            # Save the weight for each vertex
            weights = {}
            for vert in mesh_obj.data.vertices:
                for g in vert.groups:
                    if g.group == group_index:
                        weights[vert.index] = g.weight
                        break

            # Change Group Name
            group.name = new_name

            # Add an auxiliary bone vertex group
            if new_name in auxiliary_bones:
                # Create a vertex group for the auxiliary bone
                for aux_bone in auxiliary_bones[new_name]:
                    if aux_bone not in existing_groups:
                        mesh_obj.vertex_groups.new(name=aux_bone)

    existing_groups = set(vg.name for vg in mesh_obj.vertex_groups)

    breast_bones_dont_exist = 'LeftBreast' not in clothing_humanoid_to_bone and 'RightBreast' not in clothing_humanoid_to_bone

    # Process each humanoid bone from base avatar
    for humanoid_name, bone_name in base_humanoid_to_bone.items():
        # Skip if bone exists in clothing armature
        if bone_name in existing_groups:
            continue

        should_add_optional_humanoid_bone = False

        # Condition 1: Chest exists in clothing, UpperChest missing in clothing but exists in base
        if (humanoid_name == "UpperChest" and
            "Chest" in clothing_humanoid_to_bone and
            base_humanoid_to_bone["Chest"] in existing_groups and
            "UpperChest" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 2: LeftLowerLeg exists in clothing, LeftFoot missing in clothing but exists in base
        elif (humanoid_name == "LeftFoot" and
                "LeftLowerLeg" in clothing_humanoid_to_bone and
                base_humanoid_to_bone["LeftLowerLeg"] in existing_groups and
                "LeftFoot" not in clothing_humanoid_to_bone and
                "LeftFoot" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 2: RightLowerLeg exists in clothing, RightFoot missing in clothing but exists in base
        elif (humanoid_name == "RightFoot" and
                "RightLowerLeg" in clothing_humanoid_to_bone and
                base_humanoid_to_bone["RightLowerLeg"] in existing_groups and
                "RightFoot" not in clothing_humanoid_to_bone and
                "RightFoot" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 3: LeftLowerLeg or LeftFoot exists in clothing, LeftToe missing in clothing but exists in base
        elif (humanoid_name == "LeftToe" and
                (("LeftLowerLeg" in clothing_humanoid_to_bone and base_humanoid_to_bone["LeftLowerLeg"] in existing_groups) or
                ("LeftFoot" in clothing_humanoid_to_bone and base_humanoid_to_bone["LeftFoot"] in existing_groups)) and
                "LeftToe" not in clothing_humanoid_to_bone and
                "LeftToe" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 3: RightLowerLeg or RightFoot exists in clothing, RightToe missing in clothing but exists in base
        elif (humanoid_name == "RightToe" and
                (("RightLowerLeg" in clothing_humanoid_to_bone and base_humanoid_to_bone["RightLowerLeg"] in existing_groups) or
                ("RightFoot" in clothing_humanoid_to_bone and base_humanoid_to_bone["RightFoot"] in existing_groups)) and
                "RightToe" not in clothing_humanoid_to_bone and
                "RightToe" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 4: LeftShoulder exists in clothing, LeftUpperArm exists in base but not in clothing
        elif (humanoid_name == "LeftUpperArm" and
                "LeftShoulder" in clothing_humanoid_to_bone and
                base_humanoid_to_bone["LeftShoulder"] in existing_groups and
                "LeftUpperArm" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 4: RightShoulder exists in clothing, RightUpperArm exists in base but not in clothing
        elif (humanoid_name == "RightUpperArm" and
                "RightShoulder" in clothing_humanoid_to_bone and
                base_humanoid_to_bone["RightShoulder"] in existing_groups and
                "RightUpperArm" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 5: LeftBreast exists in clothing, breast bones don't exist in clothing, Chest or UpperChest exists in base
        elif (humanoid_name == "LeftBreast" and breast_bones_dont_exist and
                (base_humanoid_to_bone["Chest"] in existing_groups or base_humanoid_to_bone["UpperChest"] in existing_groups) and
                "LeftBreast" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        # Condition 5: RightBreast exists in clothing, breast bones don't exist in clothing, Chest or UpperChest exists in base
        elif (humanoid_name == "RightBreast" and breast_bones_dont_exist and
                (base_humanoid_to_bone["Chest"] in existing_groups or base_humanoid_to_bone["UpperChest"] in existing_groups) and
                "RightBreast" in base_humanoid_to_bone):
            should_add_optional_humanoid_bone = True

        if should_add_optional_humanoid_bone:
            print(f"Adding optional humanoid bone group: {humanoid_name} ({bone_name})")
            if bone_name not in existing_groups:
                mesh_obj.vertex_groups.new(name=bone_name)
            else:
                print(f"Optional humanoid bone group already exists: {bone_name}")
            # Add an auxiliary bone vertex group
            if bone_name in auxiliary_bones:
                # Create a vertex group for the auxiliary bone
                for aux_bone in auxiliary_bones[bone_name]:
                    if aux_bone not in existing_groups:
                        mesh_obj.vertex_groups.new(name=aux_bone)


def store_armature_modifier_settings(obj):
    """Save Armature Modifier Settings"""
    armature_settings = []
    for modifier in obj.modifiers:
        if modifier.type == 'ARMATURE':
            settings = {
                'name': modifier.name,
                'object': modifier.object,
                'vertex_group': modifier.vertex_group,
                'invert_vertex_group': modifier.invert_vertex_group,
                'use_vertex_groups': modifier.use_vertex_groups,
                'use_bone_envelopes': modifier.use_bone_envelopes,
                'use_deform_preserve_volume': modifier.use_deform_preserve_volume,
                'use_multi_modifier': modifier.use_multi_modifier,
                'show_viewport': modifier.show_viewport,
                'show_render': modifier.show_render,
            }
            armature_settings.append(settings)
    return armature_settings

def restore_armature_modifier(obj, settings):
    """Restore Armature Modifier"""
    for modifier_settings in settings:
        modifier = obj.modifiers.new(name=modifier_settings['name'], type='ARMATURE')
        modifier.object = modifier_settings['object']
        modifier.vertex_group = modifier_settings['vertex_group']
        modifier.invert_vertex_group = modifier_settings['invert_vertex_group']
        modifier.use_vertex_groups = modifier_settings['use_vertex_groups']
        modifier.use_bone_envelopes = modifier_settings['use_bone_envelopes']
        modifier.use_deform_preserve_volume = modifier_settings['use_deform_preserve_volume']
        modifier.use_multi_modifier = modifier_settings['use_multi_modifier']
        modifier.show_viewport = modifier_settings['show_viewport']
        modifier.show_render = modifier_settings['show_render']

def set_armature_modifier_visibility(obj, show_viewport, show_render):
    """Configure Armature Modifier Display"""
    for modifier in obj.modifiers:
        if modifier.type == 'ARMATURE':
            modifier.show_viewport = show_viewport
            modifier.show_render = show_render

def set_armature_modifier_target_armature(obj, target_armature):
    """Configure Armature Modifier Display"""
    for modifier in obj.modifiers:
        if modifier.type == 'ARMATURE':
            modifier.object = target_armature

def apply_all_shapekeys(obj):
    """Apply all shape keys to the object"""
    if not obj.data.shape_keys:
        return

    # The base shape key is always at index 0
    if obj.active_shape_key_index == 0 and len(obj.data.shape_keys.key_blocks) > 1:
        obj.active_shape_key_index = 1
    else:
        obj.active_shape_key_index = 0

    bpy.context.view_layer.objects.active = obj
    bpy.ops.object.shape_key_remove(all=True, apply_mix=True)

def apply_modifiers(obj):
    """Apply modifiers"""
    bpy.context.view_layer.objects.active = obj
    for modifier in obj.modifiers[:]:  # Use slices to create copies of lists
        try:
            bpy.ops.object.modifier_apply(modifier=modifier.name)
        except Exception as e:
            print(f"Failed to apply modifier {modifier.name}: {e}")

def apply_modifiers_keep_shapekeys_with_temp(obj):
    """Apply modifiers while preserving shape keys using temporary objects"""
    if obj.type != 'MESH':
        return

    if not obj.data.shape_keys:
        # If no shape key exists, apply the modifier normally
        bpy.context.view_layer.objects.active = obj
        for modifier in obj.modifiers:
            try:
                bpy.ops.object.modifier_apply(modifier=modifier.name)
            except Exception as e:
                print(f"Failed to apply modifier {modifier.name}: {e}")
        return

    # Initialize the global counter (if it does not exist)
    if not hasattr(apply_modifiers_keep_shapekeys_with_temp, 'counter'):
        apply_modifiers_keep_shapekeys_with_temp.counter = 0

    shape_keys = obj.data.shape_keys.key_blocks
    temp_objects = []

    # Create a temporary object for each shape key
    for i, shape_key in enumerate(shape_keys):
        if i == 0:  # Skip Basis
            continue

        # Deselect all objects
        bpy.ops.object.select_all(action='DESELECT')
        # Duplicate the object
        bpy.context.view_layer.objects.active = obj
        obj.select_set(True)
        bpy.ops.object.duplicate(linked=False)
        temp_obj = bpy.context.active_object

        temp_obj.name = f"t{apply_modifiers_keep_shapekeys_with_temp.counter}:{shape_key.name}"
        apply_modifiers_keep_shapekeys_with_temp.counter += 1

        temp_objects.append(temp_obj)

        # Set the values of other shape keys to 0 and the target shape key's value to 1
        for sk in temp_obj.data.shape_keys.key_blocks:
            if sk.name == shape_key.name:
                sk.value = 1.0
            else:
                sk.value = 0.0

        # Apply shape key
        apply_all_shapekeys(temp_obj)

        # Apply modifiers other than Armature
        apply_modifiers(temp_obj)

    # Processing of the original object
    bpy.context.view_layer.objects.active = obj
    # First, set all shape key values to 0
    for sk in obj.data.shape_keys.key_blocks:
        sk.value = 0.0
    # Apply shape key
    apply_all_shapekeys(obj)
    # Apply modifiers
    apply_modifiers(obj)

    # Add the shape of the temporary object as a shape key for the original object
    obj.shape_key_add(name="Basis")
    for temp_obj in temp_objects:
        # Add a shape key
        shape_key = obj.shape_key_add(name=temp_obj.name.split(':')[-1])
        shape_key.interpolation = 'KEY_LINEAR'
        if shape_key.name == "SymmetricDeformed":
            shape_key.value = 1.0

        # Set vertex coordinates
        for i, vert in enumerate(temp_obj.data.vertices):
            shape_key.data[i].co = vert.co.copy()

        # Delete temporary objects
        bpy.data.objects.remove(temp_obj, do_unlink=True)



def get_evaluated_mesh(obj):
    """Get the mesh after applying modifiers"""
    depsgraph = bpy.context.evaluated_depsgraph_get()
    evaluated_obj = obj.evaluated_get(depsgraph)
    evaluated_mesh = evaluated_obj.data

    # Create a BMesh to retrieve information about the evaluated mesh
    bm = bmesh.new()
    bm.from_mesh(evaluated_mesh)
    bm.transform(obj.matrix_world)
    return bm

def create_side_weight_groups(mesh_obj: bpy.types.Object, base_avatar_data: dict, clothing_armature: bpy.types.Object, clothing_avatar_data: dict) -> None:
   """
   Create a vertex group for the total weight of the right and left halves
   """
   # Classify left and right bones
   left_bones, right_bones = set(), set()
   center_bones = set()

   # Legs, feet, toes, and chest bones grouped separately on the left and right sides
   leg_foot_chest_bones = {
       "LeftUpperLeg", "RightUpperLeg", "LeftLowerLeg", "RightLowerLeg",
       "LeftFoot", "RightFoot", "LeftToes", "RightToes", "LeftBreast", "RightBreast",
       "LeftFootThumbProximal", "LeftFootThumbIntermediate", "LeftFootThumbDistal",
       "LeftFootIndexProximal", "LeftFootIndexIntermediate", "LeftFootIndexDistal",
       "LeftFootMiddleProximal", "LeftFootMiddleIntermediate", "LeftFootMiddleDistal",
       "LeftFootRingProximal", "LeftFootRingIntermediate", "LeftFootRingDistal",
       "LeftFootLittleProximal", "LeftFootLittleIntermediate", "LeftFootLittleDistal",
       "RightFootThumbProximal", "RightFootThumbIntermediate", "RightFootThumbDistal",
       "RightFootIndexProximal", "RightFootIndexIntermediate", "RightFootIndexDistal",
       "RightFootMiddleProximal", "RightFootMiddleIntermediate", "RightFootMiddleDistal",
       "RightFootRingProximal", "RightFootRingIntermediate", "RightFootRingDistal",
       "RightFootLittleProximal", "RightFootLittleIntermediate", "RightFootLittleDistal"
   }

   # Finger bone to be placed in the right group
   right_group_fingers = {
       "LeftThumbProximal", "LeftThumbIntermediate", "LeftThumbDistal",
       "LeftMiddleProximal", "LeftMiddleIntermediate", "LeftMiddleDistal",
       "LeftLittleProximal", "LeftLittleIntermediate", "LeftLittleDistal",
       "RightThumbProximal", "RightThumbIntermediate", "RightThumbDistal",
       "RightMiddleProximal", "RightMiddleIntermediate", "RightMiddleDistal",
       "RightLittleProximal", "RightLittleIntermediate", "RightLittleDistal"
   }

   # Finger bone to be placed in the left group
   left_group_fingers = {
       "LeftIndexProximal", "LeftIndexIntermediate", "LeftIndexDistal",
       "LeftRingProximal", "LeftRingIntermediate", "LeftRingDistal",
       "RightIndexProximal", "RightIndexIntermediate", "RightIndexDistal",
       "RightRingProximal", "RightRingIntermediate", "RightRingDistal"
   }

   # Non-separating shoulder, arm, and hand bones (treated as center_bones)
   excluded_bones = {
       "LeftShoulder", "RightShoulder", "LeftUpperArm", "RightUpperArm",
       "LeftLowerArm", "RightLowerArm", "LeftHand", "RightHand"
   }

   ignored_bones = {"Head"}

   for bone_map in base_avatar_data.get("humanoidBones", []):
       bone_name = bone_map["boneName"]
       humanoid_name = bone_map["humanoidBoneName"]

       if bone_name in ignored_bones:
           continue
       if humanoid_name in excluded_bones:
           # Do not separate (treated as center_bones)
           center_bones.add(bone_name)
       elif humanoid_name in leg_foot_chest_bones:
           # Legs, feet, toes, and chest will continue to be separated by left and right sides
           if any(k in humanoid_name for k in ["Left", "left"]):
               left_bones.add(bone_name)
           elif any(k in humanoid_name for k in ["Right", "right"]):
               right_bones.add(bone_name)
       elif humanoid_name in right_group_fingers:
           # Finger bone to be placed in the right group
           right_bones.add(bone_name)
       elif humanoid_name in left_group_fingers:
           # Finger bone to be placed in the left group
           left_bones.add(bone_name)
       else:
           center_bones.add(bone_name)

   for aux_set in base_avatar_data.get("auxiliaryBones", []):
        humanoid_name = aux_set["humanoidBoneName"]
        for aux_bone in aux_set["auxiliaryBones"]:
            if humanoid_name in ignored_bones:
                continue
            if humanoid_name in excluded_bones:
                # Do not separate (treated as center_bones)
                center_bones.add(aux_bone)
            elif humanoid_name in leg_foot_chest_bones:
                # Legs, feet, toes, and chest will continue to be separated by left and right sides
                if is_left_side_bone(aux_bone, humanoid_name):
                    left_bones.add(aux_bone)
                elif is_right_side_bone(aux_bone, humanoid_name):
                    right_bones.add(aux_bone)
            elif humanoid_name in right_group_fingers:
                # Finger bone to be placed in the right group
                right_bones.add(aux_bone)
            elif humanoid_name in left_group_fingers:
                # Finger bone to be placed in the left group
                left_bones.add(aux_bone)
            else:
                center_bones.add(aux_bone)

   clothing_bone_to_humanoid = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                           for bone_map in clothing_avatar_data["humanoidBones"]}
   print(f"clothing_bone_to_humanoid: {clothing_bone_to_humanoid}")
   for clothing_bone in clothing_armature.data.bones:
        current_bone = clothing_bone
        current_bone_name = current_bone.name
        parent_humanoid_name = None
        while current_bone:
            if current_bone.name in clothing_bone_to_humanoid.keys():
                parent_humanoid_name = clothing_bone_to_humanoid[current_bone.name]
                break
            current_bone = current_bone.parent
        print(f"current_bone_name: {current_bone_name}, parent_humanoid_name: {parent_humanoid_name}")
        if parent_humanoid_name:
            if parent_humanoid_name in ignored_bones:
                continue
            if parent_humanoid_name in excluded_bones:
                # Do not separate (treated as center_bones)
                center_bones.add(current_bone_name)
            elif parent_humanoid_name in leg_foot_chest_bones:
                # Legs, feet, toes, and chest will continue to be separated by left and right sides
                if is_left_side_bone(current_bone_name, parent_humanoid_name):
                    left_bones.add(current_bone_name)
                elif is_right_side_bone(current_bone_name, parent_humanoid_name):
                    right_bones.add(current_bone_name)
            elif parent_humanoid_name in right_group_fingers:
                # Finger bone to be placed in the right group
                right_bones.add(current_bone_name)
            elif parent_humanoid_name in left_group_fingers:
                # Finger bone to be placed in the left group
                left_bones.add(current_bone_name)
            else:
                center_bones.add(current_bone_name)

   # Get existing vertex groups
   vertex_groups = {vg.name: vg.index for vg in mesh_obj.vertex_groups}

   # Create a new vertex group or clear an existing one
   for side in ["RightSideWeights", "LeftSideWeights", "BothSideWeights"]:
       if side in mesh_obj.vertex_groups:
           mesh_obj.vertex_groups.remove(mesh_obj.vertex_groups[side])
   right_group = mesh_obj.vertex_groups.new(name="RightSideWeights")
   left_group = mesh_obj.vertex_groups.new(name="LeftSideWeights")
   both_group = mesh_obj.vertex_groups.new(name="BothSideWeights")

   # Calculate the weight of each vertex
   for vert in mesh_obj.data.vertices:
       right_weight = 0.0
       left_weight = 0.0

       for g in vert.groups:
           group_name = mesh_obj.vertex_groups[g.group].name
           weight = g.weight

           if group_name in right_bones:
               right_weight += weight
           elif group_name in left_bones:
               left_weight += weight
           elif group_name in center_bones:
               # The central bone is added to both
               right_weight += weight
               left_weight += weight

       # Set weights for the new vertex group
       if right_weight > 0:
           right_group.add([vert.index], right_weight, 'REPLACE')
       if left_weight > 0:
           left_group.add([vert.index], left_weight, 'REPLACE')
       both_group.add([vert.index], 1.0, 'REPLACE')

def create_distance_falloff_transfer_mask(obj: bpy.types.Object,
                                        base_avatar_data: dict,
                                        group_name: str = "DistanceFalloffMask",
                                        max_distance: float = 0.025,
                                        min_distance: float = 0.002) -> bpy.types.VertexGroup:
    """
    Create a TransferMask vertex group that fades based on distance

    Parameters:
        obj: Target mesh object
        base_avatar_data: Base avatar data
        group_name: Name of the vertex group to generate (default: "DistanceFalloffMask")
        max_distance: Maximum distance at which weights become zero (default: 0.025)
        min_distance: Minimum distance at which the weight becomes 1 (default: 0.002)

    Returns:
        bpy.types.VertexGroup: Generated vertex group
    """
    # Input validation
    if obj.type != 'MESH':
        print(f"Error: {obj.name} is not a mesh object")
        return None

    # Retrieving the source object (Body.BaseAvatar)
    source_obj = bpy.data.objects.get("Body.BaseAvatar")
    if not source_obj:
        print("Error: Body.BaseAvatar not found")
        return None

    # Get the target mesh after applying modifiers
    target_bm = get_evaluated_mesh(source_obj)
    target_bm.faces.ensure_lookup_table()

    # Create the BVH tree for the target mesh
    bvh = BVHTree.FromBMesh(target_bm)

    # Get the source mesh after applying modifiers
    source_bm = get_evaluated_mesh(obj)
    source_bm.verts.ensure_lookup_table()

    # Create a new vertex group
    transfer_mask = obj.vertex_groups.new(name=group_name)

    # Process each vertex
    for vert_idx, vert in enumerate(obj.data.vertices):

        # Use vertex positions after applying modifiers
        evaluated_vertex_co = source_bm.verts[vert_idx].co

        # Obtain the nearest contact point and normal vector
        location, normal, index, distance = bvh.find_nearest(evaluated_vertex_co)

        if location is not None:
            # Calculate base weight based on distance
            if distance > max_distance:
                weight = 0.0
            else:
                d = distance - min_distance
                if d < 0.0:
                    d = 0.0
                weight = 1.0 - d / (max_distance - min_distance)

        # Add to vertex group
        transfer_mask.add([vert_idx], weight, 'REPLACE')

    # Clean up BMesh
    source_bm.free()
    target_bm.free()

    return transfer_mask

def get_humanoid_and_auxiliary_bone_groups(base_avatar_data):
    """Get the vertex groups for Humanoid bones and Auxiliary bones"""
    bone_groups = set()

    # Add Humanoid Bone
    for bone_map in base_avatar_data.get("humanoidBones", []):
        if "boneName" in bone_map:
            bone_groups.add(bone_map["boneName"])

    # Add an Auxiliary bone
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        for aux_bone in aux_set.get("auxiliaryBones", []):
            bone_groups.add(aux_bone)

    return bone_groups

def get_humanoid_and_auxiliary_bone_groups_with_intermediate(base_armature: bpy.types.Object, base_avatar_data: dict) -> set:
    bone_groups = set()

    # First, add the basic Humanoid bones and Auxiliary bones
    humanoid_bones = set()
    humanoid_name_to_bone = {}  # humanoidBoneName -> boneName mapping
    for bone_map in base_avatar_data.get("humanoidBones", []):
        if "boneName" in bone_map:
            bone_name = bone_map["boneName"]
            bone_groups.add(bone_name)
            humanoid_bones.add(bone_name)
            if "humanoidBoneName" in bone_map:
                humanoid_name_to_bone[bone_map["humanoidBoneName"]] = bone_name

    # Get the actual bone name of the Hips bone
    hips_bone_name = humanoid_name_to_bone.get("Hips")

    # Create a mapping between the Auxiliary bone and its associated Humanoid bone
    auxiliary_to_humanoid = {}
    humanoid_to_auxiliaries = {}

    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        humanoid_bone_name = aux_set.get("humanoidBoneName")
        auxiliaries = aux_set.get("auxiliaryBones", [])

        # Retrieve the actual bone name from the Humanoid bone name
        actual_humanoid_bone = None
        for bone_map in base_avatar_data.get("humanoidBones", []):
            if bone_map.get("humanoidBoneName") == humanoid_bone_name:
                actual_humanoid_bone = bone_map.get("boneName")
                break

        if actual_humanoid_bone:
            humanoid_to_auxiliaries[actual_humanoid_bone] = set(auxiliaries)
            for aux_bone in auxiliaries:
                bone_groups.add(aux_bone)
                auxiliary_to_humanoid[aux_bone] = actual_humanoid_bone

    # Detect and add intermediate bones
    if base_armature and base_armature.pose:
        # Humanoid Bone Parent Tracing
        for bone in base_armature.pose.bones:
            if bone.name in humanoid_bones:
                # Special handling for the Hips bone: Add all parent bones up to the root
                if bone.name == hips_bone_name:
                    current_parent = bone.parent
                    while current_parent:
                        bone_groups.add(current_parent.name)
                        current_parent = current_parent.parent
                else:
                    # Processing of standard Humanoid bones
                    # Trace the parent of this Humanoid bone
                    current_parent = bone.parent
                    intermediate_bones = []

                    while current_parent:
                        if current_parent.name in humanoid_bones:
                            # Once you reach the parent Humanoid bone, add all intermediate bones
                            bone_groups.update(intermediate_bones)
                            break
                        else:
                            # Record as an intermediate bone
                            intermediate_bones.append(current_parent.name)
                            current_parent = current_parent.parent

        # Auxiliary Bone Parent Tracing Process
        for aux_bone_name in auxiliary_to_humanoid.keys():
            if aux_bone_name in base_armature.pose.bones:
                bone = base_armature.pose.bones[aux_bone_name]
                parent_humanoid_bone = auxiliary_to_humanoid[aux_bone_name]
                same_group_bones = {parent_humanoid_bone} | humanoid_to_auxiliaries.get(parent_humanoid_bone, set())

                # Trace the parent of this Auxiliary bone
                current_parent = bone.parent
                intermediate_bones = []

                while current_parent:
                    if current_parent.name in same_group_bones:
                        # When reaching the same group's bone, add all intermediate bones
                        bone_groups.update(intermediate_bones)
                        break
                    else:
                        # Record as an intermediate bone
                        intermediate_bones.append(current_parent.name)
                        current_parent = current_parent.parent

    return bone_groups

def normalize_connected_components_weights(obj, base_avatar_data):
    """
    Normalize the weights for each connected component of the mesh
    Processing is performed only when the weights of all groups of Humanoid bones and auxiliary bones within the linked component are uniform

    Parameters:
        obj: Mesh object to be processed
        base_avatar_data: Base Avatar Data
    """
    # Create a BMesh
    bm = bmesh.new()
    bm.from_mesh(obj.data)
    bm.verts.ensure_lookup_table()
    bm.edges.ensure_lookup_table()
    bm.faces.ensure_lookup_table()

    # Find the connecting element
    def find_connected_component(start_vert, visited):
        """Finding connected components using depth-first search"""
        component = {start_vert.index}
        stack = [start_vert]

        while stack:
            current = stack.pop()
            for edge in current.link_edges:
                other = edge.other_vert(current)
                if other.index not in visited:
                    visited.add(other.index)
                    component.add(other.index)
                    stack.append(other)

        return component

    # Get all concatenated components
    visited = set()
    components = []

    for vert in bm.verts:
        if vert.index not in visited:
            visited.add(vert.index)
            component = find_connected_component(vert, visited)
            components.append(component)

    # Get the vertex group to be checked
    target_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

    # Extract only the target groups existing within the mesh
    existing_target_groups = {vg.name for vg in obj.vertex_groups if vg.name in target_groups}

    # Processing for each concatenated component
    for component in components:
        # Collect the weight patterns for each vertex within the component
        vertex_weights = []
        for vert_idx in component:
            vert = obj.data.vertices[vert_idx]
            weights = {group: 0.0 for group in existing_target_groups}

            for g in vert.groups:
                group_name = obj.vertex_groups[g.group].name
                if group_name in existing_target_groups:
                    weights[group_name] = g.weight

            vertex_weights.append(weights)

        # Check if the same weight pattern exists in all groups under review
        is_uniform = True
        first_weights = vertex_weights[0]

        for weights in vertex_weights[1:]:
            for group_name in existing_target_groups:
                if abs(weights[group_name] - first_weights[group_name]) >= 0.0001:
                    is_uniform = False
                    break
            if not is_uniform:
                break

        # When the weights for all target groups are uniform, apply the average value
        if is_uniform:
            # Calculate the average weight (in this case, since it's the same for all vertices, use the weight of the first vertex)
            avg_weights = first_weights

            # Apply an average weight to all vertices
            for vert_idx in component:
                for group_name, avg_weight in avg_weights.items():
                    group = obj.vertex_groups[group_name]
                    if avg_weight > 0:
                        group.add([vert_idx], avg_weight, 'REPLACE')
                    else:
                        try:
                            group.remove([vert_idx])
                        except RuntimeError:
                            pass

    # Release BMesh
    bm.free()

def adjust_hand_weights(target_obj, armature, base_avatar_data):

    def get_bone_name(humanoid_bone_name):
        """Retrieve actual bone names from Humanoid bone names"""
        for bone_data in base_avatar_data.get("humanoidBones", []):
            if bone_data.get("humanoidBoneName") == humanoid_bone_name:
                return bone_data.get("boneName")
        return None

    def get_finger_bones(side_prefix):
        """Get finger bone names (excluding toes)"""
        finger_bones = []
        finger_types = ["Thumb", "Index", "Middle", "Ring", "Little"]
        positions = ["Proximal", "Intermediate", "Distal"]

        for finger in finger_types:
            for pos in positions:
                humanoid_name = f"{side_prefix}{finger}{pos}"
                # Process only Humanoid bone names that do not contain "Foot"
                if "Foot" not in humanoid_name:
                    bone_name = get_bone_name(humanoid_name)
                    if bone_name:
                        finger_bones.append(bone_name)

        return finger_bones

    def get_bone_head_world(bone_name):
        """Get the bone's head position in world coordinates"""
        bone = armature.pose.bones[bone_name]
        return armature.matrix_world @ bone.head

    def get_lowerarm_and_auxiliary_bones(side_prefix):
        """Get the LowerArm and its auxiliary bone"""
        lower_arm_bones = []

        # Add a LowerArm bone
        lower_arm_name = get_bone_name(f"{side_prefix}LowerArm")
        if lower_arm_name:
            lower_arm_bones.append(lower_arm_name)

        # Add auxiliary bones
        for aux_set in base_avatar_data.get("auxiliaryBones", []):
            if aux_set["humanoidBoneName"] == f"{side_prefix}LowerArm":
                lower_arm_bones.extend(aux_set["auxiliaryBones"])

        return lower_arm_bones

    def find_closest_lower_arm_bone(hand_head_pos, lower_arm_bones):
        """Find the LowerArm or auxiliary bone closest to the Head of the hand bone"""
        closest_bone = None
        min_distance = float('inf')

        for bone_name in lower_arm_bones:
            if bone_name in armature.pose.bones:
                bone_head = get_bone_head_world(bone_name)
                distance = (Vector(bone_head) - hand_head_pos).length
                if distance < min_distance:
                    min_distance = distance
                    closest_bone = bone_name

        return closest_bone

    def process_hand(is_right):
        # Set Humanoid bone names according to hand type
        side = "Right" if is_right else "Left"
        hand_bone_name = get_bone_name(f"{side}Hand")
        lower_arm_bone_name = get_bone_name(f"{side}LowerArm")

        if not hand_bone_name or not lower_arm_bone_name:
            return

        # Collecting hand and finger bone names
        vertex_groups = [hand_bone_name] + get_finger_bones(side)

        # Get the bone position in world coordinates
        hand_head = Vector(get_bone_head_world(hand_bone_name))
        lower_arm_head = Vector(get_bone_head_world(lower_arm_bone_name))

        # Calculate the forward direction vector
        tip_direction = (hand_head - lower_arm_head).normalized()

        # Find the minimum angle
        min_angle = float('inf')
        has_weight = False

        # Process each vertex
        for v in target_obj.data.vertices:
            has_vertex_weight = False
            for group_name in vertex_groups:
                if group_name not in target_obj.vertex_groups:
                    continue
                weight = 0
                try:
                    for g in v.groups:
                        if g.group == target_obj.vertex_groups[group_name].index:
                            weight = g.weight
                            break
                    if weight > 0:
                        has_weight = True
                        has_vertex_weight = True
                except RuntimeError:
                    continue

            # If this vertex has a weight for the hand or finger
            if has_vertex_weight:
                # Calculate the world coordinates of the vertex
                vertex_world = target_obj.matrix_world @ Vector(v.co)
                # Vector from vertex to hand bone
                vertex_vector = (vertex_world - hand_head).normalized()
                # Calculate the angle (within the range of 0-180 degrees)
                # Calculate the angle using the dot product
                dot_product = vertex_vector.dot(tip_direction)
                # Clamp to the range from -1.0 to 1.0
                dot_product = max(min(dot_product, 1.0), -1.0)
                angle = np.degrees(np.arccos(dot_product))
                min_angle = min(min_angle, angle)

        if not has_weight:
            return

        # Handling at temperatures above 70 degrees
        if min_angle >= 70:
            print(f"- Minimum angle exceeds 70 degrees ({min_angle} degrees), transferring weights for {side} hand")

            # Get the LowerArm and its auxiliary bone
            lower_arm_bones = get_lowerarm_and_auxiliary_bones(side)

            # Find the LowerArm bone closest to the Head bone of the hand
            closest_bone = find_closest_lower_arm_bone(hand_head, lower_arm_bones)

            if closest_bone:
                print(f"- Transferring weights to {closest_bone}")

                # Process each vertex
                for v in target_obj.data.vertices:
                    total_weight = 0.0

                    # Sum the weights of the hand and finger bones
                    for group_name in vertex_groups:
                        if group_name in target_obj.vertex_groups:
                            group = target_obj.vertex_groups[group_name]
                            try:
                                for g in v.groups:
                                    if g.group == group.index:
                                        total_weight += g.weight
                                        break
                            except RuntimeError:
                                continue

                    # Transfer the weight to the nearest LowerArm bone
                    if total_weight > 0:
                        if closest_bone not in target_obj.vertex_groups:
                            target_obj.vertex_groups.new(name=closest_bone)
                        target_obj.vertex_groups[closest_bone].add([v.index], total_weight, 'ADD')

                    # Remove original weight
                    for group_name in vertex_groups:
                        if group_name in target_obj.vertex_groups:
                            try:
                                target_obj.vertex_groups[group_name].remove([v.index])
                            except RuntimeError:
                                continue
            else:
                print(f"Warning: No suitable LowerArm bone found for {side} hand")
        else:
            print(f"- Minimum angle is within acceptable range ({min_angle} degrees), keeping weights for {side} hand")

    # Execute processing for both hands
    process_hand(is_right=True)
    process_hand(is_right=False)

def create_distance_normal_based_vertex_group(body_obj, cloth_obj, distance_threshold=0.1, min_distance_threshold=0.005, angle_threshold=30.0, new_group_name="InpaintMask", normal_radius=0.01, filter_mask=None):
    """
    Create vertex groups on the clothing mesh based on the distance and normal angle from the base mesh

    Parameters:
    body_obj (obj): Object name of the base mesh
    cloth_obj (obj): Object name of the clothing mesh
    distance_threshold (float): If the distance exceeds this value, set the weight to 1.0
    min_distance_threshold (float): If the distance is less than or equal to this value, set the weight to 0.0
    angle_threshold (float): Set weight to 1.0 when angle exceeds this value (in degrees)
    new_group_name (str): Name of the vertex group to be created
    normal_radius (float): The radius of the sphere considered when performing proximity searches on surfaces
    filter_mask (obj): The vertex group used for filtering
    """
    start_time = time.time()

    if not body_obj or not cloth_obj:
        print("The specified object was not found")
        return

    # Save current mode
    current_mode = bpy.context.object.mode
    # Switch to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Select the clothing object to make it active
    bpy.ops.object.select_all(action='DESELECT')
    cloth_obj.select_set(True)
    bpy.context.view_layer.objects.active = cloth_obj

    # Create a BVH tree (for fast nearest point search)
    # Get the target mesh after applying modifiers
    body_bm_time_start = time.time()
    body_bm = get_evaluated_mesh(body_obj)
    body_bm.verts.ensure_lookup_table()
    body_bm.faces.ensure_lookup_table()
    body_bm.normal_update()
    body_bm_time = time.time() - body_bm_time_start
    print(f"  Body BMesh creation: {body_bm_time:.2f} seconds")

    # Create the BVH tree for the target mesh
    bvh_time_start = time.time()
    bvh_tree = BVHTree.FromBMesh(body_bm)
    bvh_time = time.time() - bvh_time_start
    print(f"  BVH Tree Creation: {bvh_time:.2f} seconds")

    # If the vertex group does not yet exist, create it
    if new_group_name not in cloth_obj.vertex_groups:
        cloth_obj.vertex_groups.new(name=new_group_name)
    vertex_group = cloth_obj.vertex_groups[new_group_name]

    # Convert the angle threshold to radians
    angle_threshold_rad = math.radians(angle_threshold)

    # Get the source mesh after applying modifiers
    cloth_bm_time_start = time.time()
    cloth_bm = get_evaluated_mesh(cloth_obj)
    cloth_bm.verts.ensure_lookup_table()
    cloth_bm.faces.ensure_lookup_table()
    cloth_bm.normal_update()
    cloth_bm_time = time.time() - cloth_bm_time_start
    print(f"  Cloth BMesh creation: {cloth_bm_time:.2f} seconds")

    # Cache the transform matrix (to avoid repeated calculations)
    body_normal_matrix = body_obj.matrix_world.inverted().transposed()
    cloth_normal_matrix = cloth_obj.matrix_world.inverted().transposed()

    # Dictionary storing corrected normals
    adjusted_normals_time_start = time.time()
    adjusted_normals = {}

    # Processing normals for each vertex of the clothing mesh (checking if reversal is necessary)
    for i, vertex in enumerate(cloth_bm.verts):
        # Vertex positions and normals in the world coordinate system
        cloth_vert_world = vertex.co
        original_normal_world = (Vector((vertex.normal[0], vertex.normal[1], vertex.normal[2], 0))).xyz.normalized()

        # Search for the nearest face on the base mesh
        nearest_result = bvh_tree.find_nearest(cloth_vert_world)
        if nearest_result:
            # BVHTree.find_nearest() returns (co, normal, index, distance)
            nearest_point, nearest_normal, nearest_face_index, _ = nearest_result

            # Obtain the nearest neighbor face
            face = body_bm.faces[nearest_face_index]
            face_normal = face.normal

            # Convert the surface normal to world coordinates
            face_normal_world = (Vector((face_normal[0], face_normal[1], face_normal[2], 0))).xyz.normalized()

            # If the dot product is negative, reverse the normal vector
            dot_product = original_normal_world.dot(face_normal_world)
            if dot_product < 0:
                adjusted_normal = -original_normal_world
            else:
                adjusted_normal = original_normal_world

            # Save adjusted normals to dictionary
            adjusted_normals[i] = adjusted_normal
        else:
            # If the nearest point cannot be found, use the original normal
            adjusted_normals[i] = original_normal_world
    adjusted_normals_time = time.time() - adjusted_normals_time_start
    print(f"  Normal adjustment: {adjusted_normals_time:.2f} seconds")

    # Pre-calculate and cache the center point and area of each face
    face_cache_time_start = time.time()
    face_centers = []
    face_areas = {}
    face_adjusted_normals = {}
    face_indices = []

    for face in cloth_bm.faces:
        # Calculate the center point of the surface
        center = Vector((0, 0, 0))
        for v in face.verts:
            center += v.co
        center /= len(face.verts)
        face_centers.append(center)
        face_indices.append(face.index)

        # Calculate the area
        face_areas[face.index] = face.calc_area()

        # Calculate adjusted surface normals
        face_normal = Vector((0, 0, 0))
        for v in face.verts:
            face_normal += adjusted_normals[v.index]
        face_adjusted_normals[face.index] = face_normal.normalized()
    face_cache_time = time.time() - face_cache_time_start
    print(f"  Face cache creation: {face_cache_time:.2f} seconds")

    # Construct a KDTree for the faces of the clothing mesh
    kdtree_time_start = time.time()
    # size = len(cloth_bm.faces)
    # kd = mathutils.kdtree.KDTree(size)

    # for face_index, center in face_centers.items():
    #     kd.insert(center, face_index)

    # kd.balance()
    # kd = cKDTree(face_centers)

    # Build a KDTree for the vertices of the clothing mesh (for the new implementation)
    vertex_positions = []
    for vertex in cloth_bm.verts:
        vertex_positions.append(vertex.co)
    vertex_kd = cKDTree(vertex_positions)

    kdtree_time = time.time() - kdtree_time_start
    print(f"  KDTree construction: {kdtree_time:.2f} seconds")

    # Ready to search for faces with portions within a fixed distance from each vertex
    normal_avg_time_start = time.time()
    normal_avg_time = time.time() - normal_avg_time_start
    print(f"  Nearest neighbor search preparation complete: {normal_avg_time:.2f} seconds")
    # ----------------------------------

    # Processing for each vertex of the clothing mesh
    weight_calc_time_start = time.time()
    for i, vertex in enumerate(cloth_bm.verts):
        # Vertex position in the world coordinate system
        cloth_vert_world = vertex.co

        # Use adjusted normals
        cloth_normal_world = adjusted_normals[i]

        # Search for the nearest face on the base mesh
        nearest_result = bvh_tree.find_nearest(cloth_vert_world)
        distance = float('inf')  # Set the initial value to infinity

        if nearest_result:
            # BVHTree.find_nearest() returns (co, normal, index, distance)
            nearest_point, nearest_normal, nearest_face_index, _ = nearest_result

            # Obtain the nearest neighbor face
            face = body_bm.faces[nearest_face_index]
            face_normal = face.normal

            # Calculate the nearest point on the surface
            closest_point_on_face = mathutils.geometry.closest_point_on_tri(
                cloth_vert_world,
                face.verts[0].co,
                face.verts[1].co,
                face.verts[2].co
            )

            # Convert the surface normal to world coordinates
            face_normal_world = (Vector((face_normal[0], face_normal[1], face_normal[2], 0))).xyz.normalized()

            # Calculate distance
            distance = (cloth_vert_world - closest_point_on_face).length

            # Set the nearest point and normal
            nearest_point = closest_point_on_face
            nearest_normal = face_normal_world
        else:
            # If the nearest point cannot be found, set the initial value to None
            nearest_point = None
            nearest_normal = None

        # Initial vertex weight
        weight = 0.0

        if nearest_point and distance >= min_distance_threshold:
            # Distance-based weighting
            if distance >= distance_threshold:
                weight = 1.0

            # Normal Angle-Based Weighting (New Logic)
            if weight < 1.0 and nearest_normal:
                # Retrieve all faces within a specified distance from the vertices of the clothing mesh
                min_angle = float('inf')

                # Search for a face in the cloth_vert_world that contains at least one vertex within the normal_radius range
                nearby_vertex_indices = vertex_kd.query_ball_point(cloth_vert_world, normal_radius)
                nearby_faces = set()

                # Search for faces containing nearby vertices
                for vertex_idx in nearby_vertex_indices:
                    vertex = cloth_bm.verts[vertex_idx]
                    for face in vertex.link_faces:
                        nearby_faces.add(face.index)

                nearby_faces = list(nearby_faces)

                if nearby_faces:
                    for face_index in nearby_faces:
                        # Get the surface normal
                        face_normal = face_adjusted_normals[face_index]

                        # Calculate the angle between the base mesh's surface and the normal of the nearest surface
                        angle = math.acos(min(1.0, max(-1.0, face_normal.dot(nearest_normal))))

                        # If the angle exceeds 90 degrees, invert the normal and recalculate
                        if angle > math.pi / 2:
                            inverted_normal = -nearest_normal
                            angle = math.acos(min(1.0, max(-1.0, face_normal.dot(inverted_normal))))

                        # Update minimum angle
                        min_angle = min(min_angle, angle)
                else:
                    # If no neighboring face is found, use the original vertex normal
                    original_adjusted_normal = adjusted_normals[i]
                    min_angle = math.acos(min(1.0, max(-1.0, original_adjusted_normal.dot(nearest_normal))))

                    # If the angle exceeds 90 degrees, invert the normal and recalculate
                    if min_angle > math.pi / 2:
                        inverted_normal = -nearest_normal
                        min_angle = math.acos(min(1.0, max(-1.0, original_adjusted_normal.dot(inverted_normal))))

                # When the minimum angle threshold is exceeded
                if min_angle >= angle_threshold_rad:
                    weight = 1.0

        # Set weights for vertex groups
        vertex_group.add([i], weight, 'REPLACE')
    weight_calc_time = time.time() - weight_calc_time_start
    print(f"  Weight calculation: {weight_calc_time:.2f} seconds")

    # Set the vertex group as active
    cloth_obj.vertex_groups.active_index = vertex_group.index

    # Switch to Weight Paint mode
    bpy.ops.object.mode_set(mode='WEIGHT_PAINT')

    # Execute smoothing processing (applied to the active vertex group)
    smooth_time_start = time.time()
    bpy.ops.object.vertex_group_smooth(factor=0.3, repeat=10, expand=0.25)

    # Cleaning processing is also applied
    bpy.ops.object.vertex_group_clean(group_select_mode='ACTIVE', limit=0.5)
    smooth_time = time.time() - smooth_time_start
    print(f"  Smoothing processing: {smooth_time:.2f} seconds")

    apply_max_filter_to_vertex_group(cloth_obj, new_group_name, filter_radius=0.01, filter_mask=filter_mask)
    apply_min_filter_to_vertex_group(cloth_obj, new_group_name, filter_radius=0.01, filter_mask=filter_mask)

    # Restore to original mode
    bpy.ops.object.mode_set(mode=current_mode)

    # Clean up BMesh
    body_bm.free()
    cloth_bm.free()

    total_time = time.time() - start_time
    print(f"{new_group_name} vertex group created (Total time: {total_time:.2f} seconds)")
    return vertex_group


def apply_smoothing_to_vertex_group(cloth_obj, vertex_group_name, smoothing_radius=0.02, iteration=1, use_distance_weighting=True, gaussian_falloff=True, neighbors_cache=None):
    """
    Applies smoothing processing to the specified vertex group
    We obtain results robust to vertex density skew using distance-weighted smoothing

    Parameters:
    cloth_obj (obj): Clothing mesh object
    vertex_group_name (str): Target vertex group name
    smoothing_radius (float): Smoothing radius
    use_distance_weighting (bool): Whether to use distance-based weighting
    gaussian_falloff (bool): Whether to use Gaussian falloff
    """
    start_time = time.time()

    if vertex_group_name not in cloth_obj.vertex_groups:
        print(f"Error: Vertex group '{vertex_group_name}' not found")
        return

    vertex_group = cloth_obj.vertex_groups[vertex_group_name]

    # Save current mode
    current_mode = bpy.context.object.mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Get the mesh after applying modifiers
    cloth_bm = get_evaluated_mesh(cloth_obj)
    cloth_bm.verts.ensure_lookup_table()

    # Convert vertex coordinates to a numpy array
    vertex_coords = np.array([v.co for v in cloth_bm.verts])
    num_vertices = len(vertex_coords)

    # Get the current weight value
    current_weights = np.zeros(num_vertices, dtype=np.float32)
    for i, vertex in enumerate(cloth_obj.data.vertices):
        for group in vertex.groups:
            if group.group == vertex_group.index:
                current_weights[i] = group.weight
                break

    # Initialize the smoothed weight array
    smoothed_weights = np.copy(current_weights)

    print(f"  Smoothing processing started (Radius: {smoothing_radius}, Distance weighting: {use_distance_weighting}, Gaussian falloff: {gaussian_falloff})")

    # The sigma value of the Gaussian function (approximately 1/3 of the radius is appropriate)
    sigma = smoothing_radius / 3.0

    # Cache neighbor_indices in the first iteration
    if neighbors_cache is None:
        # Using cKDTree to Optimize Neighborhood Searches
        kdtree = cKDTree(vertex_coords)
        neighbors_cache = [np.array(x) for x in kdtree.query_ball_point(vertex_coords, smoothing_radius)]

    for iteration_idx in range(iteration):
        # Apply smoothing to each vertex
        for i in range(num_vertices):
            neighbor_indices = neighbors_cache[i]
            if len(neighbor_indices) > 1:  # When there exists a neighborhood other than oneself
                # Calculate the distance to neighboring vertices
                neighbor_coords = vertex_coords[neighbor_indices]
                distances = np.linalg.norm(neighbor_coords - vertex_coords[i], axis=1)

                # Retrieve the weight values of neighboring vertices
                neighbor_weights = current_weights[neighbor_indices]

                if use_distance_weighting:
                    if gaussian_falloff:
                        # Weight calculation using Gaussian decay
                        weights = np.exp(-(distances ** 2) / (2 * sigma ** 2))
                    else:
                        # Weight calculation by linear decay
                        weights = np.maximum(0, 1.0 - distances / smoothing_radius)

                    # Set your own weight slightly higher (retain original value)
                    # self_index = np.where(distances == 0)[0]
                    # if len(self_index) > 0:
                    #     weights[self_index[0]] *= 2.0

                    # Calculate the weighted average
                    weights_sum = np.sum(weights)
                    if weights_sum > 0.001:
                        smoothed_weights[i] = neighbor_weights @ weights / weights_sum
                    else:
                        smoothed_weights[i] = current_weights[i]
                else:
                    # Conventional simple average
                    smoothed_weights[i] = np.mean(neighbor_weights)
            else:
                # If the nearest vertex is unique to itself, retain the original value
                smoothed_weights[i] = current_weights[i]
        current_weights = np.copy(smoothed_weights)

    # Apply new weights to the vertex group
    for i in range(num_vertices):
        vertex_group.add([i], smoothed_weights[i], 'REPLACE')

    # Clean up BMesh
    cloth_bm.free()

    # Restore to original mode
    bpy.ops.object.mode_set(mode=current_mode)

    total_time = time.time() - start_time
    print(f"  Smoothing complete: {total_time:.2f} seconds")

    return neighbors_cache


def apply_max_filter_to_vertex_group(cloth_obj, vertex_group_name, filter_radius=0.02, filter_mask=None):
    """
    Apply the Max filter to the vertex group
    Obtain the maximum weight value of vertices within a fixed distance from each vertex, and set that value as the new weight

    Parameters:
    cloth_obj (obj): Clothing mesh object
    vertex_group_name (str): Target vertex group name
    filter_radius (float): Filter application radius
    filter_mask (obj): The vertex group used for filtering
    """
    start_time = time.time()

    if vertex_group_name not in cloth_obj.vertex_groups:
        print(f"Error: Vertex group '{vertex_group_name}' not found")
        return

    vertex_group = cloth_obj.vertex_groups[vertex_group_name]

    # Save current mode
    current_mode = bpy.context.object.mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Get the mesh after applying modifiers
    cloth_bm = get_evaluated_mesh(cloth_obj)
    cloth_bm.verts.ensure_lookup_table()

    # Convert vertex coordinates to a numpy array
    vertex_coords = np.array([v.co for v in cloth_bm.verts])
    num_vertices = len(vertex_coords)

    # Get the current weight value
    current_weights = np.zeros(num_vertices, dtype=np.float32)
    for i, vertex in enumerate(cloth_bm.verts):
        # Get the weight of the vertex group
        weight = 0.0
        for group in cloth_obj.data.vertices[i].groups:
            if group.group == vertex_group.index:
                weight = group.weight
                break
        current_weights[i] = weight

    # Using cKDTree to Optimize Neighborhood Searches
    kdtree = cKDTree(vertex_coords)

    # Initialize the new weight array
    new_weights = np.copy(current_weights)

    print(f"  Max filter processing start (radius: {filter_radius})")

    # Apply the Max filter to each vertex
    for i in range(num_vertices):
        # Retrieve the indices of neighboring vertices within a specified radius
        neighbor_indices = kdtree.query_ball_point(vertex_coords[i], filter_radius)

        if neighbor_indices:
            # Get the maximum weight of neighboring vertices
            neighbor_weights = current_weights[neighbor_indices]
            max_weight = np.max(neighbor_weights)
            if filter_mask is not None:
                new_weights[i] = filter_mask[i] * max_weight + (1 - filter_mask[i]) * current_weights[i]
            else:
                new_weights[i] = max_weight

    # Apply new weights to the vertex group
    for i in range(num_vertices):
        vertex_group.add([i], new_weights[i], 'REPLACE')

    # Clean up BMesh
    cloth_bm.free()

    # Restore to original mode
    bpy.ops.object.mode_set(mode=current_mode)

    total_time = time.time() - start_time
    print(f"  Max filter complete: {total_time:.2f} seconds")

def apply_min_filter_to_vertex_group(cloth_obj, vertex_group_name, filter_radius=0.02, filter_mask=None):
    """
    Apply the Min filter to the vertex group
    Obtain the minimum weight value among vertices within a fixed distance from each vertex, and set that value as the new weight

    Parameters:
    cloth_obj (obj): Clothing mesh object
    vertex_group_name (str): Target vertex group name
    filter_radius (float): Filter application radius
    filter_mask (obj): The vertex group used for filtering
    """
    start_time = time.time()

    if vertex_group_name not in cloth_obj.vertex_groups:
        print(f"Error: Vertex group '{vertex_group_name}' not found")
        return

    vertex_group = cloth_obj.vertex_groups[vertex_group_name]

    # Save current mode
    current_mode = bpy.context.object.mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Get the mesh after applying modifiers
    cloth_bm = get_evaluated_mesh(cloth_obj)
    cloth_bm.verts.ensure_lookup_table()

    # Convert vertex coordinates to a numpy array
    vertex_coords = np.array([v.co for v in cloth_bm.verts])
    num_vertices = len(vertex_coords)

    # Get the current weight value
    current_weights = np.zeros(num_vertices, dtype=np.float32)
    for i, vertex in enumerate(cloth_bm.verts):
        # Get the weight of the vertex group
        weight = 0.0
        for group in cloth_obj.data.vertices[i].groups:
            if group.group == vertex_group.index:
                weight = group.weight
                break
        current_weights[i] = weight

    # Using cKDTree to Optimize Neighborhood Searches
    kdtree = cKDTree(vertex_coords)

    # Initialize the new weight array
    new_weights = np.copy(current_weights)

    print(f"  Min filter processing start (radius: {filter_radius})")

    # Apply the Min filter to each vertex
    for i in range(num_vertices):
        # Retrieve the indices of neighboring vertices within a specified radius
        neighbor_indices = kdtree.query_ball_point(vertex_coords[i], filter_radius)

        if neighbor_indices:
            # Get the minimum weight of neighboring vertices
            neighbor_weights = current_weights[neighbor_indices]
            min_weight = np.min(neighbor_weights)
            if filter_mask is not None:
                new_weights[i] = filter_mask[i] * min_weight + (1 - filter_mask[i]) * current_weights[i]
            else:
                new_weights[i] = min_weight

    # Apply new weights to the vertex group
    for i in range(num_vertices):
        vertex_group.add([i], new_weights[i], 'REPLACE')

    # Clean up BMesh
    cloth_bm.free()

    # Restore to original mode
    bpy.ops.object.mode_set(mode=current_mode)

    total_time = time.time() - start_time
    print(f"  Min filter complete: {total_time:.2f} seconds")

def get_mesh_cache_key(obj):
    """
    Generates an object cache key
    Create a hash considering the number of vertices, number of faces, and presence of modifiers in the mesh
    """
    mesh = obj.data
    modifiers_str = "_".join([mod.name + str(mod.type) for mod in obj.modifiers])
    cache_key = f"{obj.name}_{len(mesh.vertices)}_{len(mesh.polygons)}_{modifiers_str}_{obj.matrix_world.determinant():.6f}"
    return cache_key


def get_cached_mesh_data(obj, smoothing_radius):
    """
    Retrieve an object's mesh data from the cache, or create a new one and save it to the cache
    """
    global _mesh_cache

    cache_key = get_mesh_cache_key(obj)
    radius_key = f"{cache_key}_{smoothing_radius:.6f}"

    if radius_key in _mesh_cache:
        print(f"  Retrieve mesh data from cache: {obj.name}")
        return _mesh_cache[radius_key]

    print(f"  Create new mesh data: {obj.name}")

    # Get the mesh after applying modifiers
    cloth_bm = get_evaluated_mesh(obj)
    cloth_bm.verts.ensure_lookup_table()
    cloth_bm.faces.ensure_lookup_table()

    # Retrieve vertex coordinates and normals
    vertex_coords = np.array([v.co for v in cloth_bm.verts])
    vertex_normals = np.array([v.normal for v in cloth_bm.verts])

    # Construct a cKDTree
    kdtree = cKDTree(vertex_coords)

    # Precomputing the neighborhood information for all vertices
    neighbor_data = {}
    for i in range(len(vertex_coords)):
        neighbor_indices = kdtree.query_ball_point(vertex_coords[i], smoothing_radius)
        neighbor_data[i] = neighbor_indices

    # Create cache data
    cache_data = {
        'vertex_coords': vertex_coords,
        'vertex_normals': vertex_normals,
        'kdtree': kdtree,
        'neighbor_data': neighbor_data,
        'bmesh': cloth_bm
    }

    # Save to cache
    _mesh_cache[radius_key] = cache_data

    return cache_data


def clear_mesh_cache():
    """
    Clear the mesh cache
    """
    global _mesh_cache

    # Release the BMesh object
    for cache_data in _mesh_cache.values():
        if 'bmesh' in cache_data and cache_data['bmesh']:
            cache_data['bmesh'].free()

    _mesh_cache.clear()
    print("Mesh cache cleared")

def apply_distance_normal_based_smoothing(body_obj, cloth_obj, distance_min=0.0, distance_max=0.1, angle_min=0.0, angle_max=30.0, new_group_name="InpaintMask", normal_radius=0.01, smoothing_mask_groups=None, target_vertex_groups=None, smoothing_radius=0.02, mask_group_name=None):
    """
    Create vertex groups on the clothing mesh based on distance and normal angle from the base mesh, then apply smoothing

    Parameters:
    body_obj (obj): Object name of the base mesh
    cloth_obj (obj): Object name of the clothing mesh
    distance_min (float): minimum distance value, below this value the weight 0.0
    distance_max (float): maximum distance value, weight 1.0 above this value
    angle_min (float): Minimum angle value, below this value, weight is 0.0 (in degrees)
    angle_max (float): Maximum angle value, beyond this value, weight 1.0 (in degrees)
    new_group_name (str): Name of the vertex group to be created
    normal_radius (float): The radius of the sphere considered when calculating the weighted average of normals
    smoothing_mask_groups (list): A list of vertex group names to apply as smoothing masks
    target_vertex_groups (list): List of vertex group names to smooth
    smoothing_radius (float): Distance used for smoothing
    mask_group_name (str): Name of the mask vertex group for the composite intensity of the smoothing processing result
    """
    start_time = time.time()

    if not body_obj or not cloth_obj:
        print("The specified object was not found")
        return

    # Save current mode
    current_mode = bpy.context.object.mode
    # Switch to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Select the clothing object to make it active
    bpy.ops.object.select_all(action='DESELECT')
    cloth_obj.select_set(True)
    bpy.context.view_layer.objects.active = cloth_obj

    # Create a BVH tree (for fast nearest point search)
    # Get the target mesh after applying modifiers
    body_bm_time_start = time.time()
    body_bm = get_evaluated_mesh(body_obj)
    body_bm.faces.ensure_lookup_table()
    body_bm_time = time.time() - body_bm_time_start
    print(f"  Body BMesh creation: {body_bm_time:.2f} seconds")

    # Create the BVH tree for the target mesh
    bvh_time_start = time.time()
    bvh_tree = BVHTree.FromBMesh(body_bm)
    bvh_time = time.time() - bvh_time_start
    print(f"  BVH Tree Creation: {bvh_time:.2f} seconds")

    # If the vertex group does not yet exist, create it
    if new_group_name not in cloth_obj.vertex_groups:
        cloth_obj.vertex_groups.new(name=new_group_name)
    vertex_group = cloth_obj.vertex_groups[new_group_name]

    # Convert the minimum and maximum angles to radians
    angle_min_rad = math.radians(angle_min)
    angle_max_rad = math.radians(angle_max)

    # Get the source mesh after applying modifiers
    cloth_bm_time_start = time.time()
    cloth_bm = get_evaluated_mesh(cloth_obj)
    cloth_bm.verts.ensure_lookup_table()
    cloth_bm.faces.ensure_lookup_table()
    cloth_bm_time = time.time() - cloth_bm_time_start
    print(f"  Cloth BMesh creation: {cloth_bm_time:.2f} seconds")

    # Cache the transform matrix (to avoid repeated calculations)
    body_normal_matrix = body_obj.matrix_world.inverted().transposed()
    cloth_normal_matrix = cloth_obj.matrix_world.inverted().transposed()

    # Dictionary storing corrected normals
    adjusted_normals_time_start = time.time()
    adjusted_normals = {}

    # Processing normals for each vertex of the clothing mesh (checking if reversal is necessary)
    for i, vertex in enumerate(cloth_bm.verts):
        # Vertex positions and normals in the world coordinate system
        cloth_vert_world = vertex.co
        original_normal_world = (cloth_normal_matrix @ Vector((vertex.normal[0], vertex.normal[1], vertex.normal[2], 0))).xyz.normalized()

        # Search for the nearest face on the base mesh
        nearest_result = bvh_tree.find_nearest(cloth_vert_world)
        if nearest_result:
            # BVHTree.find_nearest() returns (co, normal, index, distance)
            nearest_point, nearest_normal, nearest_face_index, _ = nearest_result

            # Obtain the nearest neighbor face
            face = body_bm.faces[nearest_face_index]
            face_normal = face.normal

            # Convert the surface normal to world coordinates
            face_normal_world = (body_normal_matrix @ Vector((face_normal[0], face_normal[1], face_normal[2], 0))).xyz.normalized()

            # If the dot product is negative, reverse the normal vector
            dot_product = original_normal_world.dot(face_normal_world)
            if dot_product < 0:
                adjusted_normal = -original_normal_world
            else:
                adjusted_normal = original_normal_world

            # Save adjusted normals to dictionary
            adjusted_normals[i] = adjusted_normal
        else:
            # If the nearest point cannot be found, use the original normal
            adjusted_normals[i] = original_normal_world
    adjusted_normals_time = time.time() - adjusted_normals_time_start
    print(f"  Normal adjustment: {adjusted_normals_time:.2f} seconds")

    # Pre-calculate and cache the center point and area of each face
    face_cache_time_start = time.time()
    face_centers = []
    face_areas = {}
    face_adjusted_normals = {}
    face_indices = []

    for face in cloth_bm.faces:
        # Calculate the center point of the surface
        center = Vector((0, 0, 0))
        for v in face.verts:
            center += v.co
        center /= len(face.verts)
        face_centers.append(center)
        face_indices.append(face.index)

        # Calculate the area
        face_areas[face.index] = face.calc_area()

        # Calculate adjusted surface normals
        face_normal = Vector((0, 0, 0))
        for v in face.verts:
            face_normal += adjusted_normals[v.index]
        face_adjusted_normals[face.index] = face_normal.normalized()
    face_cache_time = time.time() - face_cache_time_start
    print(f"  Face cache creation: {face_cache_time:.2f} seconds")

    # Construct a KDTree for the faces of the clothing mesh
    kdtree_time_start = time.time()
    # size = len(cloth_bm.faces)
    # kd = mathutils.kdtree.KDTree(size)

    # for face_index, center in face_centers.items():
    #     kd.insert(center, face_index)

    # kd.balance()
    kd = cKDTree(face_centers)
    kdtree_time = time.time() - kdtree_time_start
    print(f"  KDTree construction: {kdtree_time:.2f} seconds")

    # Update each vertex's normal with the weighted average of the surrounding faces' normals
    normal_avg_time_start = time.time()
    for i, vertex in enumerate(cloth_bm.verts):
        # Search for surfaces within a specified radius
        co = vertex.co
        weighted_normal = Vector((0, 0, 0))
        total_weight = 0

        # Efficiently search for nearby faces using KDTree
        for index in kd.query_ball_point(co, normal_radius):
            # Calculate weights based on distance (closer distances have greater influence)
            face_index = face_indices[index]
            area = face_areas[face_index]
            dist = (co - face_centers[index]).length
            # Distance-based attenuation coefficient
            distance_factor = 1.0 - (dist / normal_radius) if dist < normal_radius else 0.0
            weight = area * distance_factor

            weighted_normal += face_adjusted_normals[face_index] * weight
            total_weight += weight

        # If the sum of the weights is not zero, normalize
        if total_weight > 0:
            weighted_normal /= total_weight
            weighted_normal.normalize()
            # Update adjusted normals
            adjusted_normals[i] = weighted_normal
    normal_avg_time = time.time() - normal_avg_time_start
    print(f"  Normal-weighted average calculation: {normal_avg_time:.2f} seconds")
    # ----------------------------------

    # Processing for each vertex of the clothing mesh
    weight_calc_time_start = time.time()
    for i, vertex in enumerate(cloth_bm.verts):
        # Vertex position in the world coordinate system
        cloth_vert_world = vertex.co

        # Use adjusted normals
        cloth_normal_world = adjusted_normals[i]

        # Search for the nearest face on the base mesh
        nearest_result = bvh_tree.find_nearest(cloth_vert_world)
        distance = float('inf')  # Set the initial value to infinity

        if nearest_result:
            # BVHTree.find_nearest() returns (co, normal, index, distance)
            nearest_point, nearest_normal, nearest_face_index, _ = nearest_result

            # Obtain the nearest neighbor face
            face = body_bm.faces[nearest_face_index]
            face_normal = face.normal

            # Calculate the nearest point on the surface
            closest_point_on_face = mathutils.geometry.closest_point_on_tri(
                cloth_vert_world,
                face.verts[0].co,
                face.verts[1].co,
                face.verts[2].co
            )

            # Convert the surface normal to world coordinates
            face_normal_world = (body_normal_matrix @ Vector((face_normal[0], face_normal[1], face_normal[2], 0))).xyz.normalized()

            # Calculate distance
            distance = (cloth_vert_world - closest_point_on_face).length

            # Set the nearest point and normal
            nearest_point = closest_point_on_face
            nearest_normal = face_normal_world
        else:
            # If the nearest point cannot be found, set the initial value to None
            nearest_point = None
            nearest_normal = None

        # Initial vertex weight
        weight = 0.0

        if nearest_point:
            # Distance-based weighting (linear interpolation)
            distance_weight = 0.0
            if distance <= distance_min:
                distance_weight = 0.0
            elif distance >= distance_max:
                distance_weight = 1.0
            else:
                # Linear interpolation
                distance_weight = (distance - distance_min) / (distance_max - distance_min)

            # Weight based on normal angle (linear interpolation)
            angle_weight = 0.0
            if nearest_normal:
                # Calculate the angle of the normal
                angle = math.acos(min(1.0, max(-1.0, cloth_normal_world.dot(nearest_normal))))

                # If the angle exceeds 90 degrees, invert the normal and recalculate
                if angle > math.pi / 2:
                    inverted_normal = -nearest_normal
                    angle = math.acos(min(1.0, max(-1.0, cloth_normal_world.dot(inverted_normal))))

                # Linear interpolation of angles
                if angle <= angle_min_rad:
                    angle_weight = 0.0
                elif angle >= angle_max_rad:
                    angle_weight = 1.0
                else:
                    # Linear interpolation
                    angle_weight = (angle - angle_min_rad) / (angle_max_rad - angle_min_rad)

            weight = distance_weight *angle_weight

        # Set weights for vertex groups
        vertex_group.add([i], weight, 'REPLACE')
    weight_calc_time = time.time() - weight_calc_time_start
    print(f"  Weight calculation: {weight_calc_time:.2f} seconds")

    # Set the vertex group as active
    cloth_obj.vertex_groups.active_index = vertex_group.index

    # Switch to Weight Paint mode
    bpy.ops.object.mode_set(mode='WEIGHT_PAINT')

    # Execute smoothing processing (applied to the active vertex group)
    smooth_time_start = time.time()
    bpy.ops.object.vertex_group_smooth(group_select_mode='ACTIVE', factor=0.3, repeat=10, expand=0.0)

    # Cleaning processing is also applied
    bpy.ops.object.vertex_group_clean(group_select_mode='ACTIVE', limit=0.5)
    smooth_time = time.time() - smooth_time_start
    print(f"  Smoothing processing: {smooth_time:.2f} seconds")

    # Return to Object Mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Apply Max filter
    print("  Max filter applied...")
    apply_max_filter_to_vertex_group(cloth_obj, new_group_name, filter_radius=0.02)

    # === Smoothing Processing for Newly Created Vertex Groups ===
    print("  Applying smoothing to the newly created vertex group...")
    neighbors_cache_result = apply_smoothing_to_vertex_group(cloth_obj, new_group_name, smoothing_radius, iteration=1, use_distance_weighting=True, gaussian_falloff=True)

    if smoothing_mask_groups:
        # Get the weight of the newly generated vertex group
        new_group_weights = np.zeros(len(cloth_obj.data.vertices), dtype=np.float32)
        for i, vertex in enumerate(cloth_obj.data.vertices):
            for group in vertex.groups:
                if group.group == vertex_group.index:
                    new_group_weights[i] = group.weight
                    break
        # Calculate the total weight of the specified vertex group
        total_target_weights = np.zeros(len(cloth_obj.data.vertices), dtype=np.float32)

        for target_group_name in smoothing_mask_groups:
            if target_group_name in cloth_obj.vertex_groups:
                target_group = cloth_obj.vertex_groups[target_group_name]
                print(f"    Retrieving weights for vertex group '{target_group_name}'...")

                for i, vertex in enumerate(cloth_obj.data.vertices):
                    for group in vertex.groups:
                        if group.group == target_group.index:
                            total_target_weights[i] += group.weight
                            break
            else:
                print(f"    Warning: Vertex group '{target_group_name}' not found")

        if mask_group_name and mask_group_name in cloth_obj.vertex_groups:
            mask_group = cloth_obj.vertex_groups[mask_group_name]
            for i in range(len(cloth_obj.data.vertices)):
                weight = 0.0
                for group in cloth_obj.data.vertices[i].groups:
                    if group.group == mask_group.index:
                        weight = group.weight
                        break
                total_target_weights[i] *= weight

        # Subtract the sum from the weights of the new vertex group
        masked_weights = np.maximum(0.0, new_group_weights * total_target_weights)

        # Apply the result to the new vertex group
        for i in range(len(cloth_obj.data.vertices)):
            vertex_group.add([i], masked_weights[i], 'REPLACE')

    # === Additional Processing: Weight Processing for Specified Vertex Groups ===
    if target_vertex_groups:
        print("  Processing specified vertex group begins...")

        # Get the weight of the generated vertex group
        mask_weights = np.zeros(len(cloth_obj.data.vertices), dtype=np.float32)
        for i, vertex in enumerate(cloth_obj.data.vertices):
            for group in vertex.groups:
                if group.group == vertex_group.index:
                    mask_weights[i] = group.weight
                    break

        # Process the specified vertex group
        for target_group_name in target_vertex_groups:
            if target_group_name not in cloth_obj.vertex_groups:
                print(f"  Warning: Vertex group '{target_group_name}' not found")
                continue

            target_group = cloth_obj.vertex_groups[target_group_name]
            print(f"  Vertex group being processed: {target_group_name}")

            # 1. Retrieve the original weight
            original_weights = np.zeros(len(cloth_obj.data.vertices), dtype=np.float32)
            for i, vertex in enumerate(cloth_obj.data.vertices):
                for group in vertex.groups:
                    if group.group == target_group.index:
                        original_weights[i] = group.weight
                        break

            # 2. Smoothing process (only if original_weights are not all zero)
            if np.any(original_weights > 0):
                print("    Smoothing processing in progress...")
                neighbors_cache_result = apply_smoothing_to_vertex_group(cloth_obj, target_group_name, smoothing_radius, iteration=3, use_distance_weighting=True, gaussian_falloff=True, neighbors_cache=neighbors_cache_result)

                # 3. Obtain the weight after smoothing
                smoothed_weights = np.zeros(len(cloth_obj.data.vertices), dtype=np.float32)
                for i, vertex in enumerate(cloth_obj.data.vertices):
                    for group in vertex.groups:
                        if group.group == target_group.index:
                            smoothed_weights[i] = group.weight
                            break

                # 4. Synthesis Processing
                print("    Processing...")
                for i in range(len(cloth_obj.data.vertices)):
                    # Use the weights of the generated vertex groups as synthesis weights
                    blend_factor = mask_weights[i]

                    # Combine the original weights with the smoothing results
                    final_weight = original_weights[i] * (1.0 - blend_factor) + smoothed_weights[i] * blend_factor

                    # Set the final weight
                    target_group.add([i], final_weight, 'REPLACE')
            else:
                print("    Skip: Skipping processing because all original_weights are 0")

            print(f"    Processing completed for vertex group '{target_group_name}'")

    # Restore to original mode
    bpy.ops.object.mode_set(mode=current_mode)

    # Clean up BMesh
    body_bm.free()
    cloth_bm.free()

    # Clean up the cache (to reduce memory usage)
    clear_mesh_cache()

    total_time = time.time() - start_time
    print(f"{new_group_name} vertex group created (Total time: {total_time:.2f} seconds)")
    return vertex_group

def process_weight_transfer(target_obj, armature, base_avatar_data, clothing_avatar_data, field_path, clothing_armature, cloth_metadata=None):
    """Process weight transfer for the target object."""
    start_time = time.time()

    # Create a conversion map from Humanoid bone names to bone names
    humanoid_to_bone = {}
    for bone_map in base_avatar_data.get("humanoidBones", []):
        if "humanoidBoneName" in bone_map and "boneName" in bone_map:
            humanoid_to_bone[bone_map["humanoidBoneName"]] = bone_map["boneName"]

    # Create auxiliary bone mapping
    auxiliary_bones = {}
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        humanoid_bone = aux_set["humanoidBoneName"]
        auxiliary_bones[humanoid_bone] = aux_set["auxiliaryBones"]

    auxiliary_bones_to_humanoid = {}
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        for aux_bone in aux_set["auxiliaryBones"]:
            auxiliary_bones_to_humanoid[aux_bone] = aux_set["humanoidBoneName"]

    finger_humanoid_bones = [
        "LeftIndexProximal", "LeftIndexIntermediate", "LeftIndexDistal",
        "LeftMiddleProximal", "LeftMiddleIntermediate", "LeftMiddleDistal",
        "LeftRingProximal", "LeftRingIntermediate", "LeftRingDistal",
        "LeftLittleProximal", "LeftLittleIntermediate", "LeftLittleDistal",
        "RightIndexProximal", "RightIndexIntermediate", "RightIndexDistal",
        "RightMiddleProximal", "RightMiddleIntermediate", "RightMiddleDistal",
        "RightRingProximal", "RightRingIntermediate", "RightRingDistal",
        "RightLittleProximal", "RightLittleIntermediate", "RightLittleDistal",
        "LeftHand", "RightHand"
    ]

    left_foot_finger_humanoid_bones = [
        "LeftFootThumbProximal",
        "LeftFootThumbIntermediate",
        "LeftFootThumbDistal",
        "LeftFootIndexProximal",
        "LeftFootIndexIntermediate",
        "LeftFootIndexDistal",
        "LeftFootMiddleProximal",
        "LeftFootMiddleIntermediate",
        "LeftFootMiddleDistal",
        "LeftFootRingProximal",
        "LeftFootRingIntermediate",
        "LeftFootRingDistal",
        "LeftFootLittleProximal",
        "LeftFootLittleIntermediate",
        "LeftFootLittleDistal",
    ]
    right_foot_finger_humanoid_bones = [
        "RightFootThumbProximal",
        "RightFootThumbIntermediate",
        "RightFootThumbDistal",
        "RightFootIndexProximal",
        "RightFootIndexIntermediate",
        "RightFootIndexDistal",
        "RightFootMiddleProximal",
        "RightFootMiddleIntermediate",
        "RightFootMiddleDistal",
        "RightFootRingProximal",
        "RightFootRingIntermediate",
        "RightFootRingDistal",
        "RightFootLittleProximal",
        "RightFootLittleIntermediate",
        "RightFootLittleDistal"
    ]

    # Get the actual bone name of the finger bone
    finger_bone_names = set()
    for humanoid_bone in finger_humanoid_bones:
        if humanoid_bone in humanoid_to_bone:
            bone_name = humanoid_to_bone[humanoid_bone]
            finger_bone_names.add(bone_name)

            # Related auxiliary bones have also been added
            if humanoid_bone in auxiliary_bones:
                for aux_bone in auxiliary_bones[humanoid_bone]:
                    finger_bone_names.add(aux_bone)

    print(f"finger_bone_names: {finger_bone_names}")

    # Identify vertices with finger bone weight
    finger_vertices = set()
    if finger_bone_names:
        mesh = target_obj.data

        # Check the vertex group corresponding to each finger's bone name
        for bone_name in finger_bone_names:
            if bone_name in target_obj.vertex_groups:
                for vert in mesh.vertices:
                    weight = 0.0
                    for g in vert.groups:
                        if target_obj.vertex_groups[g.group].name == bone_name:
                            weight = g.weight
                            break
                    if weight > 0.001:  # Vertices with weights above the threshold
                        finger_vertices.add(vert.index)

        print(f"finger_vertices: {len(finger_vertices)}")

    closing_filter_mask_weights = create_blendshape_mask(target_obj, ["LeftUpperLeg", "RightUpperLeg", "Hips", "Chest", "Spine", "LeftShoulder", "RightShoulder", "LeftBreast", "RightBreast"], base_avatar_data)

    def attempt_weight_transfer(source_obj, vertex_group, max_distance_try=0.2, max_distance_tried=0.0):
        """Attempting weight transfer"""
        bone_groups_tmp = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)
        prev_weights = store_weights(target_obj, bone_groups_tmp)
        initial_max_distance = max_distance_try

        while max_distance_try <= 1.0:
            if max_distance_tried + 0.0001 < max_distance_try:
                create_distance_normal_based_vertex_group(bpy.data.objects["Body.BaseAvatar"], target_obj, max_distance_try, 0.005, 20.0, "InpaintMask", normal_radius=0.003, filter_mask=closing_filter_mask_weights)

                #Copy bpy.data.objects["Body.BaseAvatar"] for debugging purposes
                # body_base_avatar_copy = bpy.data.objects["Body.BaseAvatar"].copy()
                # body_base_avatar_copy.data = bpy.data.objects["Body.BaseAvatar"].data.copy()
                # body_base_avatar_copy.name = "Body.BaseAvatar.Copy"
                # bpy.context.scene.collection.objects.link(body_base_avatar_copy)

                # target_obj_copy = target_obj.copy()
                # target_obj_copy.data = target_obj.data.copy()
                # target_obj_copy.name = target_obj.name + ".Copy"
                # bpy.context.scene.collection.objects.link(target_obj_copy)

                # current_mode = bpy.context.object.mode
                # bpy.ops.object.mode_set(mode='OBJECT')
                # current_active = bpy.context.active_object
                # bpy.context.view_layer.objects.active = body_base_avatar_copy
                # selection = bpy.context.selected_objects
                # bpy.ops.object.select_all(action='DESELECT')

                # body_base_avatar_copy.select_set(True)
                # target_obj_copy.select_set(True)
                # bpy.ops.object.convert(target='MESH')

                # bpy.ops.object.select_all(action='DESELECT')
                # for obj in selection:
                #     obj.select_set(True)
                # bpy.context.view_layer.objects.active = current_active
                # bpy.ops.object.mode_set(mode=current_mode)

                # When a vertex has finger bone weight, create a more precise InpaintMask
                # if finger_vertices and len(finger_vertices) > 0:
                #     # Create a precise mask with normal_radius=0.001 (using a temporary name)
                #     temp_mask_name = "TempFingerInpaintMask"
                #     create_distance_normal_based_vertex_group(bpy.data.objects["Body.BaseAvatar"], target_obj, max_distance_try, 0.003, 30.0, temp_mask_name, normal_radius=0.001)

                #     # Overwrite the original InpaintMask only at the fingertip with precise mask values
                #     if temp_mask_name in target_obj.vertex_groups and "InpaintMask" in target_obj.vertex_groups:
                #         temp_group = target_obj.vertex_groups[temp_mask_name]
                #         inpaint_group = target_obj.vertex_groups["InpaintMask"]

                #         for vert_idx in finger_vertices:
                #             vert = target_obj.data.vertices[vert_idx]
                #             weight = 0.0
                #             for g in vert.groups:
                #                 if target_obj.vertex_groups[g.group].name == temp_mask_name:
                #                     weight = g.weight
                #                     break
                #             inpaint_group.add([vert_idx], weight, 'REPLACE')

                #         # Delete temporary group
                #         # target_obj.vertex_groups.remove(temp_group)

                if finger_vertices and len(finger_vertices) > 0:
                    # Set the value of InpaintMask to 0 at the tip of the finger
                    for vert_idx in finger_vertices:
                        target_obj.vertex_groups["InpaintMask"].add([vert_idx], 0.0, 'REPLACE')

                #Multiply the weight of MF_Inpaint by the weight of InpaintMask
                if "MF_Inpaint" in target_obj.vertex_groups and "InpaintMask" in target_obj.vertex_groups:
                    inpaint_group = target_obj.vertex_groups["InpaintMask"]
                    source_group = target_obj.vertex_groups["MF_Inpaint"]

                    for vert in target_obj.data.vertices:
                        source_weight = 0.0
                        for g in vert.groups:
                            if g.group == source_group.index:
                                source_weight = g.weight
                                break
                        inpaint_weight = 0.0
                        for g in vert.groups:
                            if g.group == inpaint_group.index:
                                inpaint_weight = g.weight
                                break
                        inpaint_group.add([vert.index], source_weight * inpaint_weight, 'REPLACE')

                # Set the InpaintMask weight to 0 for vertices where the vertex_group weight is 0
                if "InpaintMask" in target_obj.vertex_groups and vertex_group in target_obj.vertex_groups:
                    inpaint_group = target_obj.vertex_groups["InpaintMask"]
                    source_group = target_obj.vertex_groups[vertex_group]

                    for vert in target_obj.data.vertices:
                        source_weight = 0.0
                        # Get the weight of vertex_group
                        for g in vert.groups:
                            if g.group == source_group.index:
                                source_weight = g.weight
                                break

                        # When the weight is 0, the InpaintMask is also set to 0
                        if source_weight == 0.0:
                            inpaint_group.add([vert.index], 0.0, 'REPLACE')

            try:
                bpy.context.scene.robust_weight_transfer_settings.source_object = source_obj
                bpy.context.object.robust_weight_transfer_settings.vertex_group = vertex_group
                bpy.context.scene.robust_weight_transfer_settings.inpaint_mode = 'POINT'
                bpy.context.scene.robust_weight_transfer_settings.max_distance = max_distance_try
                bpy.context.scene.robust_weight_transfer_settings.use_deformed_target = True
                bpy.context.scene.robust_weight_transfer_settings.use_deformed_source = True
                bpy.context.scene.robust_weight_transfer_settings.enforce_four_bone_limit = True
                bpy.context.scene.robust_weight_transfer_settings.max_normal_angle_difference = 1.5708
                #bpy.context.scene.robust_weight_transfer_settings.max_normal_angle_difference = 0.349066
                bpy.context.scene.robust_weight_transfer_settings.flip_vertex_normal = True
                bpy.context.scene.robust_weight_transfer_settings.smoothing_enable = False
                bpy.context.scene.robust_weight_transfer_settings.smoothing_repeat = 4
                bpy.context.scene.robust_weight_transfer_settings.smoothing_factor = 0.5
                bpy.context.object.robust_weight_transfer_settings.inpaint_group = "InpaintMask"
                bpy.context.object.robust_weight_transfer_settings.inpaint_threshold = 0.5
                bpy.context.object.robust_weight_transfer_settings.inpaint_group_invert = False
                bpy.context.object.robust_weight_transfer_settings.vertex_group_invert = False
                bpy.context.scene.robust_weight_transfer_settings.group_selection = 'DEFORM_POSE_BONES'
                bpy.ops.object.skin_weight_transfer()
                print(f"Weight transfered with max_distance {max_distance_try}")
                return True, max_distance_try
            except RuntimeError as e:
                print(f"Weight transfer failed with max_distance {max_distance_try}: {str(e)}")
                restore_weights(target_obj, prev_weights)
                max_distance_try += 0.05
                if max_distance_try > 1.0:
                    print("Max distance exceeded 1.0, stopping weight transfer attempts")
                    return False, initial_max_distance
        return False, initial_max_distance

    def get_vertex_weight_safe(group, vertex_index):
        """Safely retrieve weights from vertex groups"""
        if not group:
            return 0.0
        try:
            for g in target_obj.data.vertices[vertex_index].groups:
                if g.group == group.index:
                    return g.weight
        except Exception:
            pass
        return 0.0

    def propagate_weights_to_side_vertices(target_obj, bone_groups, original_humanoid_weights, clothing_armature, max_iterations=100):
        """
        Propagate weights to vertices that have side weights but no bone weights
        """
        # Create a BMesh
        bm = bmesh.new()
        bm.from_mesh(target_obj.data)
        bm.verts.ensure_lookup_table()

        # Retrieve the index of the side weight group
        left_group = target_obj.vertex_groups.get("LeftSideWeights")
        right_group = target_obj.vertex_groups.get("RightSideWeights")

        # Create a target group that includes the clothing armature's bone group
        all_deform_groups = set(bone_groups)
        if clothing_armature:
            all_deform_groups.update(bone.name for bone in clothing_armature.data.bones)

        def get_side_weight(vert_idx, group):
            """Get vertex side weights"""
            if not group:
                return 0.0
            try:
                for g in target_obj.data.vertices[vert_idx].groups:
                    if g.group == group.index:
                        return g.weight
            except Exception:
                pass
            return 0.0

        def has_bone_weights(vert_idx):
            """Check if vertices have bone weights (including clothing bone groups)"""
            for g in target_obj.data.vertices[vert_idx].groups:
                if target_obj.vertex_groups[g.group].name in all_deform_groups:
                    return True
            return False

        # Identify the target vertex
        vertices_to_process = set()
        for vert in target_obj.data.vertices:
            # Identify vertices with side weights but no bone weights
            if (get_side_weight(vert.index, left_group) > 0 or
                get_side_weight(vert.index, right_group) > 0) and not has_bone_weights(vert.index):
                vertices_to_process.add(vert.index)

        if not vertices_to_process:
            bm.free()
            return

        print(f"Found {len(vertices_to_process)} vertices without bone weights but with side weights")

        # Iterative Weight Propagation
        iteration = 0
        while vertices_to_process and iteration < max_iterations:
            propagated_this_iteration = set()

            for vert_idx in vertices_to_process:
                vert = bm.verts[vert_idx]
                # Get adjacent vertices
                neighbors_with_weights = []

                for edge in vert.link_edges:
                    other = edge.other_vert(vert)
                    if has_bone_weights(other.index):
                        # Calculate the distance between vertices
                        distance = (vert.co - other.co).length
                        neighbors_with_weights.append((other.index, distance))

                if neighbors_with_weights:
                    # Select the nearest vertex
                    closest_vert_idx = min(neighbors_with_weights, key=lambda x: x[1])[0]

                    # Copy the weight
                    for group in target_obj.vertex_groups:
                        if group.name in all_deform_groups:
                            weight = 0.0
                            for g in target_obj.data.vertices[closest_vert_idx].groups:
                                if g.group == group.index:
                                    weight = g.weight
                                    break
                            if weight > 0:
                                group.add([vert_idx], weight, 'REPLACE')

                    propagated_this_iteration.add(vert_idx)

            if not propagated_this_iteration:
                break

            print(f"Iteration {iteration + 1}: Propagated weights to {len(propagated_this_iteration)} vertices")
            vertices_to_process -= propagated_this_iteration
            iteration += 1

        # Assign the original weights to the remaining vertices
        if vertices_to_process:
            print(f"Restoring original weights for {len(vertices_to_process)} remaining vertices")
            for vert_idx in vertices_to_process:
                if vert_idx in original_humanoid_weights:
                    # Delete current weight
                    for group in target_obj.vertex_groups:
                        if group.name in all_deform_groups:
                            try:
                                group.remove([vert_idx])
                            except RuntimeError:
                                continue

                    # Restore original weight
                    for group_name, weight in original_humanoid_weights[vert_idx].items():
                        if group_name in target_obj.vertex_groups:
                            target_obj.vertex_groups[group_name].add([vert_idx], weight, 'REPLACE')

        bm.free()

    print(f"Processing Started: {target_obj.name}")
    if "InpaintMask" not in target_obj.vertex_groups:
        target_obj.vertex_groups.new(name="InpaintMask")

    # Create Side Weight Group
    side_weight_time_start = time.time()
    create_side_weight_groups(target_obj, base_avatar_data, clothing_armature, clothing_avatar_data)
    side_weight_time = time.time() - side_weight_time_start
    print(f"  Side weight group creation: {side_weight_time:.2f} seconds")

    bpy.ops.object.mode_set(mode='OBJECT')
    bpy.ops.object.select_all(action='DESELECT')
    bpy.context.view_layer.objects.active = target_obj

    # Save the vertex group name before transfer
    original_groups = set(vg.name for vg in target_obj.vertex_groups)

    # Get the target bong group
    bone_groups = get_humanoid_and_auxiliary_bone_groups(base_avatar_data)

    # Save original Humanoid weights
    store_weights_time_start = time.time()
    original_humanoid_weights = store_weights(target_obj, bone_groups)
    store_weights_time = time.time() - store_weights_time_start
    print(f"  Original weight saved: {store_weights_time:.2f} seconds")

    # Create a target group that includes the clothing armature's bone group
    all_deform_groups = set(bone_groups)
    if clothing_armature:
        all_deform_groups.update(bone.name for bone in clothing_armature.data.bones)

    # Save the weights for the groups excluding bone_groups from original_groups
    original_non_humanoid_groups = all_deform_groups - bone_groups
    original_non_humanoid_weights = store_weights(target_obj, original_non_humanoid_groups)

    # Save the weights for all groups
    all_weights = store_weights(target_obj, all_deform_groups)

    # Weight Initialization
    reset_weights_time_start = time.time()
    reset_bone_weights(target_obj, all_deform_groups)
    reset_weights_time = time.time() - reset_weights_time_start
    print(f"  Weight initialization: {reset_weights_time:.2f} seconds")

    # Weight transfer on the left side
    left_transfer_time_start = time.time()
    left_transfer_success, left_distance_used = attempt_weight_transfer(bpy.data.objects["Body.BaseAvatar.LeftOnly"], "LeftSideWeights")
    left_transfer_time = time.time() - left_transfer_time_start
    print(f"  Left-side weight transfer: {left_transfer_time:.2f} seconds (Success: {left_transfer_success}, Distance: {left_distance_used})")

    failed = False

    if not left_transfer_success:
        print("  Processing interrupted due to left-side weight transfer failure")
        failed = True


    if not failed:
        # Weight transfer on the right side
        right_transfer_time_start = time.time()
        right_transfer_success, right_distance_used = attempt_weight_transfer(bpy.data.objects["Body.BaseAvatar.RightOnly"], "RightSideWeights", max_distance_tried=left_distance_used)
        right_transfer_time = time.time() - right_transfer_time_start
        print(f"  Right-side weight transfer: {right_transfer_time:.2f} seconds (Success: {right_transfer_success}, Distance: {right_distance_used})")

        if not right_transfer_success:
            print("  Process interrupted due to failure to transfer right-side weight")
            failed = True

    if failed:
        reset_bone_weights(target_obj, bone_groups)
        restore_weights(target_obj, all_weights)
        return

    # Check if the MF_Armpit group exists and if there are any vertices with a weight greater than 0.001
    mf_armpit_group = target_obj.vertex_groups.get("MF_Armpit")
    should_armpit_process = False
    if mf_armpit_group:
        for vert in target_obj.data.vertices:
            for g in vert.groups:
                if g.group == mf_armpit_group.index and g.weight > 0.001:
                    should_armpit_process = True
                    break
            if should_armpit_process:
                break

    if should_armpit_process:
        if armature and armature.type == 'ARMATURE':
            print("  The MF_Armpit group exists and has valid weights, so the processing is executed")
            base_humanoid_weights = store_weights(target_obj, bone_groups)
            reset_bone_weights(target_obj, bone_groups)
            restore_weights(target_obj, all_weights)

            # Apply Y-axis rotation to the LeftUpperArm and RightUpperArm bones
            print("  Apply Y-axis rotation to the LeftUpperArm and RightUpperArm bones")
            bpy.context.view_layer.objects.active = armature
            bpy.ops.object.mode_set(mode='POSE')

            # Get the boneName for LeftUpperArm and RightUpperArm from humanoidBones
            left_upper_arm_bone = None
            right_upper_arm_bone = None

            for bone_map in base_avatar_data.get("humanoidBones", []):
                if bone_map.get("humanoidBoneName") == "LeftUpperArm":
                    left_upper_arm_bone = bone_map.get("boneName")
                elif bone_map.get("humanoidBoneName") == "RightUpperArm":
                    right_upper_arm_bone = bone_map.get("boneName")

            # Apply a -45 degree Y-axis rotation to the LeftUpperLeg bone
            if left_upper_arm_bone and left_upper_arm_bone in armature.pose.bones:
                bone = armature.pose.bones[left_upper_arm_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(-45), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            # Apply a 45-degree rotation around the Y-axis to the RightUpperLeg bone
            if right_upper_arm_bone and right_upper_arm_bone in armature.pose.bones:
                bone = armature.pose.bones[right_upper_arm_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(45), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            bpy.ops.object.mode_set(mode='OBJECT')
            bpy.context.view_layer.objects.active = target_obj
            bpy.context.view_layer.update()

            shape_key_state = save_shape_key_state(target_obj)
            for key_block in target_obj.data.shape_keys.key_blocks:
                key_block.value = 0.0

            # Create a temporary shape key
            temp_shape_name = "WT_shape_forA.MFTemp"
            if target_obj.data.shape_keys and temp_shape_name in target_obj.data.shape_keys.key_blocks:
                temp_shape_key = target_obj.data.shape_keys.key_blocks[temp_shape_name]
            temp_shape_key.value = 1.0

            # Weight Initialization
            reset_bone_weights(target_obj, bone_groups)

            # Weight Transfer
            print("  Weight Transfer Start")
            transfer_success, distance_used = attempt_weight_transfer(bpy.data.objects["Body.BaseAvatar"], "BothSideWeights")

            restore_shape_key_state(target_obj, shape_key_state)
            temp_shape_key.value = 0.0

            # Apply Y-axis reverse rotation to the LeftUpperArm and RightUpperArm bones
            print("  Apply Y-axis reverse rotation to the LeftUpperArm and RightUpperArm bones")
            bpy.context.view_layer.objects.active = armature
            bpy.ops.object.mode_set(mode='POSE')

            # Get the boneName for LeftUpperArm and RightUpperArm from humanoidBones
            left_upper_arm_bone = None
            right_upper_arm_bone = None

            for bone_map in base_avatar_data.get("humanoidBones", []):
                if bone_map.get("humanoidBoneName") == "LeftUpperArm":
                    left_upper_arm_bone = bone_map.get("boneName")
                elif bone_map.get("humanoidBoneName") == "RightUpperArm":
                    right_upper_arm_bone = bone_map.get("boneName")

            # Apply a -45 degree Y-axis rotation to the LeftUpperLeg bone
            if left_upper_arm_bone and left_upper_arm_bone in armature.pose.bones:
                bone = armature.pose.bones[left_upper_arm_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(45), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            # Apply a 45-degree rotation around the Y-axis to the RightUpperLeg bone
            if right_upper_arm_bone and right_upper_arm_bone in armature.pose.bones:
                bone = armature.pose.bones[right_upper_arm_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(-45), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            bpy.ops.object.mode_set(mode='OBJECT')
            bpy.context.view_layer.objects.active = target_obj
            bpy.context.view_layer.update()

            # Combine the weights of bone_groups with base_humanoid_weights
            mf_armpit_group = target_obj.vertex_groups.get("MF_Armpit")
            if mf_armpit_group and base_humanoid_weights:
                print("  Weight synthesis processing start")

                for vert in target_obj.data.vertices:
                    vert_idx = vert.index

                    # Get the weight of the MF_Armpit group
                    mf_armpit_weight = 0.0
                    for g in vert.groups:
                        if g.group == mf_armpit_group.index:
                            mf_armpit_weight = g.weight
                            break

                    # Calculate the synthesis coefficient
                    current_factor = mf_armpit_weight
                    base_factor = 1.0 - mf_armpit_weight

                    # Combine the weights of groups belonging to bone_groups
                    for group_name in bone_groups:
                        if group_name in target_obj.vertex_groups:
                            group = target_obj.vertex_groups[group_name]

                            # Get the current weight
                            current_weight = 0.0
                            for g in vert.groups:
                                if g.group == group.index:
                                    current_weight = g.weight
                                    break

                            # Retrieve weights from base_humanoid_weights
                            base_weight = 0.0
                            if vert_idx in base_humanoid_weights and group_name in base_humanoid_weights[vert_idx]:
                                base_weight = base_humanoid_weights[vert_idx][group_name]

                            # Weight synthesis: (Current weight) * (MF_crotch weight) + (Weight in base_humanoid_weights) * (1.0 - MF_crotch weight)
                            blended_weight = current_weight * current_factor + base_weight * base_factor

                            # Apply the synthesized weight
                            if blended_weight > 0.0001:  # Negligible values are ignored
                                group.add([vert_idx], blended_weight, 'REPLACE')
                                base_humanoid_weights[vert_idx][group_name] = blended_weight
                            else:
                                try:
                                    group.remove([vert_idx])
                                    base_humanoid_weights[vert_idx][group_name] = 0.0
                                except RuntimeError:
                                    pass
            print("  Weight synthesis processing complete")
        else:
            print("  Process skipped because the MF_Armpit group does not exist or the armature is missing")
    else:
        print("  Process skipped because the MF_Armpit group does not exist or has no valid weights")



    # Check if the MF_crotch group exists and if there are any vertices with a weight greater than 0.001
    mf_crotch_group = target_obj.vertex_groups.get("MF_crotch")
    should_process = False
    if mf_crotch_group:
        for vert in target_obj.data.vertices:
            for g in vert.groups:
                if g.group == mf_crotch_group.index and g.weight > 0.001:
                    should_process = True
                    break
            if should_process:
                break

    if should_process:
        if armature and armature.type == 'ARMATURE':
            print("  The MF_crotch group exists and has valid weights, so the processing is executed")
            base_humanoid_weights = store_weights(target_obj, bone_groups)
            reset_bone_weights(target_obj, bone_groups)
            restore_weights(target_obj, all_weights)

            # Apply Y-axis rotation to the LeftUpperLeg and RightUpperLeg bones
            print("  Apply Y-axis rotation to the LeftUpperLeg and RightUpperLeg bones")
            bpy.context.view_layer.objects.active = armature
            bpy.ops.object.mode_set(mode='POSE')

            # Get the boneName for LeftUpperLeg and RightUpperLeg from humanoidBones
            left_upper_leg_bone = None
            right_upper_leg_bone = None

            for bone_map in base_avatar_data.get("humanoidBones", []):
                if bone_map.get("humanoidBoneName") == "LeftUpperLeg":
                    left_upper_leg_bone = bone_map.get("boneName")
                elif bone_map.get("humanoidBoneName") == "RightUpperLeg":
                    right_upper_leg_bone = bone_map.get("boneName")

            # Apply a -45 degree Y-axis rotation to the LeftUpperLeg bone
            if left_upper_leg_bone and left_upper_leg_bone in armature.pose.bones:
                bone = armature.pose.bones[left_upper_leg_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(-70), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            # Apply a 45-degree rotation around the Y-axis to the RightUpperLeg bone
            if right_upper_leg_bone and right_upper_leg_bone in armature.pose.bones:
                bone = armature.pose.bones[right_upper_leg_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(70), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            bpy.ops.object.mode_set(mode='OBJECT')
            bpy.context.view_layer.objects.active = target_obj
            bpy.context.view_layer.update()

            shape_key_state = save_shape_key_state(target_obj)
            for key_block in target_obj.data.shape_keys.key_blocks:
                key_block.value = 0.0

            # Create a temporary shape key
            temp_shape_name = "WT_shape_forCrotch.MFTemp"
            if target_obj.data.shape_keys and temp_shape_name in target_obj.data.shape_keys.key_blocks:
                temp_shape_key = target_obj.data.shape_keys.key_blocks[temp_shape_name]
            temp_shape_key.value = 1.0

            # Weight Initialization
            reset_bone_weights(target_obj, bone_groups)

            # Weight Transfer
            print("  Weight Transfer Start")
            transfer_success, distance_used = attempt_weight_transfer(bpy.data.objects["Body.BaseAvatar"], "BothSideWeights")

            restore_shape_key_state(target_obj, shape_key_state)
            temp_shape_key.value = 0.0

            # Apply Y-axis reverse rotation to the LeftUpperLeg and RightUpperLeg bones
            print("  Apply Y-axis reverse rotation to the LeftUpperLeg and RightUpperLeg bones")
            bpy.context.view_layer.objects.active = armature
            bpy.ops.object.mode_set(mode='POSE')

            # Get the boneName for LeftUpperLeg and RightUpperLeg from humanoidBones
            left_upper_leg_bone = None
            right_upper_leg_bone = None

            for bone_map in base_avatar_data.get("humanoidBones", []):
                if bone_map.get("humanoidBoneName") == "LeftUpperLeg":
                    left_upper_leg_bone = bone_map.get("boneName")
                elif bone_map.get("humanoidBoneName") == "RightUpperLeg":
                    right_upper_leg_bone = bone_map.get("boneName")

            # Apply a -45 degree Y-axis rotation to the LeftUpperLeg bone
            if left_upper_leg_bone and left_upper_leg_bone in armature.pose.bones:
                bone = armature.pose.bones[left_upper_leg_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(70), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            # Apply a 45-degree rotation around the Y-axis to the RightUpperLeg bone
            if right_upper_leg_bone and right_upper_leg_bone in armature.pose.bones:
                bone = armature.pose.bones[right_upper_leg_bone]
                current_world_matrix = armature.matrix_world @ bone.matrix
                # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                head_world_transformed = armature.matrix_world @ bone.head
                offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                rotation_matrix = mathutils.Matrix.Rotation(math.radians(-70), 4, 'Y')
                bone.matrix = armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

            bpy.ops.object.mode_set(mode='OBJECT')
            bpy.context.view_layer.objects.active = target_obj
            bpy.context.view_layer.update()

            # Combine the weights of bone_groups with base_humanoid_weights
            mf_crotch_group = target_obj.vertex_groups.get("MF_crotch")
            if mf_crotch_group and base_humanoid_weights:
                print("  Weight synthesis processing start")

                for vert in target_obj.data.vertices:
                    vert_idx = vert.index

                    # Get the weight of the MF_crotch group
                    mf_crotch_weight = 0.0
                    for g in vert.groups:
                        if g.group == mf_crotch_group.index:
                            mf_crotch_weight = g.weight
                            break

                    # Calculate the synthesis coefficient
                    current_factor = mf_crotch_weight
                    base_factor = 1.0 - mf_crotch_weight

                    # Combine the weights of groups belonging to bone_groups
                    for group_name in bone_groups:
                        if group_name in target_obj.vertex_groups:
                            group = target_obj.vertex_groups[group_name]

                            # Get the current weight
                            current_weight = 0.0
                            for g in vert.groups:
                                if g.group == group.index:
                                    current_weight = g.weight
                                    break

                            # Retrieve weights from base_humanoid_weights
                            base_weight = 0.0
                            if vert_idx in base_humanoid_weights and group_name in base_humanoid_weights[vert_idx]:
                                base_weight = base_humanoid_weights[vert_idx][group_name]

                            # Weight synthesis: (Current weight) * (MF_crotch weight) + (Weight in base_humanoid_weights) * (1.0 - MF_crotch weight)
                            blended_weight = current_weight * current_factor + base_weight * base_factor

                            # Apply the synthesized weight
                            if blended_weight > 0.0001:  # Negligible values are ignored
                                group.add([vert_idx], blended_weight, 'REPLACE')
                            else:
                                try:
                                    group.remove([vert_idx])
                                except RuntimeError:
                                    pass
            print("  Weight synthesis processing complete")
        else:
            print("  Process skipped because the MF_crotch group does not exist or the armature does not exist")
    else:
        print("  Process skipped because the MF_crotch group does not exist or has no valid weights")

    bpy.ops.object.mode_set(mode='EDIT')
    bpy.ops.mesh.select_mode(type="VERT")
    bpy.ops.mesh.select_all(action='DESELECT')
    # Select vertices with weights of 0.5 or higher in the InpaintMask group
    inpaint_mask_group = target_obj.vertex_groups.get("InpaintMask")
    if inpaint_mask_group:
        for vert in target_obj.data.vertices:
            for g in vert.groups:
                if g.group == inpaint_mask_group.index and g.weight >= 0.5:
                    vert.select = True
                    break

    # Perform smoothing on all vertex groups contained within bone_groups
    bpy.ops.object.mode_set(mode='WEIGHT_PAINT')
    bpy.context.object.data.use_paint_mask = False
    bpy.context.object.data.use_paint_mask_vertex = True
    for group_name in bone_groups:
        if group_name in target_obj.vertex_groups:
            target_obj.vertex_groups.active = target_obj.vertex_groups[group_name]
            bpy.ops.object.vertex_group_smooth(factor=0.5, repeat=3, expand=0.0)

    bpy.ops.object.mode_set(mode='OBJECT')

    # Exclude minor weights
    cleanup_weights_time_start = time.time()
    for vert in target_obj.data.vertices:
        groups_to_remove = []
        for g in vert.groups:
            group_name = target_obj.vertex_groups[g.group].name
            if group_name in bone_groups and g.weight < 0.001:
                groups_to_remove.append(g.group)

        # Remove the vertex from the group with the smallest weight
        for group_idx in groups_to_remove:
            try:
                target_obj.vertex_groups[group_idx].remove([vert.index])
            except RuntimeError:
                continue
    cleanup_weights_time = time.time() - cleanup_weights_time_start
    print(f"  Minor weight exclusion: {cleanup_weights_time:.2f} seconds")

    # Create mappings
    humanoid_to_bone = {bone_map["humanoidBoneName"]: bone_map["boneName"]
                        for bone_map in base_avatar_data["humanoidBones"]}
    bone_to_humanoid = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                        for bone_map in base_avatar_data["humanoidBones"]}

    # Identify the new vertex group after transfer
    new_groups = set(vg.name for vg in target_obj.vertex_groups)
    added_groups = new_groups - original_groups

    print(f"  Bone Group: {bone_groups}")
    print(f"  Original groups: {original_groups}")
    print(f"  New Groups: {new_groups}")
    print(f"  Added groups: {added_groups}")



    # Save the weights for all groups at this point
    num_vertices = len(target_obj.data.vertices)
    all_transferred_weights = store_weights(target_obj, all_deform_groups)

    clothing_bone_to_humanoid = {bone_map["boneName"]: bone_map["humanoidBoneName"]
                           for bone_map in clothing_avatar_data["humanoidBones"]}
    clothing_bone_to_parent_humanoid = {}
    for clothing_bone in clothing_armature.data.bones:
        current_bone = clothing_bone
        current_bone_name = current_bone.name
        parent_humanoid_name = None
        while current_bone:
            if current_bone.name in clothing_bone_to_humanoid.keys():
                parent_humanoid_name = clothing_bone_to_humanoid[current_bone.name]
                break
            current_bone = current_bone.parent
        print(f"current_bone_name: {current_bone_name}, parent_humanoid_name: {parent_humanoid_name}")
        if parent_humanoid_name:
            clothing_bone_to_parent_humanoid[current_bone_name] = parent_humanoid_name

    non_humanoid_parts_mask = np.zeros(num_vertices)
    non_humanoid_total_weights = np.zeros(num_vertices)
    for vert_idx, groups in original_non_humanoid_weights.items():
        total_weight = 0.0
        for group_name, weight in groups.items():
            total_weight += weight
        if total_weight > 1.0:
            total_weight = 1.0
        non_humanoid_total_weights[vert_idx] = total_weight
        if total_weight > 0.999:
            non_humanoid_parts_mask[vert_idx] = 1.0

    transferred_weight_patterns = [None] * num_vertices
    for vert_idx in range(num_vertices):
        groups = all_transferred_weights.get(vert_idx, {})
        converted_weights = defaultdict(float)

        for group_name, weight in groups.items():
            if weight <= 0.0:
                continue
            if group_name in auxiliary_bones_to_humanoid:
                humanoid_name = auxiliary_bones_to_humanoid[group_name]
                if humanoid_name:
                    converted_weights[humanoid_name] += weight
            else:
                humanoid_name = bone_to_humanoid.get(group_name)
                if humanoid_name:
                    converted_weights[humanoid_name] += weight
                else:
                    converted_weights[group_name] += weight
        transferred_weight_patterns[vert_idx] = dict(converted_weights)

    original_non_humanoid_weight_patterns = [None] * num_vertices
    for vert_idx in range(num_vertices):
        groups = original_non_humanoid_weights.get(vert_idx, {})
        converted_weights = defaultdict(float)

        for group_name, weight in groups.items():
            if weight <= 0.0:
                continue

            parent_humanoid = clothing_bone_to_parent_humanoid.get(group_name)
            if parent_humanoid:
                converted_weights[parent_humanoid] += weight
            else:
                converted_weights[group_name] += weight

        original_non_humanoid_weight_patterns[vert_idx] = dict(converted_weights)

    cloth_bm = get_evaluated_mesh(target_obj)
    cloth_bm.verts.ensure_lookup_table()
    cloth_bm.faces.ensure_lookup_table()
    vertex_coords = np.array([v.co for v in cloth_bm.verts])

    pattern_difference_threshold = 0.2
    neighbor_search_radius = 0.005
    non_humanoid_difference_mask = np.zeros_like(non_humanoid_parts_mask)

    hinge_bone_mask = np.zeros_like(non_humanoid_parts_mask)
    hinge_group = target_obj.vertex_groups.get("HingeBone")
    if hinge_group:
        for vert_idx in range(num_vertices):
            for g in target_obj.data.vertices[vert_idx].groups:
                if g.group == hinge_group.index and g.weight > 0.001:
                    hinge_bone_mask[vert_idx] = 1.0
                    break

    if num_vertices > 0:
        kd_tree = cKDTree(vertex_coords)

        def calculate_pattern_difference(weights_a, weights_b):
            if not weights_a and not weights_b:
                return 0.0
            keys = set(weights_a.keys()) | set(weights_b.keys())
            difference = 0.0
            for key in keys:
                difference += abs(weights_a.get(key, 0.0) - weights_b.get(key, 0.0))
            return difference

        for vert_idx, mask_value in enumerate(non_humanoid_parts_mask):
            if mask_value <= 0.0:
                continue

            base_pattern = original_non_humanoid_weight_patterns[vert_idx]
            neighbor_indices = kd_tree.query_ball_point(vertex_coords[vert_idx], neighbor_search_radius)

            for neighbor_idx in neighbor_indices:
                if neighbor_idx == vert_idx:
                    continue

                if non_humanoid_parts_mask[neighbor_idx] > 0.001:
                    continue

                neighbor_pattern = transferred_weight_patterns[neighbor_idx]
                if not neighbor_pattern:
                    continue

                difference = calculate_pattern_difference(base_pattern, neighbor_pattern)
                if difference > pattern_difference_threshold:
                    non_humanoid_difference_mask[vert_idx] = 1.0 * hinge_bone_mask[vert_idx] * hinge_bone_mask[vert_idx]
                    break

    # Add non_humanoid_difference_mask as a vertex group
    non_humanoid_difference_group = target_obj.vertex_groups.new(name="NonHumanoidDifference")
    for vert_idx, mask_value in enumerate(non_humanoid_difference_mask):
        if mask_value > 0.0:
            non_humanoid_difference_group.add([vert_idx], 1.0, 'REPLACE')

    # Save current mode
    current_mode = bpy.context.object.mode

    # Switch to Weight Paint mode
    bpy.context.view_layer.objects.active = target_obj
    bpy.ops.object.mode_set(mode='WEIGHT_PAINT')
    target_obj.vertex_groups.active_index = non_humanoid_difference_group.index
    bpy.ops.paint.vert_select_all(action='SELECT')

    # Smoothing using vertex_group_smooth
    bpy.ops.object.vertex_group_smooth(factor=0.5, repeat=5, expand=0.5)

    # Create a DistanceFalloffMask group
    falloff_mask_time_start = time.time()
    # Retrieve the distance parameter from CommonSwaySettings
    sway_settings = base_avatar_data.get("commonSwaySettings", {"startDistance": 0.025, "endDistance": 0.050})
    distance_falloff_group = create_distance_falloff_transfer_mask(target_obj, base_avatar_data, 'DistanceFalloffMask',
                                                                 max_distance=sway_settings["endDistance"],
                                                                 min_distance=sway_settings["startDistance"])
    target_obj.vertex_groups.active_index = distance_falloff_group.index

    # Smoothing using vertex_group_smooth
    bpy.ops.object.vertex_group_smooth(factor=1, repeat=3, expand=0.1)
    falloff_mask_time = time.time() - falloff_mask_time_start
    print(f"  Distance Falloff Mask Creation: {falloff_mask_time:.2f} seconds")

    distance_falloff_group2 = create_distance_falloff_transfer_mask(target_obj, base_avatar_data, 'DistanceFalloffMask2',
                                                                 max_distance=0.1,
                                                                 min_distance=0.04)
    target_obj.vertex_groups.active_index = distance_falloff_group2.index

    # Smoothing using vertex_group_smooth
    bpy.ops.object.vertex_group_smooth(factor=1, repeat=3, expand=0.1)

    print(f"  distance_falloff_group2: {distance_falloff_group2.index}")

    # Restore to original mode
    bpy.ops.object.mode_set(mode=current_mode)

    non_humanoid_difference_weights = np.zeros(num_vertices)
    distance_falloff_weights = np.zeros(num_vertices)
    for vert_idx in range(num_vertices):
        for g in target_obj.data.vertices[vert_idx].groups:
            if g.group == non_humanoid_difference_group.index:
                non_humanoid_difference_weights[vert_idx] = g.weight
            if g.group == distance_falloff_group2.index:
                distance_falloff_weights[vert_idx] = g.weight

    for vert_idx, groups in original_non_humanoid_weights.items():
        for group_name, weight in groups.items():
            if group_name in target_obj.vertex_groups:
                result_weight = weight * ( 1.0 - non_humanoid_difference_weights[vert_idx] * distance_falloff_weights[vert_idx] )
                target_obj.vertex_groups[group_name].add([vert_idx], result_weight, 'REPLACE')

    current_humanoid_weights = store_weights(target_obj, bone_groups)
    for vert_idx, groups in current_humanoid_weights.items():
        for group_name, weight in groups.items():
            if group_name in target_obj.vertex_groups:
                factor = ( 1.0 - non_humanoid_total_weights[vert_idx] * (1.0 - non_humanoid_difference_weights[vert_idx] * distance_falloff_weights[vert_idx]) )
                result_weight = weight * factor
                target_obj.vertex_groups[group_name].add([vert_idx], result_weight, 'REPLACE')

    for vert_idx in range(len(non_humanoid_total_weights)):
        non_humanoid_total_weights[vert_idx] = non_humanoid_total_weights[vert_idx] * (1.0 - non_humanoid_difference_weights[vert_idx] * distance_falloff_weights[vert_idx])

    cloth_bm.free()


    # Find the parent bone for each new group and merge the weights
    group_merge_time_start = time.time()
    max_iterations = 5
    iteration = 0
    while added_groups and iteration < max_iterations:
        changed = False
        remaining_groups = set()

        print(f"  Iteration: {iteration}")

        for group_name in added_groups:

            print(f"  Group Name: {group_name}")

            if group_name not in target_obj.vertex_groups:
                print(f"  {group_name} has been deleted. Skipping")
                continue

            # Retrieve vertices where the weight of the new group is greater than zero
            group = target_obj.vertex_groups[group_name]
            verts_with_weight = []
            for v in target_obj.data.vertices:
                weight = get_vertex_weight_safe(group, v.index)
                if weight > 0:
                    verts_with_weight.append(v)

            print(f"  Number of vertices with weight: {len(verts_with_weight)}")

            if len(verts_with_weight) == 0:
                print(f"  {group_name} is empty: Skipping")
                continue

            if group_name in bone_to_humanoid:
                humanoid_group_name = bone_to_humanoid[group_name]
                if "LeftToes" in humanoid_to_bone and humanoid_to_bone["LeftToes"] in original_groups:
                    if humanoid_group_name in left_foot_finger_humanoid_bones:
                        merge_weights_to_parent(target_obj, group_name, humanoid_to_bone["LeftToes"])
                        changed = True
                        continue
                if "RightToes" in humanoid_to_bone and humanoid_to_bone["RightToes"] in original_groups:
                    if humanoid_group_name in right_foot_finger_humanoid_bones:
                        merge_weights_to_parent(target_obj, group_name, humanoid_to_bone["RightToes"])
                        changed = True
                        continue

            # Find the corresponding existing group
            existing_groups = set()
            for vert in verts_with_weight:
                for g in vert.groups:
                    g_name = target_obj.vertex_groups[g.group].name
                    if g_name in bone_groups and g_name in original_groups and g.weight > 0:
                        existing_groups.add(g_name)

            print(f"  Existing groups: {existing_groups}")

            if len(existing_groups) == 1:
                # If there is only one existing group that matches, merge into it
                merge_weights_to_parent(target_obj, group_name, list(existing_groups)[0])
                changed = True
            elif len(existing_groups) == 0:
                # If no matching existing group exists, search adjacent vertices as well
                bm = bmesh.new()
                bm.from_mesh(target_obj.data)
                bm.verts.ensure_lookup_table()

                visited_verts = set(vert.index for vert in verts_with_weight)
                queue = deque(verts_with_weight)

                while queue:
                    vert = queue.popleft()
                    for edge in bm.verts[vert.index].link_edges:
                        other_vert = edge.other_vert(bm.verts[vert.index])
                        if other_vert.index not in visited_verts:
                            visited_verts.add(other_vert.index)
                            for g in target_obj.data.vertices[other_vert.index].groups:
                                if target_obj.vertex_groups[g.group].name in bone_groups and g.weight > 0:
                                    existing_groups.add(target_obj.vertex_groups[g.group].name)
                                    if len(existing_groups) > 1:
                                        break
                            if len(existing_groups) == 1:
                                merge_weights_to_parent(target_obj, group_name, existing_groups.pop())
                                changed = True
                                break
                            queue.append(target_obj.data.vertices[other_vert.index])

                bm.free()

                print(f"  Existing groups after adjacent search: {existing_groups}")

            if len(existing_groups) != 1:
                remaining_groups.add(group_name)

        if not changed:
            break

        added_groups = remaining_groups
        iteration += 1
    group_merge_time = time.time() - group_merge_time_start
    print(f"  Group Merge Processing: {group_merge_time:.2f} seconds")

    # Process auxiliary bones for new groups that could not be merged
    aux_bone_time_start = time.time()
    for group_name in list(added_groups):  # Create a copy of the Set and iterate
        for aux_set in base_avatar_data.get("auxiliaryBones", []):
            if group_name in aux_set["auxiliaryBones"]:
                humanoid_bone = aux_set["humanoidBoneName"]
                if humanoid_bone in humanoid_to_bone and humanoid_to_bone[humanoid_bone] in bone_groups:
                    merge_weights_to_parent(target_obj, group_name, humanoid_to_bone[humanoid_bone])
                    try:
                        added_groups.remove(group_name)
                    except KeyError:
                        pass  # Ignore if group_name has already been deleted
                    break

    # For new groups that still could not be consolidated, add the original weight
    for group_name in added_groups:
        if group_name not in target_obj.vertex_groups:
            continue
        group = target_obj.vertex_groups[group_name]
        for vert in target_obj.data.vertices:
            weight = get_vertex_weight_safe(group, vert.index)
            if weight > 0:
                for orig_group_name, orig_weight in original_humanoid_weights[vert.index].items():
                    if orig_group_name in target_obj.vertex_groups:
                        target_obj.vertex_groups[orig_group_name].add([vert.index], orig_weight * weight, 'ADD')

    # Delete new group
    for group_name in added_groups:
        if group_name in target_obj.vertex_groups:
            target_obj.vertex_groups.remove(target_obj.vertex_groups[group_name])
    aux_bone_time = time.time() - aux_bone_time_start
    print(f"  Auxiliary bone processing: {aux_bone_time:.2f} seconds")

    # Save the current weight as Result A
    store_result_a_time_start = time.time()
    weights_a = {}
    for vert_idx in range(len(target_obj.data.vertices)):
        weights_a[vert_idx] = {}
        for group in target_obj.vertex_groups:
            if group.name in bone_groups:
                try:
                    weight = 0.0
                    for g in target_obj.data.vertices[vert_idx].groups:
                        if g.group == group.index:
                            weight = g.weight
                            break
                    weights_a[vert_idx][group.name] = weight
                except Exception:
                    continue
    store_result_a_time = time.time() - store_result_a_time_start
    print(f"  Result A saved: {store_result_a_time:.2f} seconds")

    # Copy the current weight to create result B
    store_result_b_time_start = time.time()
    weights_b = {}
    for vert_idx in range(len(target_obj.data.vertices)):
        weights_b[vert_idx] = {}
        for group in target_obj.vertex_groups:
            if group.name in bone_groups:
                try:
                    weight = 0.0
                    for g in target_obj.data.vertices[vert_idx].groups:
                        if g.group == group.index:
                            weight = g.weight
                            break
                    weights_b[vert_idx][group.name] = weight
                except Exception:
                    continue
    store_result_b_time = time.time() - store_result_b_time_start
    print(f"  Result B saved: {store_result_b_time:.2f} seconds")

    # swayBones integration processing
    sway_bones_time_start = time.time()
    for sway_bone in base_avatar_data.get("swayBones", []):
        parent_bone = sway_bone["parentBoneName"]
        for affected_bone in sway_bone["affectedBones"]:
            # Process each vertex
            for vert_idx in weights_b:
                if affected_bone in weights_b[vert_idx]:
                    affected_weight = weights_b[vert_idx][affected_bone]
                    # Add to the parent bone's weight
                    if parent_bone not in weights_b[vert_idx]:
                        weights_b[vert_idx][parent_bone] = 0.0
                    weights_b[vert_idx][parent_bone] += affected_weight
                    # Remove the weight for affected_bone
                    del weights_b[vert_idx][affected_bone]
    sway_bones_time = time.time() - sway_bones_time_start
    print(f"  SwayBones processing: {sway_bones_time:.2f} seconds")

    # Combine results A and B
    weight_blend_time_start = time.time()
    for vert_idx in range(len(target_obj.data.vertices)):
        # Get the weight of the DistanceFalloffMask
        falloff_weight = 0.0
        for g in target_obj.data.vertices[vert_idx].groups:
            if g.group == distance_falloff_group.index:
                falloff_weight = g.weight
                break

        # Process each vertex group
        for group_name in bone_groups:
            if group_name in target_obj.vertex_groups:
                weight_a = weights_a[vert_idx].get(group_name, 0.0)
                weight_b = weights_b[vert_idx].get(group_name, 0.0)

                # Weight synthesis
                final_weight = (weight_a * falloff_weight) + (weight_b * (1.0 - falloff_weight))

                # Set new weight
                group = target_obj.vertex_groups[group_name]
                if final_weight > 0:
                    group.add([vert_idx], final_weight, 'REPLACE')
                else:
                    try:
                        group.remove([vert_idx])
                    except RuntimeError:
                        pass
    weight_blend_time = time.time() - weight_blend_time_start
    print(f"  Weight blend time: {weight_blend_time:.2f} seconds")

    # Hand weight adjustment
    hand_weights_time_start = time.time()
    adjust_hand_weights(target_obj, armature, base_avatar_data)
    hand_weights_time = time.time() - hand_weights_time_start
    print(f"  Hand weight adjustment: {hand_weights_time:.2f} seconds")

    #normalize_connected_components_weights(target_obj, base_avatar_data)

    # Weight Propagation to Side Vertices
    propagate_time_start = time.time()
    propagate_weights_to_side_vertices(target_obj, bone_groups, original_humanoid_weights, clothing_armature)
    propagate_time = time.time() - propagate_time_start
    print(f"  Weight propagation to side vertices: {propagate_time:.2f} seconds")

    # Comparison and Adjustment of Side Weight and Bone Weight
    comparison_time_start = time.time()
    side_left_group = target_obj.vertex_groups.get("LeftSideWeights")
    side_right_group = target_obj.vertex_groups.get("RightSideWeights")

    failed_vertices_count = 0
    if side_left_group and side_right_group:
        for vert in target_obj.data.vertices:
            # Calculate the total side weight
            total_side_weight = 0.0
            for g in vert.groups:
                if g.group == side_left_group.index or g.group == side_right_group.index:
                    total_side_weight += g.weight
            total_side_weight = min(total_side_weight, 1.0)  # Clamp at 0-1

            total_side_weight = total_side_weight - non_humanoid_total_weights[vert.index]
            total_side_weight = max(total_side_weight, 0.0)

            # Calculate the total weight of bone_groups
            total_bone_weight = 0.0
            for g in vert.groups:
                group_name = target_obj.vertex_groups[g.group].name
                if group_name in bone_groups:
                    total_bone_weight += g.weight

            # When the side weight is 0.5 or more greater than the bone weight
            if total_side_weight > total_bone_weight + 0.5:
                # Clear the current weights for bone_groups
                for group in target_obj.vertex_groups:
                    if group.name in bone_groups:
                        try:
                            group.remove([vert.index])
                        except RuntimeError:
                            continue

                # Restore original weight
                if vert.index in original_humanoid_weights:
                    for group_name, weight in original_humanoid_weights[vert.index].items():
                        if group_name in target_obj.vertex_groups:
                            target_obj.vertex_groups[group_name].add([vert.index], weight, 'REPLACE')
                failed_vertices_count += 1
    if failed_vertices_count > 0:
        print(f"  Weight transfer failed: {failed_vertices_count} vertices -> Fallback to original weights")
    comparison_time = time.time() - comparison_time_start
    print(f"  Side-by-side comparison adjustment: {comparison_time:.2f} seconds")

    # Execute apply_distance_normal_based_smoothing
    smoothing_time_start = time.time()

    # Construct target_vertex_groups (Chest, LeftBreast, RightBreast and their auxiliaryBones)
    target_vertex_groups = []
    smoothing_mask_groups = []
    target_humanoid_bones = [
        "Chest", "LeftBreast", "RightBreast", "Neck", "Head", "LeftShoulder", "RightShoulder", "LeftUpperArm", "RightUpperArm",
        "LeftHand",
        "LeftThumbProximal", "LeftThumbIntermediate", "LeftThumbDistal",
        "LeftIndexProximal", "LeftIndexIntermediate", "LeftIndexDistal",
        "LeftMiddleProximal", "LeftMiddleIntermediate", "LeftMiddleDistal",
        "LeftRingProximal", "LeftRingIntermediate", "LeftRingDistal",
        "LeftLittleProximal", "LeftLittleIntermediate", "LeftLittleDistal",
        "RightHand",
        "RightThumbProximal", "RightThumbIntermediate", "RightThumbDistal",
        "RightIndexProximal", "RightIndexIntermediate", "RightIndexDistal",
        "RightMiddleProximal", "RightMiddleIntermediate", "RightMiddleDistal",
        "RightRingProximal", "RightRingIntermediate", "RightRingDistal",
        "RightLittleProximal", "RightLittleIntermediate", "RightLittleDistal"
        ]
    smoothing_mask_humanoid_bones = [
        "Chest", "LeftBreast", "RightBreast", "Neck", "Head", "LeftShoulder", "RightShoulder",
        "LeftHand",
        "LeftThumbProximal", "LeftThumbIntermediate", "LeftThumbDistal",
        "LeftIndexProximal", "LeftIndexIntermediate", "LeftIndexDistal",
        "LeftMiddleProximal", "LeftMiddleIntermediate", "LeftMiddleDistal",
        "LeftRingProximal", "LeftRingIntermediate", "LeftRingDistal",
        "LeftLittleProximal", "LeftLittleIntermediate", "LeftLittleDistal",
        "RightHand",
        "RightThumbProximal", "RightThumbIntermediate", "RightThumbDistal",
        "RightIndexProximal", "RightIndexIntermediate", "RightIndexDistal",
        "RightMiddleProximal", "RightMiddleIntermediate", "RightMiddleDistal",
        "RightRingProximal", "RightRingIntermediate", "RightRingDistal",
        "RightLittleProximal", "RightLittleIntermediate", "RightLittleDistal"
        ]
    humanoid_to_bone = {bone_map["humanoidBoneName"]: bone_map["boneName"]
                        for bone_map in base_avatar_data["humanoidBones"]}

    for humanoid_bone in target_humanoid_bones:
        if humanoid_bone in humanoid_to_bone:
            target_vertex_groups.append(humanoid_to_bone[humanoid_bone])

    # Add auxiliaryBones
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        if aux_set["humanoidBoneName"] in target_humanoid_bones:
            target_vertex_groups.extend(aux_set["auxiliaryBones"])

    for humanoid_bone in smoothing_mask_humanoid_bones:
        if humanoid_bone in humanoid_to_bone:
            smoothing_mask_groups.append(humanoid_to_bone[humanoid_bone])

    # Add auxiliaryBones
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        if aux_set["humanoidBoneName"] in smoothing_mask_humanoid_bones:
            smoothing_mask_groups.extend(aux_set["auxiliaryBones"])

    # Get the Body.BaseAvatar object
    body_obj = bpy.data.objects.get("Body.BaseAvatar")

    # Check if there are any vertices with non-zero bone weight for LeftBreast or RightBreast
    breast_bone_groups = []
    breast_humanoid_bones = ["Hips", "LeftBreast", "RightBreast", "Neck", "Head", "LeftHand", "RightHand"]

    for humanoid_bone in breast_humanoid_bones:
        if humanoid_bone in humanoid_to_bone:
            breast_bone_groups.append(humanoid_to_bone[humanoid_bone])

    # Also added auxiliaryBones for LeftBreast and RightBreast
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        if aux_set["humanoidBoneName"] in breast_humanoid_bones:
            breast_bone_groups.extend(aux_set["auxiliaryBones"])

    # Check if target_obj has any vertices with non-zero weights in breast_bone_groups
    has_breast_weights = False
    if breast_bone_groups:
        for group_name in breast_bone_groups:
            if group_name in target_obj.vertex_groups:
                group = target_obj.vertex_groups[group_name]
                # Check whether the vertex group actually has weights
                for vert in target_obj.data.vertices:
                    try:
                        weight = 0.0
                        for g in vert.groups:
                            if g.group == group.index:
                                weight = g.weight
                                break
                        if weight > 0:
                            has_breast_weights = True
                            break
                    except RuntimeError:
                        continue
                if has_breast_weights:
                    break

    if body_obj and target_vertex_groups and has_breast_weights:
        print(f"  Perform distance- and normal-based smoothing: {len(target_vertex_groups)} target groups (LeftBreast/RightBreast weight detection)")
        apply_distance_normal_based_smoothing(
            body_obj=body_obj,
            cloth_obj=target_obj,
            distance_min=0.005,
            distance_max=0.015,
            angle_min=15.0,
            angle_max=30.0,
            new_group_name="SmoothMask",
            normal_radius=0.01,
            smoothing_mask_groups=smoothing_mask_groups,
            target_vertex_groups=target_vertex_groups,
            smoothing_radius=0.05,
            mask_group_name="MF_Blur"
        )
    else:
        print("  The Body.BaseAvatar object cannot be found, or the target group is empty")

    smoothing_time = time.time() - smoothing_time_start
    print(f"  Distance/Normal-Based Smoothing: {smoothing_time:.2f} seconds")

    # Adjust the synthesis so that the ratio of the original weight increases as the distance increases
    current_mode = bpy.context.object.mode
    bpy.context.view_layer.objects.active = target_obj
    bpy.ops.object.mode_set(mode='WEIGHT_PAINT')

    target_obj.vertex_groups.active_index = distance_falloff_group2.index

    print(f"  distance_falloff_group2: {distance_falloff_group2.index}")
    print(f"  distance_falloff_group2_index: {target_obj.vertex_groups[distance_falloff_group2.name].index}")

    # Check if there are any vertices with non-zero bone weight for LeftBreast or RightBreast
    exclude_bone_groups = []
    exclude_humanoid_bones = ["LeftBreast", "RightBreast"]

    for humanoid_bone in exclude_humanoid_bones:
        if humanoid_bone in humanoid_to_bone:
            exclude_bone_groups.append(humanoid_to_bone[humanoid_bone])

    # Also added auxiliaryBones for LeftBreast and RightBreast
    for aux_set in base_avatar_data.get("auxiliaryBones", []):
        if aux_set["humanoidBoneName"] in exclude_humanoid_bones:
            exclude_bone_groups.extend(aux_set["auxiliaryBones"])

    # The chest area is excluded from the compositing process
    if exclude_bone_groups:
        new_group_weights = np.zeros(len(target_obj.data.vertices), dtype=np.float32)
        for i, vertex in enumerate(target_obj.data.vertices):
            for group in vertex.groups:
                if group.group == distance_falloff_group2.index:
                    new_group_weights[i] = group.weight
                    break
        total_target_weights = np.zeros(len(target_obj.data.vertices), dtype=np.float32)

        for target_group_name in exclude_bone_groups:
            if target_group_name in target_obj.vertex_groups:
                target_group = target_obj.vertex_groups[target_group_name]
                print(f"    Retrieving weights for vertex group '{target_group_name}'...")

                for i, vertex in enumerate(target_obj.data.vertices):
                    for group in vertex.groups:
                        if group.group == target_group.index:
                            total_target_weights[i] += group.weight
                            break
            else:
                print(f"    Warning: Vertex group '{target_group_name}' not found")

        masked_weights = np.maximum(new_group_weights, total_target_weights)

        # Apply the result to the new vertex group
        for i in range(len(target_obj.data.vertices)):
            distance_falloff_group2.add([i], masked_weights[i], 'REPLACE')

    for vert_idx in range(len(target_obj.data.vertices)):
        if vert_idx in original_humanoid_weights and non_humanoid_parts_mask[vert_idx] < 0.0001:
            falloff_weight = 0.0
            for g in target_obj.data.vertices[vert_idx].groups:
                if g.group == distance_falloff_group2.index:
                    falloff_weight = g.weight
                    break

            for g in target_obj.data.vertices[vert_idx].groups:
                if target_obj.vertex_groups[g.group].name in bone_groups:
                    weight = g.weight
                    group_name = target_obj.vertex_groups[g.group].name
                    target_obj.vertex_groups[group_name].add([vert_idx], weight * falloff_weight, 'REPLACE')

            for group_name, weight in original_humanoid_weights[vert_idx].items():
                if group_name in target_obj.vertex_groups:
                    target_obj.vertex_groups[group_name].add([vert_idx], weight * (1.0 - falloff_weight), 'ADD')

    bpy.ops.object.mode_set(mode=current_mode)


    # Process to reset the Head bone's weight to its original value
    head_time_start = time.time()
    head_bone_name = None
    # Search for bones with the Head label in base_avatar_data
    if base_avatar_data:
        if "humanoidBones" in base_avatar_data:
            for bone_data in base_avatar_data["humanoidBones"]:
                if bone_data.get("humanoidBoneName", "") == "Head":
                    head_bone_name = bone_data.get("boneName", "")
                    break

    if head_bone_name and head_bone_name in target_obj.vertex_groups:
        print(f"  Processing Head bone weight: {head_bone_name}")
        head_vertices_count = 0

        for vert_idx in range(len(target_obj.data.vertices)):
            # Get the original Head weight
            original_head_weight = 0.0
            if vert_idx in original_humanoid_weights:
                original_head_weight = original_humanoid_weights[vert_idx].get(head_bone_name, 0.0)

            # Get the current Head weight
            current_head_weight = 0.0
            for g in target_obj.data.vertices[vert_idx].groups:
                if g.group == target_obj.vertex_groups[head_bone_name].index:
                    current_head_weight = g.weight
                    break

            # Calculate the difference in Head weight
            head_weight_diff = original_head_weight - current_head_weight

            # Set the Head weight to its original value
            if original_head_weight > 0.0:
                target_obj.vertex_groups[head_bone_name].add([vert_idx], original_head_weight, 'REPLACE')
            else:
                # If the original is 0, delete
                try:
                    target_obj.vertex_groups[head_bone_name].remove([vert_idx])
                except RuntimeError:
                    pass

            # If there is a difference, multiply the original weight of the other bone by the difference and add it
            if abs(head_weight_diff) > 0.0001 and vert_idx in original_humanoid_weights:
                for group in target_obj.vertex_groups:
                    if group.name in bone_groups and group.name != head_bone_name:
                        # Retrieve the original weight
                        original_weight = original_humanoid_weights[vert_idx].get(group.name, 0.0)

                        if original_weight > 0.0:
                            # Get the current weight
                            current_weight = 0.0
                            for g in target_obj.data.vertices[vert_idx].groups:
                                if g.group == group.index:
                                    current_weight = g.weight
                                    break

                            # Addition based on differences
                            new_weight = current_weight + (original_weight * head_weight_diff)
                            if new_weight > 0.0:
                                group.add([vert_idx], new_weight, 'REPLACE')
                            else:
                                try:
                                    group.remove([vert_idx])
                                except RuntimeError:
                                    pass

            # If the total weight of all_deform_groups is less than 1, compensate accordingly
            total_weight = 0.0
            for g in target_obj.data.vertices[vert_idx].groups:
                group_name = target_obj.vertex_groups[g.group].name
                if group_name in all_deform_groups:
                    total_weight += g.weight

            # If the total weight is less than 1, make up the difference
            if total_weight < 0.9999 and vert_idx in original_humanoid_weights:
                weight_shortage = 1.0 - total_weight

                for group in target_obj.vertex_groups:
                    if group.name in bone_groups:
                        # Retrieve the original weight
                        original_weight = original_humanoid_weights[vert_idx].get(group.name, 0.0)

                        if original_weight > 0.0:
                            # Get the current weight
                            current_weight = 0.0
                            for g in target_obj.data.vertices[vert_idx].groups:
                                if g.group == group.index:
                                    current_weight = g.weight
                                    break

                            # Add the shortfall based on the original weight
                            additional_weight = original_weight * weight_shortage
                            new_weight = current_weight + additional_weight
                            group.add([vert_idx], new_weight, 'REPLACE')

            head_vertices_count += 1

        if head_vertices_count > 0:
            print(f"  Head weight processing complete: {head_vertices_count} vertices")

    head_time = time.time() - head_time_start
    print(f"  Head processing time: {head_time:.2f} seconds")

    # Selectively restore weights based on clothMetadata
    metadata_time_start = time.time()
    if cloth_metadata:
        mesh_name = target_obj.name
        if mesh_name in cloth_metadata:
            vertex_max_distances = cloth_metadata[mesh_name]
            print(f"  Processing mesh cross metadata: {mesh_name}")

            count = 0
            # Process each vertex
            for vert_idx in range(len(target_obj.data.vertices)):
                # Get maxDistance (use 10.0 if not available)
                max_distance = float(vertex_max_distances.get(str(vert_idx), 10.0))

                # If maxDistance is greater than 1.0, restore the original weight
                if max_distance > 1.0:
                    if vert_idx in original_humanoid_weights:
                        # Delete all current groups
                        for group in target_obj.vertex_groups:
                            if group.name in bone_groups:
                                try:
                                    group.remove([vert_idx])
                                except RuntimeError:
                                    continue

                        # Restore original weight
                        for group_name, weight in original_humanoid_weights[vert_idx].items():
                            if group_name in target_obj.vertex_groups:
                                target_obj.vertex_groups[group_name].add([vert_idx], weight, 'REPLACE')
                        count += 1
            print(f"  Number of processed vertices: {count}")
    metadata_time = time.time() - metadata_time_start
    print(f"  Cross metadata processing: {metadata_time:.2f} seconds")

    total_time = time.time() - start_time
    print(f"Processing complete: {target_obj.name} - Total time: {total_time:.2f} seconds")


def apply_pose_as_rest(armature):
    # Save active objects
    original_active = bpy.context.active_object

    # Get the specified armature
    if not armature or armature.type != 'ARMATURE':
        print(f"Error: {armature.name} is not a valid armature object")
        return

    # Set the armature to active
    bpy.context.view_layer.objects.active = armature

    # Enter edit mode
    bpy.ops.object.mode_set(mode='POSE')
    bpy.ops.pose.select_all(action='SELECT')

    # Apply the current pose as a rest pose
    bpy.ops.pose.armature_apply()

    # Return to original mode
    bpy.ops.object.mode_set(mode='OBJECT')

    # Restore the original active object
    bpy.context.view_layer.objects.active = original_active



def apply_all_transforms():
    """Apply transforms to all objects while maintaining world space positions"""

    bpy.ops.object.mode_set(mode='OBJECT')

    # Save selection state
    original_selection = {obj: obj.select_get() for obj in bpy.data.objects}
    original_active = bpy.context.view_layer.objects.active

    # Retrieve all objects and sort them by depth of parent-child relationships
    def get_object_depth(obj):
        depth = 0
        parent = obj.parent
        while parent:
            depth += 1
            parent = parent.parent
        return depth

    # Sort to process from the deepest level down
    all_objects = sorted(bpy.data.objects, key=get_object_depth, reverse=True)

    # List for storing parent-child relationship information
    parent_info_list = []

    # Phase 1: Break the parent-child relationship for all objects and apply the Transform
    for obj in all_objects:
        if obj.type not in {'MESH', 'EMPTY', 'ARMATURE', 'CURVE', 'SURFACE', 'FONT'}:
            continue

        # Clear all selections
        bpy.ops.object.select_all(action='DESELECT')

        # Select the current object to make it active
        obj.select_set(True)
        bpy.context.view_layer.objects.active = obj

        # Save parent-child relationship information
        parent = obj.parent
        parent_type = obj.parent_type
        parent_bone = obj.parent_bone if parent_type == 'BONE' else None

        if parent:
            parent_info_list.append({
                'obj': obj,
                'parent': parent,
                'parent_type': parent_type,
                'parent_bone': parent_bone
            })

        # Temporarily unlink parent-child relationship (while retaining position)
        if parent:
            bpy.ops.object.parent_clear(type='CLEAR_KEEP_TRANSFORM')

        # For Mesh objects with an Armature object or Armature modifier
        has_armature = obj.type == 'ARMATURE' or \
                      (obj.type == 'MESH' and any(mod.type == 'ARMATURE' for mod in obj.modifiers))

        if has_armature:
            # Apply all Transform
            bpy.ops.object.transform_apply(location=True, rotation=True, scale=True)
        else:
            # Scale only applied
            bpy.ops.object.transform_apply(location=False, rotation=False, scale=True)

    # Phase 2: Restore all parent-child relationships in bulk
    for parent_info in parent_info_list:
        obj = parent_info['obj']
        parent = parent_info['parent']
        parent_type = parent_info['parent_type']
        parent_bone = parent_info['parent_bone']

        # Clear all selections
        bpy.ops.object.select_all(action='DESELECT')

        if parent_type == 'BONE' and parent_bone:
            # If you were the bone parent
            obj.select_set(True)
            bpy.context.view_layer.objects.active = parent
            parent.select_set(True)

            # Switch to Pose Mode and set the bone to active
            bpy.ops.object.mode_set(mode='POSE')
            parent.data.bones.active = parent.data.bones[parent_bone]

            # Return to Object Mode
            bpy.ops.object.mode_set(mode='OBJECT')

            # Set bone parent
            bpy.ops.object.parent_set(type='BONE', keep_transform=True)
            print(f"Restored bone parent '{parent_bone}' for object '{obj.name}'")
        else:
            # If it was the parent object
            obj.select_set(True)
            parent.select_set(True)
            bpy.context.view_layer.objects.active = parent
            bpy.ops.object.parent_set(type='OBJECT', keep_transform=True)

    # Restore original selection state
    for obj, was_selected in original_selection.items():
        obj.select_set(was_selected)
    bpy.context.view_layer.objects.active = original_active


def rename_shape_keys_from_mappings(meshes, blend_shape_mappings):
    """
    Replace mesh shape key names based on dictionary data

    If there is a shape key matching the dictionary value (custom name),
    Replace it with the key (label name)

    Parameters:
        meshes: List of mesh objects
        blend_shape_mappings: {label: customName} dictionary
    """
    if not blend_shape_mappings:
        return

    # Create reverse mapping (customName -> label)
    reverse_mappings = {custom_name: label for label, custom_name in blend_shape_mappings.items()}

    for obj in meshes:
        if not obj.data.shape_keys:
            continue

        # Collect shape keys that need to be renamed
        keys_to_rename = []
        for shape_key in obj.data.shape_keys.key_blocks:
            if shape_key.name in reverse_mappings:
                new_name = reverse_mappings[shape_key.name]
                keys_to_rename.append((shape_key, new_name))

        # Change name
        for shape_key, new_name in keys_to_rename:
            old_name = shape_key.name
            shape_key.name = new_name
            print(f"Renamed shape key: {old_name} -> {new_name} on mesh {obj.name}")

def merge_and_clean_generated_shapekeys(clothing_meshes, blend_shape_labels=None):
    """
    Delete the shape keys created by apply_blendshape_deformation_fields,
    Process shape keys with the _generated suffix

    If a shape key exists with a name that excludes _generated from a shape key name ending with _generated,
    Overwrite that shape key with the contents of the _generated shape key, then delete the _generated shape key

    Parameters:
        clothing_meshes: List of clothing meshes
        blend_shape_labels: List of blend shape labels
    """
    for obj in clothing_meshes:
        if not obj.data.shape_keys:
            continue

        # Processing Shape Keys with the _generated Suffix
        generated_shape_keys = []
        for shape_key in obj.data.shape_keys.key_blocks:
            if shape_key.name.endswith("_generated"):
                generated_shape_keys.append(shape_key.name)

        # Integrate _generated shape keys into corresponding base shape keys
        for generated_name in generated_shape_keys:
            base_name = generated_name[:-10]  # Remove "_generated"

            generated_key = obj.data.shape_keys.key_blocks.get(generated_name)
            base_key = obj.data.shape_keys.key_blocks.get(base_name)

            if generated_key and base_key:
                # Overwrite the base shape key with the contents of the generated shape key
                for i, point in enumerate(generated_key.data):
                    base_key.data[i].co = point.co
                print(f"Merged {generated_name} into {base_name} for {obj.name}")

                # Delete generated shape keys
                obj.shape_key_remove(generated_key)
                print(f"Removed generated shape key: {generated_name} from {obj.name}")

        # Existing functionality: delete shape keys specified in blend_shape_labels
        if blend_shape_labels:
            shape_keys_to_remove = []
            for label in blend_shape_labels:
                shape_key_name = f"{label}_BaseShape"
                if shape_key_name in obj.data.shape_keys.key_blocks:
                    shape_keys_to_remove.append(shape_key_name)

            for label in blend_shape_labels:
                shape_key_name = f"{label}_temp"
                if shape_key_name in obj.data.shape_keys.key_blocks:
                    shape_keys_to_remove.append(shape_key_name)

            # Delete Shape Key
            for shape_key_name in shape_keys_to_remove:
                shape_key = obj.data.shape_keys.key_blocks.get(shape_key_name)
                if shape_key:
                    obj.shape_key_remove(shape_key)
                    print(f"Removed shape key: {shape_key_name} from {obj.name}")

        # Delete unnecessary shape keys
        shape_keys_to_remove = []
        for shape_key in obj.data.shape_keys.key_blocks:
            if shape_key.name.endswith(".MFTemp"):
                shape_keys_to_remove.append(shape_key.name)
        for shape_key_name in shape_keys_to_remove:
            shape_key = obj.data.shape_keys.key_blocks.get(shape_key_name)
            if shape_key:
                obj.shape_key_remove(shape_key)
                print(f"Removed shape key: {shape_key_name} from {obj.name}")


def set_highheel_shapekey_values(clothing_meshes, blend_shape_labels=None, base_avatar_data=None):
    """
    Set the value of the shape key containing Highheel to 1

    Parameters:
        clothing_meshes: List of clothing meshes
        blend_shape_labels: List of blend shape labels
        base_avatar_data: Base Avatar Data
    """
    if not blend_shape_labels or not base_avatar_data:
        return

    # Verify the existence of blendShapeFields in base_avatar_data
    if "blendShapeFields" not in base_avatar_data:
        return

    # First, search for labels containing Highheel
    highheel_labels = [label for label in blend_shape_labels if "highheel" in label.lower() and "off" not in label.lower()]
    base_highheel_fields = [field for field in base_avatar_data["blendShapeFields"]
                          if "highheel" in field.get("label", "").lower() and "off" not in field.get("label", "").lower()]

    # If there are no labels containing Highheel, search for labels containing Heel
    if not highheel_labels:
        highheel_labels = [label for label in blend_shape_labels if "heel" in label.lower() and "off" not in label.lower()]
        base_highheel_fields = [field for field in base_avatar_data["blendShapeFields"]
                              if "heel" in field.get("label", "").lower() and "off" not in field.get("label", "").lower()]

    # Condition: Only one matching label exists in blend_shape_labels, and only one matching field exists in base_avatar_data
    if len(highheel_labels) != 1 or len(base_highheel_fields) != 1:
        return

    # Get the only label and field
    target_label = highheel_labels[0]
    base_field = base_highheel_fields[0]
    base_label = base_field.get("label", "")

    # Check the shape keys for each mesh
    for obj in clothing_meshes:
        if not obj.data.shape_keys:
            continue

        # Search for shape keys in the base_avatar_data label
        if base_label in obj.data.shape_keys.key_blocks:
            shape_key = obj.data.shape_keys.key_blocks[base_label]
            shape_key.value = 1.0
            print(f"Set shape key '{base_label}' value to 1.0 on {obj.name}")



def matrix_to_list(matrix):
    """
    Convert Matrix Type to List (for JSON Storage)

    Parameters:
        matrix: mathutils.Matrix - Matrix to be transformed

    Returns:
        list: Represent each element of the matrix as a list
    """
    return [list(row) for row in matrix]

def export_armature_bone_data_to_json(armature_obj: bpy.types.Object, output_path: str = None) -> dict:
    """
    Output the position, rotation, and scale of all bones contained within the specified Armature in the world coordinate system in JSON format

    Parameters:
        armature_obj: Armature object
        output_path: Output path for JSON files (no file output if not specified)

    Returns:
        dict: Bone information dictionary
    """
    import json

    if not armature_obj or armature_obj.type != 'ARMATURE':
        print(f"Error: Invalid Armature object: {armature_obj}")
        return {}

    bone_data = {
        "armature_name": armature_obj.name,
        "export_timestamp": str(bpy.context.scene.frame_current),
        "bones": {}
    }

    # Activate the armature and set it to Pose mode
    original_active = bpy.context.view_layer.objects.active
    original_mode = bpy.context.mode

    bone_convert_matrix = Matrix(((1.0, 0.0, 0.0, 0.0),
                                  (0.0, 0.0, 1.0, 0.0),
                                  (0.0, -1.0, 0.0, 0.0),
                                  (0.0, 0.0, 0.0, 1.0)))

    try:
        bpy.context.view_layer.objects.active = armature_obj
        bpy.ops.object.mode_set(mode='POSE')

        # Retrieve information for each bone
        for pose_bone in armature_obj.pose.bones:
            bone = armature_obj.data.bones[pose_bone.name]
            # Matrix in the world coordinate system
            world_matrix = armature_obj.matrix_world @ pose_bone.matrix @ bone_convert_matrix

            # Construct the path from the root
            bone_path = []
            current_bone = pose_bone
            while current_bone:
                bone_path.insert(0, current_bone.name)  # Insert at the beginning and sort in order from the root
                current_bone = current_bone.parent

            bone_info = {
                "matrix": matrix_to_list(world_matrix),
                "parent": pose_bone.parent.name if pose_bone.parent else None,
                "bone_path": bone_path,
                "bone_depth": len(bone_path) - 1,  # Calculate depth with root as 0
                "bone_length": float(pose_bone.length)
            }

            bone_data["bones"][pose_bone.name] = bone_info

        # Output as a JSON file
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(bone_data, f, indent=2, ensure_ascii=False)
            print(f"Bone information has been output to a JSON file: {output_path}")

        print(f"Bone information for Armature '{armature_obj.name}' retrieved: {len(bone_data['bones'])} bones")
        return bone_data

    except Exception as e:
        print(f"An error occurred while retrieving bone information: {e}")
        return {}

    finally:
        # Restore to original state
        if original_active:
            bpy.context.view_layer.objects.active = original_active
        if original_mode != 'POSE':
            try:
                bpy.ops.object.mode_set(mode='OBJECT')
            except:
                pass



def round_bone_coordinates(armature: bpy.types.Object, decimal_places: int = 6) -> None:
    """
    Rounds the head, tail coordinates, and Roll values of all bones in the armature to the specified decimal place.

    Args:
        armature: The target armature object
        decimal_places: Number of decimal places to round to (default: 6)
    """
    if not armature or armature.type != 'ARMATURE':
        print("Warning: Invalid armature object for rounding bone coordinates")
        return

    # Switch to edit mode
    bpy.context.view_layer.objects.active = armature
    bpy.ops.object.mode_set(mode='EDIT')

    try:
        edit_bones = armature.data.edit_bones
        rounded_count = 0

        for bone in edit_bones:
            # Round the coordinates of the head
            bone.head.x = round(bone.head.x, decimal_places)
            bone.head.y = round(bone.head.y, decimal_places)
            bone.head.z = round(bone.head.z, decimal_places)

            # Round the tail coordinates
            bone.tail.x = round(bone.tail.x, decimal_places)
            bone.tail.y = round(bone.tail.y, decimal_places)
            bone.tail.z = round(bone.tail.z, decimal_places)

            # Round the Roll value
            bone.roll = round(bone.roll, decimal_places - 3)

            rounded_count += 1

        print(f"Bone coordinate rounding complete: {rounded_count} bones (rounded to {decimal_places} decimal places)")

    finally:
        # Restore to original mode
        bpy.ops.object.mode_set(mode='OBJECT')


def export_fbx(filepath: str, selected_only: bool = True) -> None:
    """Export selected objects to FBX."""
    try:
        bpy.ops.export_scene.fbx(
            filepath=filepath,
            use_selection=selected_only,
            apply_scale_options='FBX_SCALE_ALL',
            apply_unit_scale=True,
            add_leaf_bones=False,
            axis_forward='-Z', axis_up='Y'
        )
    except Exception as e:
        raise Exception(f"Failed to export FBX: {str(e)}")



def propagate_bone_weights(mesh_obj: bpy.types.Object, temp_group_name: str = "PropagatedWeightsTemp", max_iterations: int = 500) -> Optional[str]:
    """
    Propagate weights to vertices without bone weights involved in bone deformation.

    Parameters:
        mesh_obj: Mesh object
        max_iterations: Maximum number of iterations

    Returns:
        Optional[str]: The name of the vertex group recording propagated vertices. Use None if propagation is not required
    """
    # Retrieve the armature from the Armature Modifier
    armature_obj = None
    for modifier in mesh_obj.modifiers:
        if modifier.type == 'ARMATURE':
            armature_obj = modifier.object
            break

    if not armature_obj:
        print(f"Warning: No armature modifier found in {mesh_obj.name}")
        return None

    # Get all bone names of the armature
    deform_groups = {bone.name for bone in armature_obj.data.bones}

    # Create a BMesh
    bm = bmesh.new()
    bm.from_mesh(mesh_obj.data)
    bm.verts.ensure_lookup_table()
    bm.edges.ensure_lookup_table()

    # Retrieve weight information for each vertex
    vertex_weights = {}
    vertices_without_weights = set()

    for vert in mesh_obj.data.vertices:
        has_weight = False
        weights = {}

        for group in mesh_obj.vertex_groups:
            if group.name in deform_groups:
                try:
                    weight = 0.0
                    for g in vert.groups:
                        if g.group == group.index:
                            weight = g.weight
                            has_weight = True
                            break
                    if weight > 0:
                        weights[group.name] = weight
                except RuntimeError:
                    continue

        vertex_weights[vert.index] = weights
        if not weights:
            vertices_without_weights.add(vert.index)

    # If there are no vertices without weight, terminate processing
    if not vertices_without_weights:
        return None

    print(f"Found {len(vertices_without_weights)} vertices without weights in {mesh_obj.name}")

    # Create a temporary vertex group (deleting any existing group with the same name if present)
    if temp_group_name in mesh_obj.vertex_groups:
        mesh_obj.vertex_groups.remove(mesh_obj.vertex_groups[temp_group_name])
    temp_group = mesh_obj.vertex_groups.new(name=temp_group_name)

    # Iterative processing
    total_propagated = 0
    iteration = 0
    while iteration < max_iterations and vertices_without_weights:
        propagated_this_iteration = 0
        remaining_vertices = set()

        # Process each unweighted vertex
        for vert_idx in vertices_without_weights:
            vert = bm.verts[vert_idx]
            # Get adjacent vertices
            neighbors = set()
            for edge in vert.link_edges:
                other = edge.other_vert(vert)
                if vertex_weights[other.index]:
                    neighbors.add(other)

            if neighbors:
                # Find the nearest vertex
                closest_vert = min(neighbors,
                                 key=lambda v: (v.co - vert.co).length)

                # Copy the weight
                vertex_weights[vert_idx] = vertex_weights[closest_vert.index].copy()
                temp_group.add([vert_idx], 1.0, 'REPLACE')  # Record propagation vertices
                propagated_this_iteration += 1
            else:
                remaining_vertices.add(vert_idx)

        if propagated_this_iteration == 0:
            break

        print(f"Iteration {iteration + 1}: Propagated weights to {propagated_this_iteration} vertices in {mesh_obj.name}")
        total_propagated += propagated_this_iteration
        vertices_without_weights = remaining_vertices
        iteration += 1

    # Assign the average weight to the remaining vertices without weights
    if vertices_without_weights:
        total_weights = {}
        weight_count = 0

        # First, calculate the average weight
        for vert_idx, weights in vertex_weights.items():
            if weights:
                weight_count += 1
                for group_name, weight in weights.items():
                    if group_name not in total_weights:
                        total_weights[group_name] = 0.0
                    total_weights[group_name] += weight

        if weight_count > 0:
            average_weights = {
                group_name: weight / weight_count
                for group_name, weight in total_weights.items()
            }

            # Apply the average weight to the remaining vertices
            num_averaged = len(vertices_without_weights)
            print(f"Applying average weights to remaining {num_averaged} vertices in {mesh_obj.name}")

            for vert_idx in vertices_without_weights:
                vertex_weights[vert_idx] = average_weights.copy()
                temp_group.add([vert_idx], 1.0, 'REPLACE')  # Record propagation vertices
            total_propagated += num_averaged

    # Apply new weight
    for vert_idx, weights in vertex_weights.items():
        for group_name, weight in weights.items():
            if group_name in mesh_obj.vertex_groups:
                mesh_obj.vertex_groups[group_name].add([vert_idx], weight, 'REPLACE')

    print(f"Total: Propagated weights to {total_propagated} vertices in {mesh_obj.name}")

    bm.free()
    return temp_group_name

def remove_propagated_weights(mesh_obj: bpy.types.Object, temp_group_name: str) -> None:
    """
    Remove propagated weights

    Parameters:
        mesh_obj: Mesh object
        temp_group_name: Name of the vertex group recording propagation vertices
    """
    # Confirm the existence of a temporary vertex group
    temp_group = mesh_obj.vertex_groups.get(temp_group_name)
    if not temp_group:
        return

    # Retrieve the armature from the Armature Modifier
    armature_obj = None
    for modifier in mesh_obj.modifiers:
        if modifier.type == 'ARMATURE':
            armature_obj = modifier.object
            break

    if not armature_obj:
        print(f"Warning: No armature modifier found in {mesh_obj.name}")
        return

    # Get all bone names of the armature
    deform_groups = {bone.name for bone in armature_obj.data.bones}

    # Delete the propagated vertex weights
    for vert in mesh_obj.data.vertices:
        # Get the weight of the temporary group
        weight = 0.0
        for g in vert.groups:
            if g.group == temp_group.index:
                weight = g.weight
                break

        # When the weight is greater than zero (for propagated vertices)
        if weight > 0:
            for group in mesh_obj.vertex_groups:
                try:
                    group.remove([vert.index])
                except RuntimeError:
                    continue

    # Temporarily delete the vertex group
    mesh_obj.vertex_groups.remove(temp_group)



def update_cloth_metadata(metadata_dict: dict, output_path: str, vertex_index_mapping: dict) -> None:
    """
    Update the vertex positions of ClothMetadata and save it to the specified path

    Parameters:
        metadata_dict: The original ClothMetadata dictionary
        output_path: Destination path
        vertex_index_mapping: Mapping from Unity vertex indices to Blender vertex indices
    """
    # Processing for each mesh
    for cloth_data in metadata_dict.get("clothMetadata", []):
        mesh_name = cloth_data["meshName"]
        mesh_obj = bpy.data.objects.get(mesh_name)

        if not mesh_obj or mesh_obj.type != 'MESH':
            print(f"Warning: Mesh {mesh_name} not found")
            continue

        # Retrieve the mapping information for this mesh
        mesh_mapping = vertex_index_mapping.get(mesh_name, {})
        if not mesh_mapping:
            print(f"Warning: No vertex mappings found for {mesh_name}")
            continue

        # Retrieve evaluated mesh (after applying modifiers)
        depsgraph = bpy.context.evaluated_depsgraph_get()
        evaluated_obj = mesh_obj.evaluated_get(depsgraph)
        evaluated_mesh = evaluated_obj.data

        # Update vertexData
        for i, data in enumerate(cloth_data.get("vertexData", [])):
            # Obtain Blender vertex indices corresponding to Unity vertex indices
            blender_vert_idx = mesh_mapping.get(i)

            if blender_vert_idx is not None and blender_vert_idx < len(evaluated_mesh.vertices):
                # Get world coordinates
                world_pos = evaluated_obj.matrix_world @ evaluated_mesh.vertices[blender_vert_idx].co

                # Converting from Blender Coordinate System to Unity Coordinate System
                data["position"]["x"] = -world_pos.x
                data["position"]["y"] = world_pos.z
                data["position"]["z"] = -world_pos.y
            else:
                print(f"Warning: No mapping found for Unity vertex {i} in {mesh_name}")

    # Save the updated data
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metadata_dict, f, indent=4)
        print(f"Updated cloth metadata saved to {output_path}")
    except Exception as e:
        print(f"Error saving cloth metadata: {e}")


def process_single_config(args, config_pair, pair_index, total_pairs, overall_start_time):
    try:
        import time
        start_time = time.time()

        use_subdivision = not args.no_subdivision
        if pair_index != 0:
            use_subdivision = False

        use_triangulation = not args.no_triangle

        bpy.ops.object.mode_set(mode='OBJECT')

        # Load base file
        print("Status: Loading base file")
        print(f"Progress: {(pair_index + 0.05) / total_pairs * 0.9:.3f}")
        load_base_file(args.base)
        base_load_time = time.time()
        print(f"Base file load time: {base_load_time - start_time:.2f} seconds")

        # Process base avatar
        print("Status: Base avatar processing in progress")
        print(f"Progress: {(pair_index + 0.1) / total_pairs * 0.9:.3f}")
        base_mesh, base_armature, base_avatar_data = process_base_avatar(
            config_pair['base_fbx'],
            config_pair['base_avatar_data']
        )

        # Process clothing avatar
        print("Status: Processing clothing data")
        print(f"Progress: {(pair_index + 0.15) / total_pairs * 0.9:.3f}")
        clothing_meshes, clothing_armature, clothing_avatar_data = process_clothing_avatar(
            config_pair['input_clothing_fbx_path'],
            config_pair['clothing_avatar_data'],
            config_pair['hips_position'],
            config_pair['target_meshes'],
            config_pair['mesh_renderers']
        )

        # Convert shape key names based on blend shape mapping
        if config_pair.get('blend_shape_mappings'):
            rename_shape_keys_from_mappings(clothing_meshes, config_pair['blend_shape_mappings'])

        clothing_process_time = time.time()
        print(f"Clothing data processing: {clothing_process_time - base_load_time:.2f} seconds")

        global _is_A_pose
        if pair_index == 0:
            _is_A_pose = is_A_pose(
                clothing_avatar_data,
                clothing_armature,
                init_pose_filepath=config_pair['init_pose'],
                pose_filepath=config_pair['pose_data'],
                clothing_avatar_data_filepath=config_pair['clothing_avatar_data']
            )
            print(f"is_A_pose: {_is_A_pose}")
        if _is_A_pose and base_avatar_data and base_avatar_data.get('basePoseA', None):
            print("Use the base pose for A pose for A pose")
            base_avatar_data['basePose'] = base_avatar_data['basePoseA']

        base_pose_filepath = base_avatar_data.get('basePose', None)
        if base_pose_filepath and config_pair.get('do_not_use_base_pose', 0) == 0:
            pose_dir = os.path.dirname(os.path.abspath(config_pair['base_avatar_data']))
            base_pose_filepath = os.path.join(pose_dir, base_pose_filepath)
            print(f"Applying target avatar base pose from {base_pose_filepath}")
            add_pose_from_json(base_armature, base_pose_filepath, base_avatar_data, invert=False)
        base_process_time = time.time()
        print(f"Base Avatar Processing: {base_process_time - clothing_process_time:.2f} seconds")

        print("Status: Cross metadata loading")
        print(f"Progress: {(pair_index + 0.2) / total_pairs * 0.9:.3f}")
        cloth_metadata, vertex_index_mapping = load_cloth_metadata(args.cloth_metadata)
        metadata_load_time = time.time()
        print(f"Cross metadata load: {metadata_load_time - base_process_time:.2f} seconds")

        # Load mesh material data (first time only)
        if pair_index == 0:
            print("Status: Loading mesh material data")
            print(f"Progress: {(pair_index + 0.22) / total_pairs * 0.9:.3f}")
            load_mesh_material_data(args.mesh_material_data)
            material_load_time = time.time()
            print(f"Mesh material data loading: {material_load_time - metadata_load_time:.2f} seconds")
        else:
            material_load_time = metadata_load_time

        # Setup weight transfer
        print("Status: Weight transfer setup in progress")
        print(f"Progress: {(pair_index + 0.25) / total_pairs * 0.9:.3f}")
        setup_weight_transfer()
        setup_time = time.time()
        print(f"Weight Transfer Setup: {setup_time - metadata_load_time:.2f} seconds")

        print("Status: Base Avatar Weight Update in Progress")
        print(f"Progress: {(pair_index + 0.3) / total_pairs * 0.9:.3f}")
        remove_empty_vertex_groups(base_mesh)

        # Apply bone name conversion if provided
        if hasattr(args, 'name_conv') and args.name_conv:
            try:
                with open(args.name_conv, 'r', encoding='utf-8') as f:
                    name_conv_data = json.load(f)
                apply_bone_name_conversion(clothing_armature, clothing_meshes, name_conv_data)
                print(f"Bone name conversion complete: {args.name_conv}")
            except Exception as e:
                print(f"Warning: An error occurred during the bone renaming process: {e}")

        # Normalize clothing bone names before weight updates
        normalize_clothing_bone_names(clothing_armature, clothing_avatar_data, clothing_meshes)
        update_base_avatar_weights(base_mesh, clothing_armature, base_avatar_data, clothing_avatar_data, preserve_optional_humanoid_bones=True)
        normalize_bone_weights(base_mesh, base_avatar_data)
        base_weights_time = time.time()
        print(f"Base Avatar Weight Update: {base_weights_time - setup_time:.2f} seconds")

        # When converting from a Template, create the crotch vertex group here
        if clothing_avatar_data.get("name", None) == "Template":
            print("Conversion from Template Create crotch vertex group")
            current_active_object = bpy.context.view_layer.objects.active
            template_fbx_path = clothing_avatar_data.get("defaultFBXPath", None)
            clothing_avatar_data_path = config_pair['clothing_avatar_data']
            # Convert template_fbx_path to an absolute path
            if template_fbx_path and not os.path.isabs(template_fbx_path):
                # Get the root directory of a relative path
                relative_parts = template_fbx_path.split(os.sep)
                if relative_parts:
                    top_dir = relative_parts[0]

                    # Search for the corresponding directory from the end of the clothing_avatar_data_path
                    clothing_path_parts = clothing_avatar_data_path.split(os.sep)
                    found_index = -1

                    # Search from the end
                    for i in range(len(clothing_path_parts) - 1, -1, -1):
                        if clothing_path_parts[i] == top_dir:
                            found_index = i
                            break

                    if found_index != -1:
                        # If found, construct the absolute path
                        base_path = os.sep.join(clothing_path_parts[:found_index])
                        template_fbx_path = os.path.join(base_path, template_fbx_path)
                        template_fbx_path = os.path.normpath(template_fbx_path)
            print(f"template_fbx_path: {template_fbx_path}")
            import_base_fbx(template_fbx_path)
            template_obj = bpy.data.objects.get(clothing_avatar_data.get("meshName", None))
            template_armature = None
            for modifier in template_obj.modifiers:
                if modifier.type == 'ARMATURE':
                    template_armature = modifier.object
                    break
            if template_armature is None:
                print("Warning: Armature modifier not found")
                return
            # select_vertices_by_conditions(template_obj, "MF_crotch", clothing_avatar_data, radius=0.075, max_angle_degrees=45.0)
            # for obj in clothing_meshes:
            #     find_vertices_near_faces(template_obj, obj, "MF_crotch", 0.01)
            crotch_vertex_group_filepath = os.path.join(os.path.dirname(template_fbx_path), "vertex_group_weights_crotch.json")
            crotch_group_name = load_vertex_group(template_obj, crotch_vertex_group_filepath)
            if crotch_group_name:
                # Apply Y-axis rotation to the LeftUpperLeg and RightUpperLeg bones
                print("  Apply Y-axis rotation to the LeftUpperLeg and RightUpperLeg bones")
                bpy.context.view_layer.objects.active = template_armature
                bpy.ops.object.mode_set(mode='POSE')

                # Get the boneName for LeftUpperLeg and RightUpperLeg from humanoidBones
                left_upper_leg_bone = None
                right_upper_leg_bone = None

                for bone_map in clothing_avatar_data.get("humanoidBones", []):
                    if bone_map.get("humanoidBoneName") == "LeftUpperLeg":
                        left_upper_leg_bone = bone_map.get("boneName")
                    elif bone_map.get("humanoidBoneName") == "RightUpperLeg":
                        right_upper_leg_bone = bone_map.get("boneName")

                # Apply a -45 degree Y-axis rotation to the LeftUpperLeg bone
                if left_upper_leg_bone and left_upper_leg_bone in template_armature.pose.bones:
                    bone = template_armature.pose.bones[left_upper_leg_bone]
                    current_world_matrix = template_armature.matrix_world @ bone.matrix
                    # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = template_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(-40), 4, 'Y')
                    bone.matrix = template_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                # Apply a 45-degree rotation around the Y-axis to the RightUpperLeg bone
                if right_upper_leg_bone and right_upper_leg_bone in template_armature.pose.bones:
                    bone = template_armature.pose.bones[right_upper_leg_bone]
                    current_world_matrix = template_armature.matrix_world @ bone.matrix
                    # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = template_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(40), 4, 'Y')
                    bone.matrix = template_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                if left_upper_leg_bone and left_upper_leg_bone in clothing_armature.pose.bones:
                    bone = clothing_armature.pose.bones[left_upper_leg_bone]
                    current_world_matrix = clothing_armature.matrix_world @ bone.matrix
                    # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = clothing_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(-40), 4, 'Y')
                    bone.matrix = clothing_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                if right_upper_leg_bone and right_upper_leg_bone in clothing_armature.pose.bones:
                    bone = clothing_armature.pose.bones[right_upper_leg_bone]
                    current_world_matrix = clothing_armature.matrix_world @ bone.matrix
                    # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = clothing_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(40), 4, 'Y')
                    bone.matrix = clothing_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                bpy.ops.object.mode_set(mode='OBJECT')
                bpy.context.view_layer.update()

                for obj in clothing_meshes:
                    #transfer_weights_from_nearest_vertex(template_obj, obj, crotch_group_name)
                    find_vertices_near_faces(template_obj, obj, crotch_group_name, 0.01, use_all_faces=True, smooth_repeat=3)

                # Apply a -45 degree Y-axis rotation to the LeftUpperLeg bone
                if left_upper_leg_bone and left_upper_leg_bone in template_armature.pose.bones:
                    bone = template_armature.pose.bones[left_upper_leg_bone]
                    current_world_matrix = template_armature.matrix_world @ bone.matrix
                    # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = template_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(40), 4, 'Y')
                    bone.matrix = template_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                # Apply a 45-degree rotation around the Y-axis to the RightUpperLeg bone
                if right_upper_leg_bone and right_upper_leg_bone in template_armature.pose.bones:
                    bone = template_armature.pose.bones[right_upper_leg_bone]
                    current_world_matrix = template_armature.matrix_world @ bone.matrix
                    # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = template_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(-40), 4, 'Y')
                    bone.matrix = template_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                if left_upper_leg_bone and left_upper_leg_bone in clothing_armature.pose.bones:
                    bone = clothing_armature.pose.bones[left_upper_leg_bone]
                    current_world_matrix = clothing_armature.matrix_world @ bone.matrix
                    # Apply a -45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = clothing_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(40), 4, 'Y')
                    bone.matrix = clothing_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                if right_upper_leg_bone and right_upper_leg_bone in clothing_armature.pose.bones:
                    bone = clothing_armature.pose.bones[right_upper_leg_bone]
                    current_world_matrix = clothing_armature.matrix_world @ bone.matrix
                    # Apply a 45-degree rotation around the Y-axis in the global coordinate system
                    head_world_transformed = clothing_armature.matrix_world @ bone.head
                    offset_matrix = mathutils.Matrix.Translation(head_world_transformed * -1.0)
                    rotation_matrix = mathutils.Matrix.Rotation(math.radians(-40), 4, 'Y')
                    bone.matrix = clothing_armature.matrix_world.inverted() @ offset_matrix.inverted() @ rotation_matrix @ offset_matrix @ current_world_matrix

                bpy.context.view_layer.update()

            blur_vertex_group_filepath = os.path.join(os.path.dirname(template_fbx_path), "vertex_group_weights_blur.json")
            blur_group_name = load_vertex_group(template_obj, blur_vertex_group_filepath)
            if blur_group_name:
                for obj in clothing_meshes:
                    transfer_weights_from_nearest_vertex(template_obj, obj, blur_group_name)
            inpaint_vertex_group_filepath = os.path.join(os.path.dirname(template_fbx_path), "vertex_group_weights_inpaint.json")
            inpaint_group_name = load_vertex_group(template_obj, inpaint_vertex_group_filepath)
            if inpaint_group_name:
                for obj in clothing_meshes:
                    transfer_weights_from_nearest_vertex(template_obj, obj, inpaint_group_name)
            bpy.data.objects.remove(bpy.data.objects["Body.Template"], do_unlink=True)
            bpy.data.objects.remove(bpy.data.objects["Body.Template.Eyes"], do_unlink=True)
            bpy.data.objects.remove(bpy.data.objects["Body.Template.Head"], do_unlink=True)
            bpy.data.objects.remove(bpy.data.objects["Armature.Template"], do_unlink=True)
            print("Conversion from Template Creation of crotch vertex group complete")
            bpy.context.view_layer.objects.active = current_active_object

        # Apply BlendShape Deformation Fields before pose application
        print("Status: Deformation Field applied for BlendShape")
        print(f"Progress: {(pair_index + 0.33) / total_pairs * 0.9:.3f}")
        blend_shape_labels = config_pair['blend_shapes'].split(',') if config_pair['blend_shapes'] else None
        if blend_shape_labels:
            for obj in clothing_meshes:
                reset_shape_keys(obj)
                remove_empty_vertex_groups(obj)
                normalize_vertex_weights(obj)
                apply_blendshape_deformation_fields(obj, config_pair['field_data'], blend_shape_labels, clothing_avatar_data, config_pair['blend_shape_values'])
        blendshape_time = time.time()
        print(f"Deformation Field for BlendShape applied: {blendshape_time - base_weights_time:.2f} seconds")

        # Apply pose from JSON
        print("Status: Pose applied")
        print(f"Progress: {(pair_index + 0.35) / total_pairs * 0.9:.3f}")
        add_clothing_pose_from_json(clothing_armature, config_pair['pose_data'], config_pair['init_pose'], config_pair['clothing_avatar_data'], config_pair['base_avatar_data'])
        pose_time = time.time()
        print(f"Pose applied: {pose_time - blendshape_time:.2f} seconds")

        print("Status: Duplicate vertex attribute configuration in progress")
        print(f"Progress: {(pair_index + 0.4) / total_pairs * 0.9:.3f}")
        create_overlapping_vertices_attributes(clothing_meshes, base_avatar_data)
        vertices_attributes_time = time.time()
        print(f"Duplicate Vertex Attribute Settings: {vertices_attributes_time - pose_time:.2f} seconds")

        for obj in clothing_meshes:
            create_hinge_bone_group(obj, clothing_armature, clothing_avatar_data)

        # Process each mesh object with armature modifier
        print("Status: Mesh deformation processing in progress")
        print(f"Progress: {(pair_index + 0.45) / total_pairs * 0.9:.3f}")
        propagated_groups_map = {}  # Record the propagation log group name for each mesh
        field_distance_groups = {}  # Record the field distance vertex groups for each mesh
        cycle1_start = time.time()
        for obj in clothing_meshes:
            obj_start = time.time()
            print("cycle1 " + obj.name)

            reset_shape_keys(obj)
            remove_empty_vertex_groups(obj)
            normalize_vertex_weights(obj)
            merge_auxiliary_to_humanoid_weights(obj, clothing_avatar_data)

            # Perform weight propagation
            temp_group_name = propagate_bone_weights(obj)
            if temp_group_name:  # Record only when propagation is executed
                propagated_groups_map[obj.name] = temp_group_name

            # Exclude minor weights
            cleanup_weights_time_start = time.time()
            for vert in obj.data.vertices:
                groups_to_remove = []
                for g in vert.groups:
                    if g.weight < 0.0005:
                        groups_to_remove.append(g.group)

                # Remove the vertex from the group with the smallest weight
                for group_idx in groups_to_remove:
                    try:
                        obj.vertex_groups[group_idx].remove([vert.index])
                    except RuntimeError:
                        continue
            cleanup_weights_time = time.time() - cleanup_weights_time_start
            print(f"  Minor weight exclusion: {cleanup_weights_time:.2f} seconds")

            create_deformation_mask(obj, clothing_avatar_data)

            if pair_index == 0 and use_subdivision and obj.name not in cloth_metadata:
                subdivide_long_edges(obj)
                subdivide_breast_faces(obj, clothing_avatar_data)

            if use_triangulation and not use_subdivision and obj.name not in cloth_metadata and pair_index == total_pairs - 1:
                triangulate_mesh(obj)

            # Record vertex weights and merge bone weights
            original_weights = save_vertex_weights(obj)

            # Integrated Processing of Bone Weight
            process_bone_weight_consolidation(obj, clothing_avatar_data)

            process_mesh_with_connected_components_inline(
                obj,
                config_pair['field_data'],
                blend_shape_labels,
                clothing_avatar_data,
                base_avatar_data,
                clothing_armature,
                cloth_metadata,
                subdivision=use_subdivision,
                skip_blend_shape_generation=config_pair['skip_blend_shape_generation'],
                config_data=config_pair['config_data']
            )

            # At iteration end: Restore to original weight state
            restore_vertex_weights(obj, original_weights)

            if obj.data.shape_keys:
                # Processing Shape Keys with the _generated Suffix
                generated_shape_keys = []
                for shape_key in obj.data.shape_keys.key_blocks:
                    if shape_key.name.endswith("_generated"):
                        generated_shape_keys.append(shape_key.name)

                # Integrate _generated shape keys into corresponding base shape keys
                for generated_name in generated_shape_keys:
                    base_name = generated_name[:-10]  # Remove "_generated"

                    generated_key = obj.data.shape_keys.key_blocks.get(generated_name)
                    base_key = obj.data.shape_keys.key_blocks.get(base_name)

                    if generated_key and base_key:
                        # Overwrite the base shape key with the contents of the generated shape key
                        for i, point in enumerate(generated_key.data):
                            base_key.data[i].co = point.co
                        print(f"Merged {generated_name} into {base_name} for {obj.name}")

                        # Delete generated shape keys
                        obj.shape_key_remove(generated_key)
                        print(f"Removed generated shape key: {generated_name} from {obj.name}")

            print(f"  Processing {obj.name}: {time.time() - obj_start:.2f} seconds")

        cycle1_end = time.time()
        print(f"Entire Cycle 1: {cycle1_end - cycle1_start:.2f} seconds")

        for obj in clothing_meshes:
            if obj.data.shape_keys:
                for key_block in obj.data.shape_keys.key_blocks:
                    print(f"Shape key: {key_block.name} / {key_block.value} found on {obj.name}")

        right_base_mesh, left_base_mesh = duplicate_mesh_with_partial_weights(base_mesh, base_avatar_data)
        duplicate_time = time.time()
        print(f"Base mesh duplication: {duplicate_time - cycle1_end:.2f} seconds")

        # First, examine the inclusion relationship
        print("Status: Detecting mesh containment relationships")
        print(f"Progress: {(pair_index + 0.5) / total_pairs * 0.9:.3f}")
        containing_objects = find_containing_objects(clothing_meshes, threshold=0.04)
        print(f"Found {sum(len(contained) for contained in containing_objects.values())} objects that are contained within others")
        containing_time = time.time()
        print(f"Containment detection: {containing_time - duplicate_time:.2f} seconds")

        # Tracking processed objects (weight transfer processing only)
        weight_transfer_processed = set()
        armature_settings_dict = {}

        # Cycle2's individual processing section
        print("Status: Cycle 2 Preprocessing in Progress")
        print(f"Progress: {(pair_index + 0.55) / total_pairs * 0.9:.3f}")
        cycle2_pre_start = time.time()

        # Retrieve subHumanoidBones and subAuxiliaryBones from the avatar data and overwrite the current data
        # Before that, save the data before the change
        original_humanoid_bones = None
        original_auxiliary_bones = None

        if base_avatar_data.get('subHumanoidBones') or base_avatar_data.get('subAuxiliaryBones'):
            print("Applying subHumanoidBones and subAuxiliaryBones...")

            # Back up the original data
            original_humanoid_bones = base_avatar_data.get('humanoidBones', []).copy() if base_avatar_data.get('humanoidBones') else []
            original_auxiliary_bones = base_avatar_data.get('auxiliaryBones', []).copy() if base_avatar_data.get('auxiliaryBones') else []

            # Overwrite with subHumanoidBones
            if base_avatar_data.get('subHumanoidBones'):
                # Search for the same humanoidBoneName in the current humanoidBones and overwrite it
                sub_humanoid_bones = base_avatar_data['subHumanoidBones']
                humanoid_bones = base_avatar_data.get('humanoidBones', [])

                for sub_bone in sub_humanoid_bones:
                    sub_humanoid_name = sub_bone.get('humanoidBoneName')
                    if sub_humanoid_name:
                        # Find existing humanoidBones with the same humanoidBoneName
                        for i, existing_bone in enumerate(humanoid_bones):
                            if existing_bone.get('humanoidBoneName') == sub_humanoid_name:
                                humanoid_bones[i] = sub_bone.copy()
                                break
                        else:
                            # If not found, add
                            humanoid_bones.append(sub_bone.copy())

            # Overwrite with subAuxiliaryBones
            if base_avatar_data.get('subAuxiliaryBones'):
                # Search for the bone with the same humanoidBoneName as the current auxiliaryBones and overwrite it
                sub_auxiliary_bones = base_avatar_data['subAuxiliaryBones']
                auxiliary_bones = base_avatar_data.get('auxiliaryBones', [])

                for sub_aux in sub_auxiliary_bones:
                    sub_humanoid_name = sub_aux.get('humanoidBoneName')
                    if sub_humanoid_name:
                        # Find the bone with the same humanoidBoneName among existing auxiliaryBones
                        for i, existing_aux in enumerate(auxiliary_bones):
                            if existing_aux.get('humanoidBoneName') == sub_humanoid_name:
                                auxiliary_bones[i] = sub_aux.copy()
                                break
                        else:
                            # If not found, add
                            auxiliary_bones.append(sub_aux.copy())

            print("Application of subHumanoidBones and subAuxiliaryBones complete")

        # if clothing_avatar_data.get("name", None) != "Template":
        #     select_vertices_by_conditions(base_mesh, "MF_crotch", base_avatar_data, radius=0.075, max_angle_degrees=45.0)

        if base_avatar_data.get("name", None) == "Template" and _is_A_pose and base_avatar_data.get('basePoseA', None):
            armpit_vertex_group_filepath2 = os.path.join(os.path.dirname(config_pair['base_fbx']), "vertex_group_weights_armpit.json")
            armpit_group_name2 = load_vertex_group(base_mesh, armpit_vertex_group_filepath2)
            if armpit_group_name2:
                for obj in clothing_meshes:
                    find_vertices_near_faces(base_mesh, obj, armpit_group_name2, 0.1, 45.0)
        if base_avatar_data.get("name", None) == "Template":
            crotch_vertex_group_filepath2 = os.path.join(os.path.dirname(config_pair['base_fbx']), "vertex_group_weights_crotch2.json")
            crotch_group_name2 = load_vertex_group(base_mesh, crotch_vertex_group_filepath2)
            if crotch_group_name2:
                for obj in clothing_meshes:
                    # transfer_weights_from_nearest_vertex(base_mesh, obj, crotch_group_name2)
                    find_vertices_near_faces(base_mesh, obj, crotch_group_name2, 0.01, smooth_repeat=3)
            blur_vertex_group_filepath2 = os.path.join(os.path.dirname(config_pair['base_fbx']), "vertex_group_weights_blur.json")
            blur_group_name2 = load_vertex_group(base_mesh, blur_vertex_group_filepath2)
            if blur_group_name2:
                for obj in clothing_meshes:
                    transfer_weights_from_nearest_vertex(base_mesh, obj, blur_group_name2)
            inpaint_vertex_group_filepath2 = os.path.join(os.path.dirname(config_pair['base_fbx']), "vertex_group_weights_inpaint.json")
            inpaint_group_name2 = load_vertex_group(base_mesh, inpaint_vertex_group_filepath2)
            if inpaint_group_name2  :
                for obj in clothing_meshes:
                    transfer_weights_from_nearest_vertex(base_mesh, obj, inpaint_group_name2)

        for obj in clothing_meshes:
            obj_start = time.time()
            print("cycle2 (pre-weight transfer) " + obj.name)

            # Store armature modifier settings
            armature_settings = store_armature_modifier_settings(obj)
            armature_settings_dict[obj] = armature_settings

            # Apply modifiers and process humanoid vertex groups (these are applied separately)
            #apply_modifiers_keep_shapekeys_with_temp(obj)
            generate_temp_shapekeys_for_weight_transfer(obj, clothing_armature, clothing_avatar_data, _is_A_pose)
            process_missing_bone_weights(obj, base_armature, clothing_avatar_data, base_avatar_data, preserve_optional_humanoid_bones=False)
            process_humanoid_vertex_groups(obj, clothing_armature, base_avatar_data, clothing_avatar_data)
            # if clothing_avatar_data.get("name", None) != "Template":
            #     find_vertices_near_faces(base_mesh, obj, "MF_crotch", 0.01)
            restore_armature_modifier(obj, armature_settings_dict[obj])
            set_armature_modifier_visibility(obj, False, False)
            set_armature_modifier_target_armature(obj, base_armature)
            print(f"  {obj.name} preprocessing: {time.time() - obj_start:.2f} seconds")

        cycle2_pre_end = time.time()
        print(f"Cycle 2 preprocessing total: {cycle2_pre_end - cycle2_pre_start:.2f} seconds")

        # Weight Transfer Processing (Considering Inclusion Relationships)
        print("Status: Cycle 2 Weight Transfer in Progress")
        print(f"Progress: {(pair_index + 0.6) / total_pairs * 0.9:.3f}")
        weight_transfer_start = time.time()
        for obj in clothing_meshes:
            if obj in weight_transfer_processed:
                continue

            obj_start = time.time()
            # If this object contains other objects
            if obj in containing_objects and containing_objects[obj]:
                contained_objects = containing_objects[obj]
                print(f"{obj.name} contains {contained_objects} other objects within distance 0.02 - applying joint weight transfer")

                # Temporarily combine to apply only weight transfer processing
                temporarily_merge_for_weight_transfer(
                    obj,
                    contained_objects,
                    base_armature,
                    base_avatar_data,
                    clothing_avatar_data,
                    config_pair['field_data'],
                    clothing_armature,
                    config_pair.get('next_blendshape_settings', []),
                    cloth_metadata
                )

                # Mark as processed
                weight_transfer_processed.add(obj)
                weight_transfer_processed.update(contained_objects)
            print(f"  {obj.name} Inclusion Weight Transfer: {time.time() - obj_start:.2f} seconds")

        for obj in clothing_meshes:
            if obj in weight_transfer_processed:
                continue
            # If there is no inclusion relationship, perform the standard weight transfer process
            obj_start = time.time()
            print(f"Applying individual weight transfer to {obj.name}")
            # Weight transfer
            # process_weight_transfer(obj, base_armature, base_avatar_data, config_pair['field_data'], clothing_armature, cloth_metadata)
            process_weight_transfer_with_component_normalization(obj, base_armature, base_avatar_data, clothing_avatar_data, config_pair['field_data'], clothing_armature, config_pair.get('next_blendshape_settings', []), cloth_metadata)

            # Mark as processed
            weight_transfer_processed.add(obj)
            print(f"  {obj.name}'s individual weight transfer: {time.time() - obj_start:.2f} seconds")

        # Process to align the weights of overlapping vertices
        normalize_overlapping_vertices_weights(clothing_meshes, base_avatar_data)

        weight_transfer_end = time.time()
        print(f"Overall weight transfer processing: {weight_transfer_end - weight_transfer_start:.2f} seconds")

        print("Status: Cycle 2 Post-Processing")
        print(f"Progress: {(pair_index + 0.65) / total_pairs * 0.9:.3f}")
        cycle2_post_start = time.time()
        for obj in clothing_meshes:
            obj_start = time.time()
            print("cycle2 (post-weight transfer) " + obj.name)
            set_armature_modifier_visibility(obj, True, True)
            set_armature_modifier_target_armature(obj, clothing_armature)
            print(f"  Post-processing for {obj.name}: {time.time() - obj_start:.2f} seconds")

        cycle2_post_end = time.time()
        print(f"Cycle 2 post-processing total: {cycle2_post_end - cycle2_post_start:.2f} seconds")

        print("Status: Pose applied")
        print(f"Progress: {(pair_index + 0.7) / total_pairs * 0.9:.3f}")
        apply_pose_as_rest(clothing_armature)
        pose_rest_time = time.time()
        print(f"Apply pose as rest pose: {pose_rest_time - cycle2_post_end:.2f} seconds")

        print("Status: Bonefield Delta in progress")
        print(f"Progress: {(pair_index + 0.75) / total_pairs * 0.9:.3f}")
        apply_bone_field_delta(clothing_armature, config_pair['field_data'], clothing_avatar_data)
        bone_delta_time = time.time()
        print(f"Bone field delta applied: {bone_delta_time - pose_rest_time:.2f} seconds")

        print("Status: Pose applied")
        print(f"Progress: {(pair_index + 0.85) / total_pairs * 0.9:.3f}")
        apply_pose_as_rest(clothing_armature)
        second_pose_rest_time = time.time()
        print(f"Apply the second pose as a rest pose: {second_pose_rest_time - bone_delta_time:.2f} seconds")

        print("Status: All conversions are being applied")
        print(f"Progress: {(pair_index + 0.9) / total_pairs * 0.9:.3f}")
        apply_all_transforms()
        transforms_time = time.time()
        print(f"Apply all transformations: {transforms_time - second_pose_rest_time:.2f} seconds")

        # Remove propagated weights
        print("Status: Propagation weight removal in progress")
        print(f"Progress: {(pair_index + 0.95) / total_pairs * 0.9:.3f}")
        propagated_start = time.time()
        for obj in clothing_meshes:
            if obj.name in propagated_groups_map:
                remove_propagated_weights(obj, propagated_groups_map[obj.name])
        propagated_end = time.time()
        print(f"Propagation weight removal: {propagated_end - propagated_start:.2f} seconds")

        # If subHumanoidBones and subAuxiliaryBones were applied, restore the original data
        if original_humanoid_bones is not None or original_auxiliary_bones is not None:
            print("Restoring original humanoidBones and auxiliaryBones...")
            if original_humanoid_bones is not None:
                base_avatar_data['humanoidBones'] = original_humanoid_bones
            if original_auxiliary_bones is not None:
                base_avatar_data['auxiliaryBones'] = original_auxiliary_bones
            print("Restoration of original bone data complete")

        print("Status: Replacing humanoid bones")
        print(f"Progress: {(pair_index + 0.95) / total_pairs * 0.9:.3f}")
        base_pose_filepath = None
        if config_pair.get('do_not_use_base_pose', 0) == 0:
            base_pose_filepath = base_avatar_data.get('basePose', None)
            if base_pose_filepath:
                pose_dir = os.path.dirname(os.path.abspath(config_pair['base_avatar_data']))
                base_pose_filepath = os.path.join(pose_dir, base_pose_filepath)
        if pair_index == 0:
            replace_humanoid_bones(base_armature, clothing_armature, base_avatar_data, clothing_avatar_data, True, base_pose_filepath, clothing_meshes, False)
        else:
            replace_humanoid_bones(base_armature, clothing_armature, base_avatar_data, clothing_avatar_data, False, base_pose_filepath, clothing_meshes, True)
        bones_replace_time = time.time()
        print(f"Humanoid Bone Replacement: {bones_replace_time - propagated_end:.2f} seconds")

        # Blend shape settings based on clothingBlendShapeSettings in the Config file
        print("Status: Blend Shape Configuration in Progress")
        print(f"Progress: {(pair_index + 0.96) / total_pairs * 0.9:.3f}")
        blendshape_start = time.time()
        if "clothingBlendShapeSettings" in config_pair['config_data']:
            blend_shape_settings = config_pair['config_data']["clothingBlendShapeSettings"]

            for setting in blend_shape_settings:
                label = setting.get("label")
                if label in blend_shape_labels:
                    blendshapes = setting.get("blendshapes", [])
                    for bs in blendshapes:
                        shape_key_name = bs.get("name")
                        value = bs.get("value", 0)
                        for obj in clothing_meshes:
                            if obj.data.shape_keys and shape_key_name in obj.data.shape_keys.key_blocks:
                                obj.data.shape_keys.key_blocks[shape_key_name].value = value / 100.0
                                print(f"Set blendshape '{shape_key_name}' on {obj.name} to {value/100.0}")
        blendshape_end = time.time()
        print(f"Blend Shape Setting: {blendshape_end - blendshape_start:.2f} seconds")

        print("Status: Cross-metadata update in progress")
        print(f"Progress: {(pair_index + 0.97) / total_pairs * 0.9:.3f}")
        metadata_update_start = time.time()
        if args.cloth_metadata and os.path.exists(args.cloth_metadata):
            try:
                # Load ClothMetadata
                with open(args.cloth_metadata, 'r', encoding='utf-8') as f:
                    metadata_dict = json.load(f)
                # Update and save ClothMetadata
                update_cloth_metadata(metadata_dict, args.cloth_metadata, vertex_index_mapping)

            except Exception as e:
                print(f"Error processing cloth metadata: {e}")
                import traceback
                traceback.print_exc()
        metadata_update_end = time.time()
        print(f"Cross-metadata update: {metadata_update_end - metadata_update_start:.2f} seconds")

        # FBX Export Preprocessing
        print("Status: Preparing for FBX export")
        print(f"Progress: {(pair_index + 0.975) / total_pairs * 0.9:.3f}")
        preprocess_start = time.time()

        # Retrieving blend_shape_labels
        blend_shape_labels = []
        if args.blend_shapes:
            blend_shape_labels = [label for label in args.blend_shapes.split(',')]

        for obj in clothing_meshes:
            if obj.data.shape_keys:
                for key_block in obj.data.shape_keys.key_blocks:
                    print(f"Shape key: {key_block.name} / {key_block.value} found on {obj.name}")

        # Delete shape keys created by apply_blendshape_deformation_fields
        merge_and_clean_generated_shapekeys(clothing_meshes, blend_shape_labels)
        if clothing_avatar_data.get("name", None) == "Template":
            import re
            pattern = re.compile(r'___\d+$')
            for obj in clothing_meshes:
                if obj.data.shape_keys:
                    keys_to_remove = []
                    for key_block in obj.data.shape_keys.key_blocks:
                        if pattern.search(key_block.name):
                            keys_to_remove.append(key_block.name)
                    for key_name in keys_to_remove:
                        key_block = obj.data.shape_keys.key_blocks.get(key_name)
                        if key_block:
                            obj.shape_key_remove(key_block)
                            print(f"Removed shape key: {key_name} from {obj.name}")

        if pair_index > 0:
            bpy.ops.object.mode_set(mode='OBJECT')
            clothing_blend_shape_labels = []
            for blend_shape_field in clothing_avatar_data['blendShapeFields']:
                clothing_blend_shape_labels.append(blend_shape_field['label'])
            base_blend_shape_labels = []
            for blend_shape_field in base_avatar_data['blendShapeFields']:
                base_blend_shape_labels.append(blend_shape_field['label'])
            for obj in clothing_meshes:
                if obj.data.shape_keys:
                    for key_block in obj.data.shape_keys.key_blocks:
                        if key_block.name in clothing_blend_shape_labels and key_block.name not in base_blend_shape_labels:
                            prev_shape_key = obj.data.shape_keys.key_blocks.get(key_block.name)
                            obj.shape_key_remove(prev_shape_key)
                            print(f"Removed shape key: {key_block.name} from {obj.name}")

        # Set the Highheel shape key value to 1
        set_highheel_shapekey_values(clothing_meshes, blend_shape_labels, base_avatar_data)

        # Export armature bone data to JSON before FBX export
        # print(f"Status: Exporting bone information JSON")
        # json_output_path = args.output.rsplit('.', 1)[0] + '_bone_data.json'
        # bpy.context.view_layer.update()
        # export_armature_bone_data_to_json(clothing_armature, json_output_path)

        preprocess_end = time.time()
        print(f"FBX export preprocessing: {preprocess_end - preprocess_start:.2f} seconds")

        # Select only imported objects for export
        bpy.ops.object.select_all(action='DESELECT')
        for obj in bpy.data.objects:
            if obj.name not in ["Body.BaseAvatar", "Armature.BaseAvatar", "Body.BaseAvatar.RightOnly", "Body.BaseAvatar.LeftOnly"]:
                obj.select_set(True)

        round_bone_coordinates(clothing_armature, decimal_places=6)

        # Export as FBX
        print("Status: Exporting FBX")
        print(f"Progress: {(pair_index + 0.98) / total_pairs * 0.9:.3f}")
        export_start = time.time()
        export_fbx(args.output)
        export_end = time.time()
        print(f"FBX Export: {export_end - export_start:.2f} seconds")

        # Save the current scene
        # if pair_index == 0:
        #     save_start = time.time()
        #     output_blend = args.output.rsplit('.', 1)[0] + '.blend'
        #     bpy.ops.wm.save_as_mainfile(filepath=output_blend)
        #     save_end = time.time()
        #     print(f"Blend file save: {save_end - save_start:.2f} seconds")

        total_time = time.time() - start_time
        print(f"Progress: {(pair_index + 1.0) / total_pairs * 0.9:.3f}")
        print(f"Processing complete: Total {total_time:.2f} seconds")
        return True

    except Exception as e:
        import traceback
        print("============= Error Details =============")
        print(f"Error message: {str(e)}")
        print("\n============= Full Stack Trace =============")
        print(traceback.format_exc())
        print("==========================================")

        output_blend = args.output.rsplit('.', 1)[0] + '.blend'
        bpy.ops.wm.save_as_mainfile(filepath=output_blend)

        return False

def main():
    try:
        import time
        start_time = time.time()

        sys.stdout.reconfigure(line_buffering=True)

        print("Status: Add-on activation in progress")
        print("Progress: 0.01")
        bpy.ops.preferences.addon_enable(module='robust-weight-transfer')
        print(f"Addon enabled: {time.time() - start_time:.2f} seconds")

        # Parse command line arguments
        print("Status: Parsing arguments")
        print("Progress: 0.02")
        args = parse_args()
        parse_time = time.time()
        print(f"Argument parsing: {parse_time - start_time:.2f} seconds")

        # Process each config pair
        total_pairs = len(args.config_pairs)
        successful_pairs = 0

        for pair_index, config_pair in enumerate(args.config_pairs):
            try:
                print(f"\n{'='*60}")
                print(f"Processing Start: Pair {pair_index + 1}/{total_pairs}")
                print(f"Base FBX: {config_pair['base_fbx']}")
                print(f"Config: {config_pair['config_path']}")
                print(f"{'='*60}")

                # Create output filename with index for multiple pairs
                # if total_pairs > 1:
                #     base_output = args.output.rsplit('.', 1)[0]
                #     extension = args.output.rsplit('.', 1)[1] if '.' in args.output else 'fbx'
                #     output_file = f"{base_output}_{pair_index + 1:03d}.{extension}"
                # else:
                #     output_file = args.output
                output_file = args.output

                # Create a copy of args with updated output path
                pair_args = argparse.Namespace(**vars(args))
                pair_args.output = output_file

                success = process_single_config(pair_args, config_pair, pair_index, total_pairs, start_time)
                if success:
                    successful_pairs += 1
                    print(f"✓ Pair {pair_index + 1} completed successfully: {output_file}")
                else:
                    print(f"✗ Pair {pair_index + 1} processing failed")
                    break
            except Exception as e:
                import traceback
                print(f"✗ An error occurred at pair {pair_index + 1}:")
                print("============= Error Details =============")
                print(f"Error message: {str(e)}")
                print("\n============= Full Stack Trace =============")
                print(traceback.format_exc())
                print("==========================================")

                # Save error scene
                try:
                    error_output = args.output.rsplit('.', 1)[0] + f'_error_{pair_index + 1:03d}.blend'
                    bpy.ops.wm.save_as_mainfile(filepath=error_output)
                    print(f"Save error scene: {error_output}")
                except:
                    pass

        total_time = time.time() - start_time
        print("Progress: 1.00")
        print(f"\n{'='*60}")
        print("Processing complete")
        print(f"Success: {successful_pairs}/{total_pairs} pairs")
        print(f"Total time: {total_time:.2f} seconds")
        print(f"{'='*60}")

        return successful_pairs == total_pairs

    except Exception as e:
        import traceback
        print("============= Fatal Error =============")
        print(f"Error message: {str(e)}")
        print("\n============= Full Stack Trace =============")
        print(traceback.format_exc())
        print("=====================================")
        return False

if __name__ == "__main__":
    main()